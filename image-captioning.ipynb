{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":1706129,"sourceType":"datasetVersion","datasetId":1011404},{"sourceId":11490486,"sourceType":"datasetVersion","datasetId":7202696},{"sourceId":4235478,"sourceType":"kernelVersion"},{"sourceId":163770428,"sourceType":"kernelVersion"},{"sourceId":184692496,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport random\nfrom collections import defaultdict\n\n# Paths\nimport argparse\nimport os\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Image Captioning Model\")\n    parser.add_argument('--data-dir', default='/kaggle/input/coco-2017-dataset/coco2017', help='Path to COCO 2017 dataset')\n    parser.add_argument('--checkpoint-dir', default='/kaggle/working/checkpoints', help='Path to save checkpoints')\n    parser.add_argument('--glove-path', default='/kaggle/working/glove.6B.300d.txt', help='Path to GloVe embeddings')\n    return parser.parse_args(args=[])  # Use empty args for Kaggle notebook\n\nargs = parse_args()\n\n# Paths\nDATA_DIR = args.data_dir\nANNOTATION_FILE = os.path.join(DATA_DIR, 'annotations', 'captions_train2017.json')\nANNOTATION_FILE2 = os.path.join(DATA_DIR, 'annotations', 'captions_val2017.json')\nIMAGE_FOLDER = os.path.join(DATA_DIR, 'train2017')\nIMAGE_FOLDER2 = os.path.join(DATA_DIR, 'val2017')\nCHECKPOINT_DIR = args.checkpoint_dir\nGLOVE_PATH = args.glove_path\n\n# Load annotations\nwith open(ANNOTATION_FILE, 'r') as f:\n    annotations = json.load(f)\n\n# Build a dictionary: image_id -> list of captions\ncaptions_dict = defaultdict(list)\nfor ann in annotations['annotations']:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    captions_dict[image_id].append(caption)\n\n# Check sample\n# sample_image_id = list(captions_dict.keys())[0]\nsample_image_id = random.choice(list(captions_dict.keys()))\nprint(f\"Image ID: {sample_image_id}\")\nprint(\"Captions:\")\nfor cap in captions_dict[sample_image_id]:\n    print(\"-\", cap)","metadata":{"_uuid":"d961bf5c-64df-40ca-bd69-4a4586029471","_cell_guid":"62deee2b-37ec-4dc8-8ab8-6e28389729dd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T20:33:43.151022Z","iopub.execute_input":"2025-04-20T20:33:43.151387Z","iopub.status.idle":"2025-04-20T20:33:44.669733Z","shell.execute_reply.started":"2025-04-20T20:33:43.151357Z","shell.execute_reply":"2025-04-20T20:33:44.668827Z"}},"outputs":[{"name":"stdout","text":"Image ID: 450845\nCaptions:\n- A man in blue shirt and shorts swinging a baseball bat.\n- A man in a field about to swing at a ball with a bat.\n- A man swinging a baseball bat on top of a lush green field.\n- A man attempting to swing a bat at a ball.\n- A man taking a swing at a baseball on a field\n","output_type":"stream"}],"execution_count":177},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('punkt')  # for word_tokenize\nfrom nltk.tokenize import word_tokenize\n\ndef clean_caption(caption):\n    caption = caption.lower()                            # Lowercase\n    caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)        # Remove punctuation\n    caption = re.sub(r\"\\s+\", \" \", caption).strip()       # Trim extra spaces\n    return caption\n\n# Clean and tokenize all captions\ncleaned_captions_dict = {}\nfor image_id, captions in captions_dict.items():\n    cleaned_captions = []\n    for cap in captions:\n        clean_cap = clean_caption(cap)\n        tokens = word_tokenize(clean_cap)\n        cleaned_captions.append(tokens)\n    cleaned_captions_dict[image_id] = cleaned_captions\n\n# Check cleaned sample\nprint(\"Cleaned captions for image ID:\", sample_image_id)\nfor cap in cleaned_captions_dict[sample_image_id]:\n    print(cap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:33:44.670669Z","iopub.execute_input":"2025-04-20T20:33:44.670975Z","iopub.status.idle":"2025-04-20T20:34:37.186676Z","shell.execute_reply.started":"2025-04-20T20:33:44.670952Z","shell.execute_reply":"2025-04-20T20:34:37.185892Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nCleaned captions for image ID: 450845\n['a', 'man', 'in', 'blue', 'shirt', 'and', 'shorts', 'swinging', 'a', 'baseball', 'bat']\n['a', 'man', 'in', 'a', 'field', 'about', 'to', 'swing', 'at', 'a', 'ball', 'with', 'a', 'bat']\n['a', 'man', 'swinging', 'a', 'baseball', 'bat', 'on', 'top', 'of', 'a', 'lush', 'green', 'field']\n['a', 'man', 'attempting', 'to', 'swing', 'a', 'bat', 'at', 'a', 'ball']\n['a', 'man', 'taking', 'a', 'swing', 'at', 'a', 'baseball', 'on', 'a', 'field']\n","output_type":"stream"}],"execution_count":178},{"cell_type":"code","source":"import os \n# os.makedirs('/kaggle/working/models', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:37.188003Z","iopub.execute_input":"2025-04-20T20:34:37.188242Z","iopub.status.idle":"2025-04-20T20:34:37.191713Z","shell.execute_reply.started":"2025-04-20T20:34:37.188222Z","shell.execute_reply":"2025-04-20T20:34:37.190829Z"}},"outputs":[],"execution_count":179},{"cell_type":"code","source":"# open('/kaggle/working/models/__init__.py', 'a').close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:37.192904Z","iopub.execute_input":"2025-04-20T20:34:37.193127Z","iopub.status.idle":"2025-04-20T20:34:37.206042Z","shell.execute_reply.started":"2025-04-20T20:34:37.193108Z","shell.execute_reply":"2025-04-20T20:34:37.205247Z"}},"outputs":[],"execution_count":180},{"cell_type":"code","source":"from collections import Counter\n\nmin_word_freq = 5  # You can tune this\nword_freq = Counter()\n\n# Count word frequencies\nfor captions in cleaned_captions_dict.values():\n    for tokens in captions:\n        word_freq.update(tokens)\n\n# Filter words below the threshold\nwords = [word for word in word_freq if word_freq[word] >= min_word_freq]\n\n# Special tokens\nword_map = {\n    '<pad>': 0,\n    '<start>': 1,\n    '<end>': 2,\n    '<unk>': 3\n}\n\n# Add the remaining words\nfor i, word in enumerate(words, start=4):\n    word_map[word] = i\n\n# Reverse map\nidx2word = {v: k for k, v in word_map.items()}\n\nprint(f\"Vocabulary size: {len(word_map)}\")\nprint(\"Sample word map entries:\")\nfor i, (word, idx) in enumerate(list(word_map.items())[:10]):\n    print(f\"{word}: {idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:37.206871Z","iopub.execute_input":"2025-04-20T20:34:37.207169Z","iopub.status.idle":"2025-04-20T20:34:38.373701Z","shell.execute_reply.started":"2025-04-20T20:34:37.207141Z","shell.execute_reply":"2025-04-20T20:34:38.372981Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 10307\nSample word map entries:\n<pad>: 0\n<start>: 1\n<end>: 2\n<unk>: 3\na: 4\nbicycle: 5\nreplica: 6\nwith: 7\nclock: 8\nas: 9\n","output_type":"stream"}],"execution_count":181},{"cell_type":"code","source":"encoded_captions = {}\n\nfor image_id, captions in cleaned_captions_dict.items():\n    encoded = []\n    for tokens in captions:\n        # Encode each word or use <unk> if not in vocab\n        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n        # Add <start> and <end> tokens\n        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n        encoded.append(enc)\n    encoded_captions[image_id] = encoded\n\n# Check sample\nprint(\"Encoded captions for image ID:\", sample_image_id)\nfor cap in encoded_captions[sample_image_id]:\n    print(cap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:38.374347Z","iopub.execute_input":"2025-04-20T20:34:38.374585Z","iopub.status.idle":"2025-04-20T20:34:42.673114Z","shell.execute_reply.started":"2025-04-20T20:34:38.374567Z","shell.execute_reply":"2025-04-20T20:34:42.672277Z"}},"outputs":[{"name":"stdout","text":"Encoded captions for image ID: 450845\n[1, 4, 208, 20, 27, 1011, 29, 942, 2816, 4, 2200, 4563, 2]\n[1, 4, 208, 20, 4, 266, 840, 50, 5346, 175, 4, 2577, 7, 4, 4563, 2]\n[1, 4, 208, 2816, 4, 2200, 4563, 39, 171, 25, 4, 2436, 150, 266, 2]\n[1, 4, 208, 3319, 50, 5346, 4, 4563, 175, 4, 2577, 2]\n[1, 4, 208, 79, 4, 5346, 175, 4, 2200, 39, 4, 266, 2]\n","output_type":"stream"}],"execution_count":182},{"cell_type":"code","source":"import json\n\n# Save encoded captions\nwith open('encoded_captions.json', 'w') as f:\n    json.dump({str(k): v for k, v in encoded_captions.items()}, f)\n\n# Save word map\nwith open('word_map.json', 'w') as f:\n    json.dump(word_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:42.673838Z","iopub.execute_input":"2025-04-20T20:34:42.674089Z","iopub.status.idle":"2025-04-20T20:34:47.354995Z","shell.execute_reply.started":"2025-04-20T20:34:42.674069Z","shell.execute_reply":"2025-04-20T20:34:47.354073Z"}},"outputs":[],"execution_count":183},{"cell_type":"code","source":"import os\nif not os.path.exists('/kaggle/working/glove.6B.300d.txt'):\n    !wget http://nlp.stanford.edu/data/glove.6B.zip -O /kaggle/working/glove.6B.zip\n    !unzip /kaggle/working/glove.6B.zip -d /kaggle/working/\n    !rm /kaggle/working/glove.6B.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:47.357690Z","iopub.execute_input":"2025-04-20T20:34:47.357936Z","iopub.status.idle":"2025-04-20T20:34:47.363088Z","shell.execute_reply.started":"2025-04-20T20:34:47.357916Z","shell.execute_reply":"2025-04-20T20:34:47.362231Z"}},"outputs":[],"execution_count":184},{"cell_type":"code","source":"import numpy as np\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Path to GloVe 300d\nglove_path = GLOVE_PATH\nembedding_dimension = 300\nvocab_size = len(word_map)\n\n# Load GloVe embeddings\nlogger.info(\"Loading GloVe embeddings...\")\nglove = {}\ntry:\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            tokens = line.split()\n            word = tokens[0]\n            vec = np.array(tokens[1:], dtype=np.float32)\n            glove[word] = vec\nexcept FileNotFoundError:\n    logger.error(f\"GloVe file {glove_path} not found\")\n    raise\n\n# Create embedding matrix\nprint(\"Building embedding matrix...\")\nembedding_matrix = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim)).astype(np.float32)\n\nfor word, idx in word_map.items():\n    if word in glove:\n        embedding_matrix[idx] = glove[word]\n\nprint(\"Done. Shape:\", embedding_matrix.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:34:47.364454Z","iopub.execute_input":"2025-04-20T20:34:47.364701Z","iopub.status.idle":"2025-04-20T20:35:10.454729Z","shell.execute_reply.started":"2025-04-20T20:34:47.364682Z","shell.execute_reply":"2025-04-20T20:35:10.453934Z"}},"outputs":[{"name":"stdout","text":"Building embedding matrix...\nDone. Shape: (10307, 300)\n","output_type":"stream"}],"execution_count":185},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nimport json\nimport os\n\nclass CaptionDataset(Dataset):\n    def __init__(self, image_folder, encoded_captions_file, word_map_file, transform=None):\n        # Load encoded captions and word map\n        with open(encoded_captions_file, 'r') as j:\n            self.captions = json.load(j)\n        with open(word_map_file, 'r') as j:\n            self.word_map = json.load(j)\n\n        self.image_folder = image_folder\n        self.image_ids = list(self.captions.keys())\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        image_path = os.path.join(self.image_folder, f\"{int(image_id):012}.jpg\")\n        \n        # Load image\n        img = Image.open(image_path).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n\n        # Randomly select one caption for the image\n        caps = self.captions[image_id]\n        caption = random.choice(caps)\n        caption = torch.tensor(caption, dtype=torch.long)\n\n        return img, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:10.455509Z","iopub.execute_input":"2025-04-20T20:35:10.455836Z","iopub.status.idle":"2025-04-20T20:35:10.462149Z","shell.execute_reply.started":"2025-04-20T20:35:10.455797Z","shell.execute_reply":"2025-04-20T20:35:10.461436Z"}},"outputs":[],"execution_count":186},{"cell_type":"code","source":"def caption_collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle batches of (image, caption) with variable-length captions.\n    \"\"\"\n    images = []\n    captions = []\n\n    for img, cap in batch:\n        images.append(img)\n        captions.append(cap)\n\n    # Stack images (they are all same size)\n    images = torch.stack(images, dim=0)\n\n    # Pad captions to the max length in the batch\n    lengths = [len(cap) for cap in captions]\n    max_len = max(lengths)\n    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        padded_captions[i, :end] = cap[:end]\n\n    return images, padded_captions, torch.tensor(lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:10.462854Z","iopub.execute_input":"2025-04-20T20:35:10.463060Z","iopub.status.idle":"2025-04-20T20:35:10.480378Z","shell.execute_reply.started":"2025-04-20T20:35:10.463042Z","shell.execute_reply":"2025-04-20T20:35:10.479593Z"}},"outputs":[],"execution_count":187},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # from ImageNet\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Dataset\ndataset = CaptionDataset(\n    image_folder='/kaggle/input/coco-2017-dataset/coco2017/train2017',\n    encoded_captions_file='encoded_captions.json',\n    word_map_file='word_map.json',\n    transform=transform\n)\n\n# DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=4, #reduced for memory originally 4\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=caption_collate_fn\n)\n\n# Check sample batch\nfor images, captions, lengths in dataloader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Caption batch shape:\", captions.shape)\n    print(\"Lengths:\", lengths)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:10.481252Z","iopub.execute_input":"2025-04-20T20:35:10.481513Z","iopub.status.idle":"2025-04-20T20:35:12.923090Z","shell.execute_reply.started":"2025-04-20T20:35:10.481487Z","shell.execute_reply":"2025-04-20T20:35:12.922017Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([4, 3, 256, 256])\nCaption batch shape: torch.Size([4, 14])\nLengths: tensor([14, 10, 13, 14])\n","output_type":"stream"}],"execution_count":188},{"cell_type":"code","source":"# ----------- Process Validation Captions -----------\nwith open(ANNOTATION_FILE2, 'r') as f:\n    val_annotations = json.load(f)\n\nval_captions_dict = defaultdict(list)\nfor ann in val_annotations['annotations']:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    val_captions_dict[image_id].append(caption)\n\n# Clean and tokenize validation captions\ncleaned_val_captions_dict = {}\nfor image_id, captions in val_captions_dict.items():\n    cleaned_captions = []\n    for cap in captions:\n        clean_cap = clean_caption(cap)\n        tokens = word_tokenize(clean_cap)\n        cleaned_captions.append(tokens)\n    cleaned_val_captions_dict[image_id] = cleaned_captions\n\n# Encode validation captions\nencoded_val_captions = {}\nfor image_id, captions in cleaned_val_captions_dict.items():\n    encoded = []\n    for tokens in captions:\n        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n        encoded.append(enc)\n    encoded_val_captions[image_id] = encoded\n\n# Save encoded val captions\nwith open('encoded_captions_val.json', 'w') as f:\n    json.dump({str(k): v for k, v in encoded_val_captions.items()}, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:12.924127Z","iopub.execute_input":"2025-04-20T20:35:12.924440Z","iopub.status.idle":"2025-04-20T20:35:15.487986Z","shell.execute_reply.started":"2025-04-20T20:35:12.924410Z","shell.execute_reply":"2025-04-20T20:35:15.487053Z"}},"outputs":[],"execution_count":189},{"cell_type":"code","source":"# Validation dataset\nval_dataset = CaptionDataset(\n    image_folder='/kaggle/input/coco-2017-dataset/coco2017/val2017',\n    encoded_captions_file='encoded_captions_val.json',  # You need to create this\n    word_map_file='word_map.json',\n    transform=transform\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=caption_collate_fn\n)\n\n# Check sample batch\nfor images, captions, lengths in dataloader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Caption batch shape:\", captions.shape)\n    print(\"Lengths:\", lengths)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:15.488975Z","iopub.execute_input":"2025-04-20T20:35:15.489222Z","iopub.status.idle":"2025-04-20T20:35:15.853093Z","shell.execute_reply.started":"2025-04-20T20:35:15.489201Z","shell.execute_reply":"2025-04-20T20:35:15.851923Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([4, 3, 256, 256])\nCaption batch shape: torch.Size([4, 14])\nLengths: tensor([13, 14, 11, 11])\n","output_type":"stream"}],"execution_count":190},{"cell_type":"markdown","source":"## Attention Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Linear layer to transform encoder's output\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # Combine them and produce scalar energy\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # Softmax over the pixels\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: encoded images, shape -> (batch_size, num_pixels, encoder_dim)\n        decoder_hidden: previous decoder hidden state, shape -> (batch_size, decoder_dim)\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:15.854644Z","iopub.execute_input":"2025-04-20T20:35:15.855004Z","iopub.status.idle":"2025-04-20T20:35:15.863231Z","shell.execute_reply.started":"2025-04-20T20:35:15.854970Z","shell.execute_reply":"2025-04-20T20:35:15.862122Z"}},"outputs":[],"execution_count":191},{"cell_type":"markdown","source":"## Decoder with Attention","metadata":{}},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n        self.embedding.weight.requires_grad = False  # Optional: freeze during training\n\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # initialize hidden state\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # initialize cell state\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # create a gating scalar\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # output layer\n\n        self.init_weights()  # initialize weights\n\n    def init_weights(self):\n        # self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, shape (batch_size, num_pixels, encoder_dim)\n        :param encoded_captions: encoded captions, shape (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, shape (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths\n        # Corrected line\n        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        decode_lengths = caption_lengths - 1\n\n        # Create tensors to hold word prediction scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(encoder_out.device)\n\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar\n            attention_weighted_encoding = gate * attention_weighted_encoding\n\n            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n            h, c = self.decode_step(input_lstm, (h[:batch_size_t], c[:batch_size_t]))  # LSTM step\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:15.864477Z","iopub.execute_input":"2025-04-20T20:35:15.864965Z","iopub.status.idle":"2025-04-20T20:35:15.882757Z","shell.execute_reply.started":"2025-04-20T20:35:15.864933Z","shell.execute_reply":"2025-04-20T20:35:15.881811Z"}},"outputs":[],"execution_count":192},{"cell_type":"code","source":"from torchvision.models import resnet101, ResNet101_Weights\n\nclass Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.cnn = resnet101(weights=ResNet101_Weights.IMAGENET1K_V1)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune()\n\n    def forward(self, images):\n        x = self.cnn.conv1(images)\n        x = self.cnn.bn1(x)\n        x = self.cnn.relu(x)\n        x = self.cnn.maxpool(x)\n\n        x = self.cnn.layer1(x)\n        x = self.cnn.layer2(x)\n        x = self.cnn.layer3(x)\n        x = self.cnn.layer4(x)  # Shape: (batch_size, 2048, 7, 7)\n\n        x = self.adaptive_pool(x)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        x = x.permute(0, 2, 3, 1)  # (batch_size, encoded_size, encoded_size, 2048)\n        x = x.view(x.size(0), -1, x.size(-1))  # (batch_size, num_pixels=encoded_size^2, 2048)\n        return x\n\n    def fine_tune(self, fine_tune=True):\n        for p in self.cnn.parameters():\n            p.requires_grad = False  # Freeze all by default\n        if fine_tune:\n            # Fine-tune layer3 and layer4\n            for p in self.cnn.layer3.parameters():\n                p.requires_grad = True\n            for p in self.cnn.layer4.parameters():\n                p.requires_grad = True\n            for p in self.adaptive_pool.parameters():\n                p.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:15.884119Z","iopub.execute_input":"2025-04-20T20:35:15.884462Z","iopub.status.idle":"2025-04-20T20:35:15.903058Z","shell.execute_reply.started":"2025-04-20T20:35:15.884429Z","shell.execute_reply":"2025-04-20T20:35:15.902331Z"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":"import os\nimport glob\n\ndef save_checkpoint(encoder, decoder, optimizer, epoch, train_loss, val_loss, word_map, \n                   checkpoint_dir, best_val_loss=float('inf'), is_best=False):\n    \"\"\"Save model checkpoint\"\"\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Save regular checkpoint\n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n    \n    checkpoint = {\n        'epoch': epoch + 1,  # Save as next epoch to resume from\n        'encoder_state_dict': encoder.state_dict(),\n        'decoder_state_dict': decoder.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'word_map': word_map,\n        'best_val_loss': best_val_loss\n    }\n    \n    torch.save(checkpoint, checkpoint_path)\n    print(f\"Checkpoint saved: {checkpoint_path}\")\n    \n    # Save best model separately if this is the best one\n    if is_best:\n        best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n        torch.save(checkpoint, best_model_path)\n        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n\ndef resume_from_checkpoint(checkpoint_path, encoder, decoder, optimizer, device):\n    \"\"\"Load checkpoint and resume training\"\"\"\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    \n    # Load checkpoint on CPU to avoid GPU memory issues\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Load model states\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    \n    # Move models to device after loading\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n    \n    # Load optimizer state\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    # Return the starting epoch and best validation loss\n    start_epoch = checkpoint['epoch']  # Continue from next epoch\n    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n    \n    print(f\"Resuming from epoch {start_epoch} with best validation loss: {best_val_loss:.4f}\")\n    return encoder, decoder, optimizer, start_epoch, best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:15.904011Z","iopub.execute_input":"2025-04-20T20:35:15.904289Z","iopub.status.idle":"2025-04-20T20:35:15.929634Z","shell.execute_reply.started":"2025-04-20T20:35:15.904261Z","shell.execute_reply":"2025-04-20T20:35:15.928711Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"# Test encoder-decoder integration\nimport torch\nfrom torchvision.models import ResNet101_Weights\n\n# Check if GPU is available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# encoder = Encoder(weights=ResNet101_Weights.IMAGENET1K_V1).to(device)\nencoder = Encoder().to(device)\n# encoder = Encoder(weights=ResNet101_Weights.DEFAULT).to(device)\n\ndecoder = DecoderWithAttention(\n    attention_dim=512,\n    embed_dim=300,\n    decoder_dim=512,\n    vocab_size=len(word_map),\n    encoder_dim=2048,\n    dropout=0.5\n).to(device)\n\n# Test forward pass\nimages, captions, lengths = next(iter(dataloader))\nimages = images.to(device)\ncaptions = captions.to(device)\n\nencoder_out = encoder(images)\n# predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths, device=device))\npredictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths).clone().detach().to(device))\n\nprint(\"Encoder output shape:\", encoder_out.shape)  # Should be (batch_size, 196, 2048)\nprint(\"Predictions shape:\", predictions.shape)     # Should be (batch_size, max_len, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:15.930509Z","iopub.execute_input":"2025-04-20T20:35:15.931095Z","iopub.status.idle":"2025-04-20T20:35:17.325601Z","shell.execute_reply.started":"2025-04-20T20:35:15.931063Z","shell.execute_reply":"2025-04-20T20:35:17.324470Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nEncoder output shape: torch.Size([4, 196, 2048])\nPredictions shape: torch.Size([4, 12, 10307])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-195-e68131b897fa>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths).clone().detach().to(device))\n","output_type":"stream"}],"execution_count":195},{"cell_type":"code","source":"class MaskedCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=0)  # ignore <pad>\n\n    def forward(self, predictions, targets, lengths):\n        batch_size, max_len, vocab_size = predictions.shape\n\n        predictions = predictions.view(-1, vocab_size)      # (batch_size * max_len, vocab_size)\n        targets = targets.contiguous().view(-1)              # (batch_size * max_len)\n\n        losses = self.criterion(predictions, targets)        # (batch_size * max_len)\n\n        # Create mask\n        mask = torch.arange(max_len).expand(batch_size, max_len).to(lengths.device)\n        mask = (mask < lengths.unsqueeze(1)).float()         # (batch_size, max_len)\n        mask = mask.view(-1)                                 # Flatten to (batch_size * max_len)\n\n        losses = losses * mask\n        return losses.sum() / mask.sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:17.326681Z","iopub.execute_input":"2025-04-20T20:35:17.327041Z","iopub.status.idle":"2025-04-20T20:35:17.332806Z","shell.execute_reply.started":"2025-04-20T20:35:17.327001Z","shell.execute_reply":"2025-04-20T20:35:17.331925Z"}},"outputs":[],"execution_count":196},{"cell_type":"code","source":"encoder = Encoder().to(device)\ndecoder = DecoderWithAttention(\n    attention_dim=512,\n    embed_dim=300,\n    decoder_dim=512,\n    vocab_size=len(word_map),\n    encoder_dim=2048,\n    dropout=0.5\n).to(device)\n\n# Only fine-tune the encoder's adaptive pool layer\nencoder_params = (\n    list(encoder.cnn.layer3.parameters()) +\n    list(encoder.cnn.layer4.parameters()) +\n    list(encoder.adaptive_pool.parameters())\n)\ndecoder_params = decoder.parameters()\n\noptimizer = torch.optim.Adam(\n    params=[\n        {'params': encoder_params, 'lr': 1e-4},  # Lower LR for encoder\n        {'params': decoder_params, 'lr': 4e-4}    # Higher LR for decoder\n    ],\n    weight_decay=1e-5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:17.333857Z","iopub.execute_input":"2025-04-20T20:35:17.334148Z","iopub.status.idle":"2025-04-20T20:35:18.350912Z","shell.execute_reply.started":"2025-04-20T20:35:17.334119Z","shell.execute_reply":"2025-04-20T20:35:18.350216Z"}},"outputs":[],"execution_count":197},{"cell_type":"code","source":"from torch.amp import GradScaler, autocast\nimport time\nimport logging\n\ndef train_epoch(encoder, decoder, dataloader, criterion, optimizer, device, grad_clip=5.0):\n    encoder.train()\n    decoder.train()\n    total_loss = 0\n    scaler = GradScaler('cuda')\n    start_time = time.time()\n    total_batches = len(dataloader)\n    print_interval = max(1, total_batches // 100)  # Update every ~1% of batches\n\n    for i, (images, captions, lengths) in enumerate(dataloader):\n        try:\n            images = images.to(device, non_blocking=True)\n            captions = captions.to(device, non_blocking=True)\n            lengths_tensor = torch.as_tensor(lengths, device=device)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast('cuda'):\n                encoder_out = encoder(images)\n                predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths_tensor)\n                targets = captions[:, 1:]\n                predictions = predictions[:, :max(decode_lengths), :]\n                loss = criterion(predictions, targets, lengths_tensor)\n\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n            torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n            scaler.step(optimizer)\n            scaler.update()\n\n            total_loss += loss.item()\n\n            # Print progress as percentage\n            if i % print_interval == 0 or i == total_batches - 1:\n                progress = (i + 1) / total_batches * 100\n                print(f\"Progress: {progress:.1f}% ({i + 1}/{total_batches} batches), Loss: {loss.item():.4f}, Time: {time.time() - start_time:.2f}s\", flush=True)\n                logger.info(f\"Progress: {progress:.1f}% ({i + 1}/{total_batches} batches), Loss: {loss.item():.4f}\")\n        except RuntimeError as e:\n            print(f\"RuntimeError in batch {i}: {str(e)}\", flush=True)\n            logger.error(f\"RuntimeError in batch {i}: {str(e)}\")\n            torch.cuda.empty_cache()\n            continue\n        except Exception as e:\n            print(f\"Unexpected error in batch {i}: {str(e)}\", flush=True)\n            logger.error(f\"Unexpected error in batch {i}: {str(e)}\")\n            continue\n\n    if i == 0:\n        print(\"No batches processed successfully\", flush=True)\n        logger.warning(\"No batches processed successfully\")\n        return float('inf')\n\n    avg_loss = total_loss / (i + 1)\n    print(f\"Epoch completed. Train Loss: {avg_loss:.4f}, Total Time: {time.time() - start_time:.2f}s\", flush=True)\n    logger.info(f\"Epoch completed. Train Loss: {avg_loss:.4f}\")\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:18.351705Z","iopub.execute_input":"2025-04-20T20:35:18.351908Z","iopub.status.idle":"2025-04-20T20:35:18.361061Z","shell.execute_reply.started":"2025-04-20T20:35:18.351890Z","shell.execute_reply":"2025-04-20T20:35:18.360248Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"def validate(encoder, decoder, val_loader, criterion, device):\n    encoder.eval()\n    decoder.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for images, captions, lengths in val_loader:\n            images = images.to(device)\n            captions = captions.to(device)\n            # lengths = torch.tensor(lengths).to(device)\n            lengths_tensor = torch.tensor(lengths).to(device)\n\n            \n            encoder_out = encoder(images)\n            predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n            \n            targets = captions[:, 1:]\n            predictions = predictions[:, :max(decode_lengths), :]\n            \n            loss = criterion(predictions, targets, lengths_tensor)\n            total_loss += loss.item()\n    \n    return total_loss / len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:18.365545Z","iopub.execute_input":"2025-04-20T20:35:18.365787Z","iopub.status.idle":"2025-04-20T20:35:18.377213Z","shell.execute_reply.started":"2025-04-20T20:35:18.365767Z","shell.execute_reply":"2025-04-20T20:35:18.376464Z"}},"outputs":[],"execution_count":199},{"cell_type":"code","source":"import os\nimport glob\n\ndef save_checkpoint(encoder, decoder, optimizer, epoch, train_loss, val_loss, word_map, \n                   checkpoint_dir, best_val_loss=float('inf'), is_best=False):\n    \"\"\"Save model checkpoint\"\"\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Save regular checkpoint\n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n    \n    checkpoint = {\n        'epoch': epoch + 1,  # Save as next epoch to resume from\n        'encoder_state_dict': encoder.state_dict(),\n        'decoder_state_dict': decoder.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'word_map': word_map,\n        'best_val_loss': best_val_loss\n    }\n    \n    try:\n        torch.save(checkpoint, checkpoint_path)\n        print(f\"Checkpoint saved: {checkpoint_path}\")\n        \n        # Save best model separately if this is the best one\n        if is_best:\n            best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n            torch.save(checkpoint, best_model_path)\n            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n            \n        # Keep only the last 3 checkpoints to save disk space\n        checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth')))\n        if len(checkpoint_files) > 3:\n            for old_checkpoint in checkpoint_files[:-3]:\n                os.remove(old_checkpoint)\n                \n    except Exception as e:\n        print(f\"Error saving checkpoint: {e}\")\n        # Try alternative save location\n        torch.save(checkpoint, '/kaggle/working/emergency_checkpoint.pth')\n\ndef resume_from_checkpoint(checkpoint_path, encoder, decoder, optimizer, device):\n    \"\"\"Load checkpoint and resume training\"\"\"\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    \n    # Load checkpoint on CPU to avoid GPU memory issues\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Load model states\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    \n    # Move models to device after loading\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n    \n    # Load optimizer state\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    # Move optimizer state to device\n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if isinstance(v, torch.Tensor):\n                state[k] = v.to(device)\n    \n    # Return the starting epoch and best validation loss\n    start_epoch = checkpoint['epoch']  # Continue from next epoch\n    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n    \n    print(f\"Resuming from epoch {start_epoch} with best validation loss: {best_val_loss:.4f}\")\n    return encoder, decoder, optimizer, start_epoch, best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:18.378561Z","iopub.execute_input":"2025-04-20T20:35:18.378823Z","iopub.status.idle":"2025-04-20T20:35:18.388880Z","shell.execute_reply.started":"2025-04-20T20:35:18.378804Z","shell.execute_reply":"2025-04-20T20:35:18.388257Z"}},"outputs":[],"execution_count":200},{"cell_type":"code","source":"# Initialize components\ncriterion = MaskedCrossEntropyLoss().to(device)\nnum_epochs = 1  # For initial test\n\n# Quick test with 1 batch\ntest_images, test_captions, test_lengths = next(iter(dataloader))\ntest_images = test_images.to(device)\ntest_captions = test_captions.to(device)\n# test_lengths = torch.tensor(test_lengths).to(device)\ntest_lengths = test_lengths.clone().detach().to(device)\n\n\n# Forward test\nencoder_out = encoder(test_images)\npredictions, _, decode_lengths, _, _ = decoder(encoder_out, test_captions, test_lengths)\ntargets = test_captions[:, 1:]\n\n# Convert decode_lengths to tensor\n# decode_lengths = torch.tensor(decode_lengths).to(device)\n\nloss = criterion(predictions, targets, decode_lengths)\n# decode_lengths = torch.tensor(decode_lengths).to(device)\n# loss = criterion(predictions, targets, decode_lengths)\n\nprint(f\"Initial loss: {loss.item():.4f}\")  # Should be ~log(vocab_size) = ~9.2 for vocab_size=10307\noptimizer.step()  # Verify backprop works without errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:18.389623Z","iopub.execute_input":"2025-04-20T20:35:18.389849Z","iopub.status.idle":"2025-04-20T20:35:18.828836Z","shell.execute_reply.started":"2025-04-20T20:35:18.389831Z","shell.execute_reply":"2025-04-20T20:35:18.827925Z"}},"outputs":[{"name":"stdout","text":"Initial loss: 7.5577\n","output_type":"stream"}],"execution_count":201},{"cell_type":"code","source":"# Define checkpoint directory\ncheckpoint_dir = '/kaggle/working/checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Find latest checkpoint if it exists\nlatest_checkpoint = None\ncheckpoint_files = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth'))\nif checkpoint_files:\n    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n\n# Resume from checkpoint if available\nstart_epoch = 0\nbest_val_loss = float('inf')\n\nif latest_checkpoint:\n    encoder, decoder, optimizer, start_epoch, best_val_loss = resume_from_checkpoint(\n        latest_checkpoint, encoder, decoder, optimizer, device\n    )\n\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nnum_epochs = 1  # Increased for proper training\npatience = 3  # Early stopping patience\nbest_val_loss = float('inf')\nepochs_no_improve = 0\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    logger.info(f\"--- Epoch {epoch + 1}/{num_epochs} ---\")\n\n    # Train for one epoch\n    train_loss = train_epoch(\n        encoder, decoder, dataloader,\n        criterion, optimizer, device\n    )\n    logger.info(f\"Train Loss: {train_loss:.4f}\")\n\n    # Validate\n    val_loss = validate(\n        encoder, decoder, val_dataloader,\n        criterion, device\n    )\n    logger.info(f\"Validation Loss: {val_loss:.4f}\")\n\n    # Check if this is the best model\n    is_best = val_loss < best_val_loss\n    if is_best:\n        best_val_loss = val_loss\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n\n    # Save checkpoint\n    save_checkpoint(\n        encoder, decoder, optimizer,\n        epoch, train_loss, val_loss,\n        word_map, checkpoint_dir,\n        best_val_loss, is_best\n    )\n\n    # Early stopping\n    if epochs_no_improve >= patience:\n        logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n        break\n        \n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n    \n    # Train for one epoch\n    train_loss = train_epoch(\n        encoder, decoder, dataloader,\n        criterion, optimizer, device\n    )\n    print(f\"Train Loss: {train_loss:.4f}\")\n    \n    # Validate\n    val_loss = validate(\n        encoder, decoder, val_dataloader,\n        criterion, device\n    )\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # Check if this is the best model\n    is_best = val_loss < best_val_loss\n    if is_best:\n        best_val_loss = val_loss\n    \n    # Save checkpoint\n    save_checkpoint(\n        encoder, decoder, optimizer,\n        epoch, train_loss, val_loss,\n        word_map, checkpoint_dir,\n        best_val_loss, is_best\n    )\n\n# Mark training as complete\nwith open(os.path.join(checkpoint_dir, 'TRAINING_COMPLETE'), 'w') as f:\n    f.write('Training completed successfully')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:35:18.829955Z","iopub.execute_input":"2025-04-20T20:35:18.830210Z"}},"outputs":[{"name":"stdout","text":"Progress: 0.0% (1/29572 batches), Loss: 8.7134, Time: 0.42s\nProgress: 1.0% (296/29572 batches), Loss: 5.9701, Time: 36.75s\nProgress: 2.0% (591/29572 batches), Loss: 4.7865, Time: 72.97s\nProgress: 3.0% (886/29572 batches), Loss: 5.0756, Time: 108.93s\nProgress: 4.0% (1181/29572 batches), Loss: 5.2620, Time: 145.00s\nProgress: 5.0% (1476/29572 batches), Loss: 5.5826, Time: 181.11s\nProgress: 6.0% (1771/29572 batches), Loss: 5.2093, Time: 217.08s\nProgress: 7.0% (2066/29572 batches), Loss: 5.7231, Time: 253.10s\nProgress: 8.0% (2361/29572 batches), Loss: 5.0764, Time: 289.00s\nProgress: 9.0% (2656/29572 batches), Loss: 5.3071, Time: 324.81s\nProgress: 10.0% (2951/29572 batches), Loss: 6.9624, Time: 360.87s\nProgress: 11.0% (3246/29572 batches), Loss: 5.3903, Time: 396.72s\nProgress: 12.0% (3541/29572 batches), Loss: 4.3763, Time: 432.67s\nProgress: 13.0% (3836/29572 batches), Loss: 5.3750, Time: 468.66s\nProgress: 14.0% (4131/29572 batches), Loss: 4.6123, Time: 504.32s\nProgress: 15.0% (4426/29572 batches), Loss: 5.6638, Time: 540.43s\nProgress: 16.0% (4721/29572 batches), Loss: 4.8870, Time: 576.04s\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# for epoch in range(num_epochs):\n#     print(f\"\\n--- Epoch {epoch + 1} ---\")\n\n#     train_loss = train_epoch(\n#         encoder, decoder, dataloader,\n#         criterion, optimizer, device\n#     )\n#     print(f\"Train Loss: {train_loss:.4f}\")\n\n#     val_loss = validate(\n#         encoder, decoder, val_dataloader,\n#         criterion, device\n#     )\n#     print(f\"Validation Loss: {val_loss:.4f}\")\n\n#     # Save the model after each epoch\n#     torch.save({\n#         'epoch': epoch,\n#         'encoder_state_dict': encoder.state_dict(),\n#         'decoder_state_dict': decoder.state_dict(),\n#         'optimizer_state_dict': optimizer.state_dict(),\n#         'loss': train_loss\n#     }, f'model_epoch_{epoch + 1}.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_path = '/kaggle/working/checkpoint.pth'\n\ndef save_checkpoint(state, filename=checkpoint_path):\n    torch.save(state, filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, encoder, decoder, optimizer):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r /kaggle/input/requirement/requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y nlg-eval gensim preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install xdg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip list | grep -E 'torch|torchvision|numpy|scipy|scikit-learn|pandas|nltk|pycocoevalcap|matplotlib'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nlgeval import NLGEval\nnlgeval = NLGEval()  # Should work without errors","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the best model for evaluation\nbest_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\nif os.path.exists(best_model_path):\n    print(f\"Loading best model from {best_model_path}\")\n    checkpoint = torch.load(best_model_path, map_location=device)\nelse:\n    # Fall back to latest checkpoint if no best model exists\n    latest_checkpoint = max(glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth')), \n                           key=os.path.getctime)\n    print(f\"No best model found, loading latest checkpoint: {latest_checkpoint}\")\n    checkpoint = torch.load(latest_checkpoint, map_location=device)\n\nencoder.load_state_dict(checkpoint['encoder_state_dict'])\ndecoder.load_state_dict(checkpoint['decoder_state_dict'])\n\n# Set models to evaluation mode\nencoder.eval()\ndecoder.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.meteor.meteor import Meteor\n\ndef evaluate_model(encoder, decoder, val_dataloader, word_map, device):\n    encoder.eval()\n    decoder.eval()\n    hypotheses = {}\n    references = defaultdict(list)\n    image_ids = []\n\n    with torch.no_grad():\n        for images, captions, lengths in val_dataloader:\n            images = images.to(device)\n            encoder_out = encoder(images)\n\n            for i in range(images.size(0)):\n                img_enc = encoder_out[i].unsqueeze(0)\n                img_id = val_dataloader.dataset.image_ids[i]\n                generated_caption = generate_caption(decoder, img_enc)\n                hypotheses[img_id] = [generated_caption]\n                image_ids.append(img_id)\n\n                # Collect all reference captions\n                for cap in encoded_val_captions[int(img_id)]:\n                    ref_tokens = [idx2word[idx] for idx in cap if idx not in {word_map['<pad>'], word_map['<start>'], word_map['<end>'], word_map['<unk>']}]\n                    references[img_id].append(' '.join(ref_tokens))\n\n    # Evaluate using pycocoevalcap\n    scorers = [\n        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n        (Rouge(), \"ROUGE_L\"),\n        (Meteor(), \"METEOR\")\n    ]\n    results = {}\n    for scorer, method in scorers:\n        score, _ = scorer.compute_score(references, hypotheses)\n        if isinstance(method, list):\n            for m, s in zip(method, score):\n                results[m] = s\n        else:\n            results[method] = score\n\n    return results\n\n# Run evaluation\nmetrics = evaluate_model(encoder, decoder, val_dataloader, word_map, device)\nlogger.info(\"Evaluation Metrics:\")\nfor metric, score in metrics.items():\n    logger.info(f\"{metric}: {score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(decoder, encoder_out, max_len=20, beam_size=3):\n    decoder.eval()\n    device = encoder_out.device\n    encoder_out = encoder_out.view(1, -1, encoder_out.size(-1))\n    word_map_rev = {v: k for k, v in word_map.items()}\n    \n    # Initialize beam\n    h, c = decoder.init_hidden_state(encoder_out)\n    start_token = torch.tensor([word_map['<start>']], device=device)\n    beams = [([start_token], (h, c), 0.0)]  # (sequence, state, score)\n    completed = []\n\n    for _ in range(max_len):\n        new_beams = []\n        for seq, (h, c), score in beams:\n            last_word = seq[-1]\n            if last_word.item() == word_map['<end>']:\n                completed.append((seq, score))\n                continue\n\n            embeddings = decoder.embedding(last_word).unsqueeze(0)\n            awe, _ = decoder.attention(encoder_out, h)\n            gate = decoder.sigmoid(decoder.f_beta(h))\n            awe = gate * awe\n            h, c = decoder.decode_step(torch.cat([embeddings.squeeze(1), awe], dim=1), (h, c))\n            preds = decoder.fc(h)\n            probs = torch.log_softmax(preds, dim=1).squeeze(0)\n\n            # Get top-k candidates\n            top_probs, top_words = probs.topk(beam_size)\n            for i in range(beam_size):\n                new_seq = seq + [top_words[i].unsqueeze(0)]\n                new_score = score + top_probs[i].item()\n                new_beams.append((new_seq, (h, c), new_score))\n\n        # Keep top beam_size beams\n        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:beam_size]\n\n        if len(completed) >= beam_size:\n            break\n\n    # Select best completed caption or fall back to best beam\n    if completed:\n        best_seq, best_score = max(completed, key=lambda x: x[1])\n    else:\n        best_seq, _, best_score = max(beams, key=lambda x: x[2])\n\n    # Convert to words\n    caption = [word_map_rev.get(word.item(), '<unk>') for word in best_seq[1:] if word.item() != word_map['<end>']]\n    return ' '.join(caption)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Attention Maps","metadata":{}},{"cell_type":"code","source":"def generate_caption_with_attention(decoder, encoder_out, word_map, max_len=20):\n    decoder.eval()\n\n    h, c = decoder.init_hidden_state(encoder_out)\n    encoder_out = encoder_out.view(1, -1, encoder_out.size(-1))\n    word = torch.tensor([word_map['<start>']]).to(device)\n\n    rev_word_map = {v: k for k, v in word_map.items()}\n\n    caption = []\n    alphas = []\n\n    for _ in range(max_len):\n        embeddings = decoder.embedding(word).unsqueeze(0)  # (1, 1, embed_dim)\n        awe, alpha = decoder.attention(encoder_out, h)\n        gate = decoder.sigmoid(decoder.f_beta(h))\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings.squeeze(1), awe], dim=1), (h, c))\n        preds = decoder.fc(h)\n        word = preds.argmax(1)\n\n        predicted_word = word.item()\n        if predicted_word == word_map['<end>']:\n            break\n\n        caption.append(rev_word_map.get(predicted_word, '<unk>'))\n        alphas.append(alpha.cpu().detach().numpy())\n\n    return caption, alphas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_filter\n\ndef visualize_attention(image_path, caption, alphas, smooth=True):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        image = image.resize([224, 224], Image.LANCZOS)\n    except FileNotFoundError:\n        logger.error(f\"Image file {image_path} not found\")\n        return\n\n    plt.figure(figsize=(15, 15))\n    for t in range(len(caption)):\n        plt.subplot(np.ceil(len(caption) / 5.), 5, t + 1)\n        plt.text(0, 1, '%s' % caption[t], color='black', backgroundcolor='white', fontsize=12)\n        plt.imshow(image)\n        alpha = alphas[t].reshape(14, 14)\n        if smooth:\n            alpha = gaussian_filter(alpha, sigma=1)\n        plt.imshow(alpha, alpha=0.6, extent=(0, 224, 224, 0), cmap='viridis')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load image\nimage_path = \"/kaggle/input/coco-2017-dataset/coco2017/val2017/000000391895.jpg\"\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Transform image\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\nimage_tensor = transform(image).unsqueeze(0).to(device)\n\n# Encode image\nencoder_out = encoder(image_tensor)\ncaption, alphas = generate_caption_with_attention(decoder, encoder_out, word_map)\n\n# Visualize\nvisualize_attention(image_path, caption, alphas)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}