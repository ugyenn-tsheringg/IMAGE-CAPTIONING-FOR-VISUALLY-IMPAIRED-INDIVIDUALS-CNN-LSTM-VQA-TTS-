{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52dbcdbb",
   "metadata": {
    "_cell_guid": "62deee2b-37ec-4dc8-8ab8-6e28389729dd",
    "_uuid": "d961bf5c-64df-40ca-bd69-4a4586029471",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-17T07:17:40.607581Z",
     "iopub.status.busy": "2025-04-17T07:17:40.607338Z",
     "iopub.status.idle": "2025-04-17T07:17:42.865180Z",
     "shell.execute_reply": "2025-04-17T07:17:42.863972Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.26516,
     "end_time": "2025-04-17T07:17:42.866816",
     "exception": false,
     "start_time": "2025-04-17T07:17:40.601656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image ID: 203564\n",
      "Captions:\n",
      "- A bicycle replica with a clock as the front wheel.\n",
      "- The bike has a clock as a tire.\n",
      "- A black metal bicycle with a clock inside the front wheel.\n",
      "- A bicycle figurine in which the front wheel is replaced with a clock\n",
      "\n",
      "- A clock with the appearance of the wheel of a bicycle \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '/kaggle/input/coco-2017-dataset/coco2017'\n",
    "ANNOTATION_FILE = os.path.join(DATA_DIR, 'annotations', 'captions_train2017.json')\n",
    "ANNOTATION_FILE2 = os.path.join(DATA_DIR, 'annotations', 'captions_val2017.json')\n",
    "IMAGE_FOLDER = os.path.join(DATA_DIR, 'train2017')\n",
    "IMAGE_FOLDER2 = os.path.join(DATA_DIR, 'val2017')\n",
    "\n",
    "# Load annotations\n",
    "with open(ANNOTATION_FILE, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Build a dictionary: image_id -> list of captions\n",
    "captions_dict = defaultdict(list)\n",
    "for ann in annotations['annotations']:\n",
    "    image_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    captions_dict[image_id].append(caption)\n",
    "\n",
    "# Check sample\n",
    "sample_image_id = list(captions_dict.keys())[0]\n",
    "print(f\"Image ID: {sample_image_id}\")\n",
    "print(\"Captions:\")\n",
    "for cap in captions_dict[sample_image_id]:\n",
    "    print(\"-\", cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3ffdcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:17:42.876660Z",
     "iopub.status.busy": "2025-04-17T07:17:42.876381Z",
     "iopub.status.idle": "2025-04-17T07:18:39.061164Z",
     "shell.execute_reply": "2025-04-17T07:18:39.060254Z"
    },
    "papermill": {
     "duration": 56.193383,
     "end_time": "2025-04-17T07:18:39.064763",
     "exception": false,
     "start_time": "2025-04-17T07:17:42.871380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Cleaned captions for image ID: 203564\n",
      "['a', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel']\n",
      "['the', 'bike', 'has', 'a', 'clock', 'as', 'a', 'tire']\n",
      "['a', 'black', 'metal', 'bicycle', 'with', 'a', 'clock', 'inside', 'the', 'front', 'wheel']\n",
      "['a', 'bicycle', 'figurine', 'in', 'which', 'the', 'front', 'wheel', 'is', 'replaced', 'with', 'a', 'clock']\n",
      "['a', 'clock', 'with', 'the', 'appearance', 'of', 'the', 'wheel', 'of', 'a', 'bicycle']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')  # for word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = caption.lower()                            # Lowercase\n",
    "    caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)        # Remove punctuation\n",
    "    caption = re.sub(r\"\\s+\", \" \", caption).strip()       # Trim extra spaces\n",
    "    return caption\n",
    "\n",
    "# Clean and tokenize all captions\n",
    "cleaned_captions_dict = {}\n",
    "for image_id, captions in captions_dict.items():\n",
    "    cleaned_captions = []\n",
    "    for cap in captions:\n",
    "        clean_cap = clean_caption(cap)\n",
    "        tokens = word_tokenize(clean_cap)\n",
    "        cleaned_captions.append(tokens)\n",
    "    cleaned_captions_dict[image_id] = cleaned_captions\n",
    "\n",
    "# Check cleaned sample\n",
    "print(\"Cleaned captions for image ID:\", sample_image_id)\n",
    "for cap in cleaned_captions_dict[sample_image_id]:\n",
    "    print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97cf917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:39.074175Z",
     "iopub.status.busy": "2025-04-17T07:18:39.073787Z",
     "iopub.status.idle": "2025-04-17T07:18:40.138019Z",
     "shell.execute_reply": "2025-04-17T07:18:40.137293Z"
    },
    "papermill": {
     "duration": 1.07037,
     "end_time": "2025-04-17T07:18:40.139410",
     "exception": false,
     "start_time": "2025-04-17T07:18:39.069040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10307\n",
      "Sample word map entries:\n",
      "<pad>: 0\n",
      "<start>: 1\n",
      "<end>: 2\n",
      "<unk>: 3\n",
      "a: 4\n",
      "bicycle: 5\n",
      "replica: 6\n",
      "with: 7\n",
      "clock: 8\n",
      "as: 9\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "min_word_freq = 5  # You can tune this\n",
    "word_freq = Counter()\n",
    "\n",
    "# Count word frequencies\n",
    "for captions in cleaned_captions_dict.values():\n",
    "    for tokens in captions:\n",
    "        word_freq.update(tokens)\n",
    "\n",
    "# Filter words below the threshold\n",
    "words = [word for word in word_freq if word_freq[word] >= min_word_freq]\n",
    "\n",
    "# Special tokens\n",
    "word_map = {\n",
    "    '<pad>': 0,\n",
    "    '<start>': 1,\n",
    "    '<end>': 2,\n",
    "    '<unk>': 3\n",
    "}\n",
    "\n",
    "# Add the remaining words\n",
    "for i, word in enumerate(words, start=4):\n",
    "    word_map[word] = i\n",
    "\n",
    "# Reverse map\n",
    "idx2word = {v: k for k, v in word_map.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_map)}\")\n",
    "print(\"Sample word map entries:\")\n",
    "for i, (word, idx) in enumerate(list(word_map.items())[:10]):\n",
    "    print(f\"{word}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd627a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:40.149534Z",
     "iopub.status.busy": "2025-04-17T07:18:40.149257Z",
     "iopub.status.idle": "2025-04-17T07:18:42.824566Z",
     "shell.execute_reply": "2025-04-17T07:18:42.823785Z"
    },
    "papermill": {
     "duration": 2.681794,
     "end_time": "2025-04-17T07:18:42.826094",
     "exception": false,
     "start_time": "2025-04-17T07:18:40.144300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded captions for image ID: 203564\n",
      "[1, 4, 5, 6, 7, 4, 8, 9, 10, 11, 12, 2]\n",
      "[1, 10, 13, 14, 4, 8, 9, 4, 15, 2]\n",
      "[1, 4, 16, 17, 5, 7, 4, 8, 18, 10, 11, 12, 2]\n",
      "[1, 4, 5, 19, 20, 21, 10, 11, 12, 22, 23, 7, 4, 8, 2]\n",
      "[1, 4, 8, 7, 10, 24, 25, 10, 12, 25, 4, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "encoded_captions = {}\n",
    "\n",
    "for image_id, captions in cleaned_captions_dict.items():\n",
    "    encoded = []\n",
    "    for tokens in captions:\n",
    "        # Encode each word or use <unk> if not in vocab\n",
    "        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n",
    "        # Add <start> and <end> tokens\n",
    "        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n",
    "        encoded.append(enc)\n",
    "    encoded_captions[image_id] = encoded\n",
    "\n",
    "# Check sample\n",
    "print(\"Encoded captions for image ID:\", sample_image_id)\n",
    "for cap in encoded_captions[sample_image_id]:\n",
    "    print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00bc07e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:42.836227Z",
     "iopub.status.busy": "2025-04-17T07:18:42.835922Z",
     "iopub.status.idle": "2025-04-17T07:18:47.409871Z",
     "shell.execute_reply": "2025-04-17T07:18:47.409155Z"
    },
    "papermill": {
     "duration": 4.580609,
     "end_time": "2025-04-17T07:18:47.411415",
     "exception": false,
     "start_time": "2025-04-17T07:18:42.830806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save encoded captions\n",
    "with open('encoded_captions.json', 'w') as f:\n",
    "    json.dump({str(k): v for k, v in encoded_captions.items()}, f)\n",
    "\n",
    "# Save word map\n",
    "with open('word_map.json', 'w') as f:\n",
    "    json.dump(word_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39493440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:47.421398Z",
     "iopub.status.busy": "2025-04-17T07:18:47.421170Z",
     "iopub.status.idle": "2025-04-17T07:18:55.286851Z",
     "shell.execute_reply": "2025-04-17T07:18:55.286123Z"
    },
    "papermill": {
     "duration": 7.872102,
     "end_time": "2025-04-17T07:18:55.288297",
     "exception": false,
     "start_time": "2025-04-17T07:18:47.416195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, image_folder, encoded_captions_file, word_map_file, transform=None):\n",
    "        # Load encoded captions and word map\n",
    "        with open(encoded_captions_file, 'r') as j:\n",
    "            self.captions = json.load(j)\n",
    "        with open(word_map_file, 'r') as j:\n",
    "            self.word_map = json.load(j)\n",
    "\n",
    "        self.image_folder = image_folder\n",
    "        self.image_ids = list(self.captions.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image_path = os.path.join(self.image_folder, f\"{int(image_id):012}.jpg\")\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Randomly select one caption for the image\n",
    "        caps = self.captions[image_id]\n",
    "        caption = random.choice(caps)\n",
    "        caption = torch.tensor(caption, dtype=torch.long)\n",
    "\n",
    "        return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d4a0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:55.298317Z",
     "iopub.status.busy": "2025-04-17T07:18:55.297996Z",
     "iopub.status.idle": "2025-04-17T07:18:55.302444Z",
     "shell.execute_reply": "2025-04-17T07:18:55.301825Z"
    },
    "papermill": {
     "duration": 0.010524,
     "end_time": "2025-04-17T07:18:55.303610",
     "exception": false,
     "start_time": "2025-04-17T07:18:55.293086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def caption_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batches of (image, caption) with variable-length captions.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    captions = []\n",
    "\n",
    "    for img, cap in batch:\n",
    "        images.append(img)\n",
    "        captions.append(cap)\n",
    "\n",
    "    # Stack images (they are all same size)\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Pad captions to the max length in the batch\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    max_len = max(lengths)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        padded_captions[i, :end] = cap[:end]\n",
    "\n",
    "    return images, padded_captions, torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bd4709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:55.312927Z",
     "iopub.status.busy": "2025-04-17T07:18:55.312702Z",
     "iopub.status.idle": "2025-04-17T07:18:57.769609Z",
     "shell.execute_reply": "2025-04-17T07:18:57.768658Z"
    },
    "papermill": {
     "duration": 2.463056,
     "end_time": "2025-04-17T07:18:57.770980",
     "exception": false,
     "start_time": "2025-04-17T07:18:55.307924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([4, 3, 256, 256])\n",
      "Caption batch shape: torch.Size([4, 13])\n",
      "Lengths: tensor([11, 11, 12, 13])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # from ImageNet\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "dataset = CaptionDataset(\n",
    "    image_folder='/kaggle/input/coco-2017-dataset/coco2017/train2017',\n",
    "    encoded_captions_file='encoded_captions.json',\n",
    "    word_map_file='word_map.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "# Check sample batch\n",
    "for images, captions, lengths in dataloader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Caption batch shape:\", captions.shape)\n",
    "    print(\"Lengths:\", lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd9e5de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:18:57.781176Z",
     "iopub.status.busy": "2025-04-17T07:18:57.780934Z",
     "iopub.status.idle": "2025-04-17T07:19:00.366668Z",
     "shell.execute_reply": "2025-04-17T07:19:00.365737Z"
    },
    "papermill": {
     "duration": 2.592472,
     "end_time": "2025-04-17T07:19:00.368325",
     "exception": false,
     "start_time": "2025-04-17T07:18:57.775853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------- Process Validation Captions -----------\n",
    "with open(ANNOTATION_FILE2, 'r') as f:\n",
    "    val_annotations = json.load(f)\n",
    "\n",
    "val_captions_dict = defaultdict(list)\n",
    "for ann in val_annotations['annotations']:\n",
    "    image_id = ann['image_id']\n",
    "    caption = ann['caption']\n",
    "    val_captions_dict[image_id].append(caption)\n",
    "\n",
    "# Clean and tokenize validation captions\n",
    "cleaned_val_captions_dict = {}\n",
    "for image_id, captions in val_captions_dict.items():\n",
    "    cleaned_captions = []\n",
    "    for cap in captions:\n",
    "        clean_cap = clean_caption(cap)\n",
    "        tokens = word_tokenize(clean_cap)\n",
    "        cleaned_captions.append(tokens)\n",
    "    cleaned_val_captions_dict[image_id] = cleaned_captions\n",
    "\n",
    "# Encode validation captions\n",
    "encoded_val_captions = {}\n",
    "for image_id, captions in cleaned_val_captions_dict.items():\n",
    "    encoded = []\n",
    "    for tokens in captions:\n",
    "        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n",
    "        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n",
    "        encoded.append(enc)\n",
    "    encoded_val_captions[image_id] = encoded\n",
    "\n",
    "# Save encoded val captions\n",
    "with open('encoded_captions_val.json', 'w') as f:\n",
    "    json.dump({str(k): v for k, v in encoded_val_captions.items()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4438f88c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:00.378645Z",
     "iopub.status.busy": "2025-04-17T07:19:00.378351Z",
     "iopub.status.idle": "2025-04-17T07:19:00.669674Z",
     "shell.execute_reply": "2025-04-17T07:19:00.668505Z"
    },
    "papermill": {
     "duration": 0.298108,
     "end_time": "2025-04-17T07:19:00.671358",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.373250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([4, 3, 256, 256])\n",
      "Caption batch shape: torch.Size([4, 11])\n",
      "Lengths: tensor([11, 11, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "# Validation dataset\n",
    "val_dataset = CaptionDataset(\n",
    "    image_folder='/kaggle/input/coco-2017-dataset/coco2017/val2017',\n",
    "    encoded_captions_file='encoded_captions_val.json',  # You need to create this\n",
    "    word_map_file='word_map.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "# Check sample batch\n",
    "for images, captions, lengths in dataloader:\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Caption batch shape:\", captions.shape)\n",
    "    print(\"Lengths:\", lengths)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d76fe",
   "metadata": {
    "papermill": {
     "duration": 0.004191,
     "end_time": "2025-04-17T07:19:00.680294",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.676103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e5bc79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:00.690070Z",
     "iopub.status.busy": "2025-04-17T07:19:00.689822Z",
     "iopub.status.idle": "2025-04-17T07:19:00.695979Z",
     "shell.execute_reply": "2025-04-17T07:19:00.695180Z"
    },
    "papermill": {
     "duration": 0.012524,
     "end_time": "2025-04-17T07:19:00.697178",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.684654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Linear layer to transform encoder's output\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # Combine them and produce scalar energy\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax over the pixels\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        encoder_out: encoded images, shape -> (batch_size, num_pixels, encoder_dim)\n",
    "        decoder_hidden: previous decoder hidden state, shape -> (batch_size, decoder_dim)\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c878bb0",
   "metadata": {
    "papermill": {
     "duration": 0.004164,
     "end_time": "2025-04-17T07:19:00.705673",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.701509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5673e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:00.715191Z",
     "iopub.status.busy": "2025-04-17T07:19:00.714988Z",
     "iopub.status.idle": "2025-04-17T07:19:00.724890Z",
     "shell.execute_reply": "2025-04-17T07:19:00.724262Z"
    },
    "papermill": {
     "duration": 0.015942,
     "end_time": "2025-04-17T07:19:00.725996",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.710054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # initialize hidden state\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # initialize cell state\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # create a gating scalar\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # output layer\n",
    "\n",
    "        self.init_weights()  # initialize weights\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, shape (batch_size, num_pixels, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, shape (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, shape (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths\n",
    "        # Corrected line\n",
    "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        decode_lengths = caption_lengths - 1\n",
    "\n",
    "        # Create tensors to hold word prediction scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(encoder_out.device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "\n",
    "            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n",
    "            h, c = self.decode_step(input_lstm, (h[:batch_size_t], c[:batch_size_t]))  # LSTM step\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0c88e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:00.735473Z",
     "iopub.status.busy": "2025-04-17T07:19:00.735272Z",
     "iopub.status.idle": "2025-04-17T07:19:00.740797Z",
     "shell.execute_reply": "2025-04-17T07:19:00.740079Z"
    },
    "papermill": {
     "duration": 0.01152,
     "end_time": "2025-04-17T07:19:00.741948",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.730428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super().__init__()\n",
    "        self.cnn = models.resnet101(pretrained=True)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune(fine_tune=False)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.cnn.conv1(images)\n",
    "        x = self.cnn.bn1(x)\n",
    "        x = self.cnn.relu(x)\n",
    "        x = self.cnn.maxpool(x)\n",
    "\n",
    "        x = self.cnn.layer1(x)\n",
    "        x = self.cnn.layer2(x)\n",
    "        x = self.cnn.layer3(x)\n",
    "        x = self.cnn.layer4(x)  # Shape: (batch_size, 2048, 7, 7)\n",
    "        \n",
    "        x = self.adaptive_pool(x)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch_size, encoded_size, encoded_size, 2048)\n",
    "        x = x.view(x.size(0), -1, x.size(-1))  # (batch_size, num_pixels=encoded_size^2, 2048)\n",
    "        return x\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        for p in self.cnn.parameters():\n",
    "            p.requires_grad = fine_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c760d965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:00.751146Z",
     "iopub.status.busy": "2025-04-17T07:19:00.750944Z",
     "iopub.status.idle": "2025-04-17T07:19:03.984063Z",
     "shell.execute_reply": "2025-04-17T07:19:03.983077Z"
    },
    "papermill": {
     "duration": 3.239281,
     "end_time": "2025-04-17T07:19:03.985519",
     "exception": false,
     "start_time": "2025-04-17T07:19:00.746238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:00<00:00, 230MB/s]\n",
      "<ipython-input-14-91d566bfc588>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([4, 196, 2048])\n",
      "Predictions shape: torch.Size([4, 14, 10307])\n"
     ]
    }
   ],
   "source": [
    "# Test encoder-decoder integration\n",
    "import torch\n",
    "\n",
    "# Check if GPU is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "decoder = DecoderWithAttention(\n",
    "    attention_dim=512,\n",
    "    embed_dim=512,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=len(word_map),\n",
    "    encoder_dim=2048,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Test forward pass\n",
    "images, captions, lengths = next(iter(dataloader))\n",
    "images = images.to(device)\n",
    "captions = captions.to(device)\n",
    "\n",
    "encoder_out = encoder(images)\n",
    "predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths))\n",
    "\n",
    "print(\"Encoder output shape:\", encoder_out.shape)  # Should be (batch_size, 196, 2048)\n",
    "print(\"Predictions shape:\", predictions.shape)     # Should be (batch_size, max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b432f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:03.997300Z",
     "iopub.status.busy": "2025-04-17T07:19:03.997025Z",
     "iopub.status.idle": "2025-04-17T07:19:04.002422Z",
     "shell.execute_reply": "2025-04-17T07:19:04.001585Z"
    },
    "papermill": {
     "duration": 0.012599,
     "end_time": "2025-04-17T07:19:04.003826",
     "exception": false,
     "start_time": "2025-04-17T07:19:03.991227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=0)  # ignore <pad>\n",
    "\n",
    "    def forward(self, predictions, targets, lengths):\n",
    "        batch_size, max_len, vocab_size = predictions.shape\n",
    "\n",
    "        predictions = predictions.view(-1, vocab_size)      # (batch_size * max_len, vocab_size)\n",
    "        targets = targets.contiguous().view(-1)              # (batch_size * max_len)\n",
    "\n",
    "        losses = self.criterion(predictions, targets)        # (batch_size * max_len)\n",
    "\n",
    "        # Create mask\n",
    "        mask = torch.arange(max_len).expand(batch_size, max_len).to(lengths.device)\n",
    "        mask = (mask < lengths.unsqueeze(1)).float()         # (batch_size, max_len)\n",
    "        mask = mask.view(-1)                                 # Flatten to (batch_size * max_len)\n",
    "\n",
    "        losses = losses * mask\n",
    "        return losses.sum() / mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "658a82a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:04.015004Z",
     "iopub.status.busy": "2025-04-17T07:19:04.014774Z",
     "iopub.status.idle": "2025-04-17T07:19:05.183336Z",
     "shell.execute_reply": "2025-04-17T07:19:05.182641Z"
    },
    "papermill": {
     "duration": 1.175846,
     "end_time": "2025-04-17T07:19:05.184918",
     "exception": false,
     "start_time": "2025-04-17T07:19:04.009072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = DecoderWithAttention(\n",
    "    attention_dim=512,\n",
    "    embed_dim=512,\n",
    "    decoder_dim=512,\n",
    "    vocab_size=len(word_map),\n",
    "    encoder_dim=2048,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "# Only fine-tune the encoder's adaptive pool layer\n",
    "encoder_params = list(encoder.adaptive_pool.parameters()) + list(encoder.cnn.layer4.parameters())\n",
    "decoder_params = decoder.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=[\n",
    "        {'params': encoder_params, 'lr': 1e-4},  # Lower LR for encoder\n",
    "        {'params': decoder_params, 'lr': 4e-4}    # Higher LR for decoder\n",
    "    ],\n",
    "    weight_decay=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9be69350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:05.197762Z",
     "iopub.status.busy": "2025-04-17T07:19:05.197500Z",
     "iopub.status.idle": "2025-04-17T07:19:05.203297Z",
     "shell.execute_reply": "2025-04-17T07:19:05.202512Z"
    },
    "papermill": {
     "duration": 0.013765,
     "end_time": "2025-04-17T07:19:05.204576",
     "exception": false,
     "start_time": "2025-04-17T07:19:05.190811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(encoder, decoder, dataloader, criterion, optimizer, device, grad_clip=5.0):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # lengths = torch.tensor(lengths).to(device)\n",
    "        lengths_tensor = torch.tensor(lengths).to(device)\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        encoder_out = encoder(images)\n",
    "        predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n",
    "        \n",
    "        # Remove <start> token and truncate to actual lengths\n",
    "        targets = captions[:, 1:]  # (batch_size, max_len-1)\n",
    "        predictions = predictions[:, :max(decode_lengths), :]  # (batch_size, actual_max_len, vocab_size)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, targets, lengths_tensor)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch [{i}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9e61cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:05.215698Z",
     "iopub.status.busy": "2025-04-17T07:19:05.215457Z",
     "iopub.status.idle": "2025-04-17T07:19:05.219911Z",
     "shell.execute_reply": "2025-04-17T07:19:05.219299Z"
    },
    "papermill": {
     "duration": 0.011229,
     "end_time": "2025-04-17T07:19:05.221085",
     "exception": false,
     "start_time": "2025-04-17T07:19:05.209856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, val_loader, criterion, device):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            # lengths = torch.tensor(lengths).to(device)\n",
    "            lengths_tensor = torch.tensor(lengths).to(device)\n",
    "\n",
    "            \n",
    "            encoder_out = encoder(images)\n",
    "            predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n",
    "            \n",
    "            targets = captions[:, 1:]\n",
    "            predictions = predictions[:, :max(decode_lengths), :]\n",
    "            \n",
    "            loss = criterion(predictions, targets, lengths_tensor)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2df736c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:05.232020Z",
     "iopub.status.busy": "2025-04-17T07:19:05.231818Z",
     "iopub.status.idle": "2025-04-17T07:19:05.660163Z",
     "shell.execute_reply": "2025-04-17T07:19:05.658993Z"
    },
    "papermill": {
     "duration": 0.435572,
     "end_time": "2025-04-17T07:19:05.661771",
     "exception": false,
     "start_time": "2025-04-17T07:19:05.226199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 9.0526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-9cf91de6609c>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_lengths = torch.tensor(test_lengths).to(device)\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "criterion = MaskedCrossEntropyLoss().to(device)\n",
    "num_epochs = 5  # For initial test\n",
    "\n",
    "# Quick test with 1 batch\n",
    "test_images, test_captions, test_lengths = next(iter(dataloader))\n",
    "test_images = test_images.to(device)\n",
    "test_captions = test_captions.to(device)\n",
    "test_lengths = torch.tensor(test_lengths).to(device)\n",
    "\n",
    "# Forward test\n",
    "encoder_out = encoder(test_images)\n",
    "predictions, _, decode_lengths, _, _ = decoder(encoder_out, test_captions, test_lengths)\n",
    "targets = test_captions[:, 1:]\n",
    "\n",
    "# Convert decode_lengths to tensor\n",
    "# decode_lengths = torch.tensor(decode_lengths).to(device)\n",
    "\n",
    "loss = criterion(predictions, targets, decode_lengths)\n",
    "# decode_lengths = torch.tensor(decode_lengths).to(device)\n",
    "# loss = criterion(predictions, targets, decode_lengths)\n",
    "\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")  # Should be ~log(vocab_size) = ~9.2 for vocab_size=10307\n",
    "optimizer.step()  # Verify backprop works without errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a2a9c1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:19:05.673895Z",
     "iopub.status.busy": "2025-04-17T07:19:05.673631Z",
     "iopub.status.idle": "2025-04-17T09:54:53.235938Z",
     "shell.execute_reply": "2025-04-17T09:54:53.234920Z"
    },
    "papermill": {
     "duration": 9347.570931,
     "end_time": "2025-04-17T09:54:53.238509",
     "exception": false,
     "start_time": "2025-04-17T07:19:05.667578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-47412d4627b6>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lengths_tensor = torch.tensor(lengths).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0/29572] Loss: 8.6444\n",
      "Batch [100/29572] Loss: 6.4333\n",
      "Batch [200/29572] Loss: 5.4490\n",
      "Batch [300/29572] Loss: 6.5647\n",
      "Batch [400/29572] Loss: 4.2135\n",
      "Batch [500/29572] Loss: 5.6773\n",
      "Batch [600/29572] Loss: 5.4023\n",
      "Batch [700/29572] Loss: 4.9603\n",
      "Batch [800/29572] Loss: 5.7843\n",
      "Batch [900/29572] Loss: 5.6119\n",
      "Batch [1000/29572] Loss: 5.3654\n",
      "Batch [1100/29572] Loss: 4.5122\n",
      "Batch [1200/29572] Loss: 4.6386\n",
      "Batch [1300/29572] Loss: 4.4022\n",
      "Batch [1400/29572] Loss: 5.3985\n",
      "Batch [1500/29572] Loss: 5.7319\n",
      "Batch [1600/29572] Loss: 5.2108\n",
      "Batch [1700/29572] Loss: 4.7169\n",
      "Batch [1800/29572] Loss: 4.6248\n",
      "Batch [1900/29572] Loss: 4.9519\n",
      "Batch [2000/29572] Loss: 4.9791\n",
      "Batch [2100/29572] Loss: 4.6899\n",
      "Batch [2200/29572] Loss: 5.5136\n",
      "Batch [2300/29572] Loss: 5.3614\n",
      "Batch [2400/29572] Loss: 5.1038\n",
      "Batch [2500/29572] Loss: 4.6971\n",
      "Batch [2600/29572] Loss: 4.8693\n",
      "Batch [2700/29572] Loss: 5.4848\n",
      "Batch [2800/29572] Loss: 4.0599\n",
      "Batch [2900/29572] Loss: 5.3955\n",
      "Batch [3000/29572] Loss: 4.8505\n",
      "Batch [3100/29572] Loss: 5.1989\n",
      "Batch [3200/29572] Loss: 5.5333\n",
      "Batch [3300/29572] Loss: 5.0484\n",
      "Batch [3400/29572] Loss: 5.2311\n",
      "Batch [3500/29572] Loss: 5.1329\n",
      "Batch [3600/29572] Loss: 5.6627\n",
      "Batch [3700/29572] Loss: 5.1637\n",
      "Batch [3800/29572] Loss: 4.2128\n",
      "Batch [3900/29572] Loss: 5.5152\n",
      "Batch [4000/29572] Loss: 4.7004\n",
      "Batch [4100/29572] Loss: 5.0859\n",
      "Batch [4200/29572] Loss: 5.0856\n",
      "Batch [4300/29572] Loss: 4.5041\n",
      "Batch [4400/29572] Loss: 4.9187\n",
      "Batch [4500/29572] Loss: 4.7979\n",
      "Batch [4600/29572] Loss: 5.6835\n",
      "Batch [4700/29572] Loss: 5.0808\n",
      "Batch [4800/29572] Loss: 5.0664\n",
      "Batch [4900/29572] Loss: 5.5579\n",
      "Batch [5000/29572] Loss: 4.8322\n",
      "Batch [5100/29572] Loss: 5.5586\n",
      "Batch [5200/29572] Loss: 4.8422\n",
      "Batch [5300/29572] Loss: 5.0160\n",
      "Batch [5400/29572] Loss: 6.0896\n",
      "Batch [5500/29572] Loss: 4.8783\n",
      "Batch [5600/29572] Loss: 4.6358\n",
      "Batch [5700/29572] Loss: 4.9689\n",
      "Batch [5800/29572] Loss: 4.5589\n",
      "Batch [5900/29572] Loss: 5.8558\n",
      "Batch [6000/29572] Loss: 4.7413\n",
      "Batch [6100/29572] Loss: 5.9285\n",
      "Batch [6200/29572] Loss: 5.1779\n",
      "Batch [6300/29572] Loss: 4.3848\n",
      "Batch [6400/29572] Loss: 5.8799\n",
      "Batch [6500/29572] Loss: 4.8149\n",
      "Batch [6600/29572] Loss: 3.9823\n",
      "Batch [6700/29572] Loss: 4.3294\n",
      "Batch [6800/29572] Loss: 4.6912\n",
      "Batch [6900/29572] Loss: 5.5672\n",
      "Batch [7000/29572] Loss: 5.3992\n",
      "Batch [7100/29572] Loss: 4.3732\n",
      "Batch [7200/29572] Loss: 4.0790\n",
      "Batch [7300/29572] Loss: 5.4979\n",
      "Batch [7400/29572] Loss: 4.6879\n",
      "Batch [7500/29572] Loss: 5.2806\n",
      "Batch [7600/29572] Loss: 5.0829\n",
      "Batch [7700/29572] Loss: 5.3965\n",
      "Batch [7800/29572] Loss: 5.6828\n",
      "Batch [7900/29572] Loss: 4.5298\n",
      "Batch [8000/29572] Loss: 5.1380\n",
      "Batch [8100/29572] Loss: 4.0279\n",
      "Batch [8200/29572] Loss: 4.3552\n",
      "Batch [8300/29572] Loss: 4.8857\n",
      "Batch [8400/29572] Loss: 5.1888\n",
      "Batch [8500/29572] Loss: 5.4861\n",
      "Batch [8600/29572] Loss: 5.1070\n",
      "Batch [8700/29572] Loss: 5.5993\n",
      "Batch [8800/29572] Loss: 4.5700\n",
      "Batch [8900/29572] Loss: 4.2195\n",
      "Batch [9000/29572] Loss: 5.6024\n",
      "Batch [9100/29572] Loss: 4.6717\n",
      "Batch [9200/29572] Loss: 4.2574\n",
      "Batch [9300/29572] Loss: 4.9297\n",
      "Batch [9400/29572] Loss: 4.8255\n",
      "Batch [9500/29572] Loss: 5.7950\n",
      "Batch [9600/29572] Loss: 5.4543\n",
      "Batch [9700/29572] Loss: 5.2475\n",
      "Batch [9800/29572] Loss: 5.3700\n",
      "Batch [9900/29572] Loss: 4.2490\n",
      "Batch [10000/29572] Loss: 4.5785\n",
      "Batch [10100/29572] Loss: 4.9365\n",
      "Batch [10200/29572] Loss: 5.4898\n",
      "Batch [10300/29572] Loss: 4.9734\n",
      "Batch [10400/29572] Loss: 4.8399\n",
      "Batch [10500/29572] Loss: 4.4798\n",
      "Batch [10600/29572] Loss: 5.1349\n",
      "Batch [10700/29572] Loss: 5.4080\n",
      "Batch [10800/29572] Loss: 4.6561\n",
      "Batch [10900/29572] Loss: 5.5678\n",
      "Batch [11000/29572] Loss: 5.2894\n",
      "Batch [11100/29572] Loss: 5.6865\n",
      "Batch [11200/29572] Loss: 4.5623\n",
      "Batch [11300/29572] Loss: 5.1839\n",
      "Batch [11400/29572] Loss: 5.2821\n",
      "Batch [11500/29572] Loss: 4.8373\n",
      "Batch [11600/29572] Loss: 5.8105\n",
      "Batch [11700/29572] Loss: 5.1277\n",
      "Batch [11800/29572] Loss: 4.3004\n",
      "Batch [11900/29572] Loss: 4.6482\n",
      "Batch [12000/29572] Loss: 5.2040\n",
      "Batch [12100/29572] Loss: 5.2481\n",
      "Batch [12200/29572] Loss: 5.3495\n",
      "Batch [12300/29572] Loss: 4.8125\n",
      "Batch [12400/29572] Loss: 5.0812\n",
      "Batch [12500/29572] Loss: 5.1161\n",
      "Batch [12600/29572] Loss: 4.9063\n",
      "Batch [12700/29572] Loss: 5.3182\n",
      "Batch [12800/29572] Loss: 4.6196\n",
      "Batch [12900/29572] Loss: 4.6264\n",
      "Batch [13000/29572] Loss: 4.8300\n",
      "Batch [13100/29572] Loss: 4.6920\n",
      "Batch [13200/29572] Loss: 5.6264\n",
      "Batch [13300/29572] Loss: 4.6753\n",
      "Batch [13400/29572] Loss: 6.1202\n",
      "Batch [13500/29572] Loss: 4.7891\n",
      "Batch [13600/29572] Loss: 4.9563\n",
      "Batch [13700/29572] Loss: 4.7816\n",
      "Batch [13800/29572] Loss: 5.7155\n",
      "Batch [13900/29572] Loss: 4.5676\n",
      "Batch [14000/29572] Loss: 5.4048\n",
      "Batch [14100/29572] Loss: 6.0647\n",
      "Batch [14200/29572] Loss: 4.9631\n",
      "Batch [14300/29572] Loss: 4.6003\n",
      "Batch [14400/29572] Loss: 5.2080\n",
      "Batch [14500/29572] Loss: 5.2653\n",
      "Batch [14600/29572] Loss: 5.4947\n",
      "Batch [14700/29572] Loss: 4.0207\n",
      "Batch [14800/29572] Loss: 4.9571\n",
      "Batch [14900/29572] Loss: 5.4993\n",
      "Batch [15000/29572] Loss: 5.6468\n",
      "Batch [15100/29572] Loss: 5.2350\n",
      "Batch [15200/29572] Loss: 4.5878\n",
      "Batch [15300/29572] Loss: 4.7931\n",
      "Batch [15400/29572] Loss: 5.3430\n",
      "Batch [15500/29572] Loss: 4.7740\n",
      "Batch [15600/29572] Loss: 3.9268\n",
      "Batch [15700/29572] Loss: 4.5142\n",
      "Batch [15800/29572] Loss: 4.5333\n",
      "Batch [15900/29572] Loss: 5.1793\n",
      "Batch [16000/29572] Loss: 4.1319\n",
      "Batch [16100/29572] Loss: 4.6545\n",
      "Batch [16200/29572] Loss: 4.7485\n",
      "Batch [16300/29572] Loss: 5.4352\n",
      "Batch [16400/29572] Loss: 4.9393\n",
      "Batch [16500/29572] Loss: 4.4975\n",
      "Batch [16600/29572] Loss: 5.3924\n",
      "Batch [16700/29572] Loss: 5.5252\n",
      "Batch [16800/29572] Loss: 4.2064\n",
      "Batch [16900/29572] Loss: 4.6942\n",
      "Batch [17000/29572] Loss: 5.4847\n",
      "Batch [17100/29572] Loss: 3.6391\n",
      "Batch [17200/29572] Loss: 5.5577\n",
      "Batch [17300/29572] Loss: 5.0594\n",
      "Batch [17400/29572] Loss: 5.6551\n",
      "Batch [17500/29572] Loss: 4.4728\n",
      "Batch [17600/29572] Loss: 5.5180\n",
      "Batch [17700/29572] Loss: 5.2515\n",
      "Batch [17800/29572] Loss: 5.3579\n",
      "Batch [17900/29572] Loss: 5.0275\n",
      "Batch [18000/29572] Loss: 4.7432\n",
      "Batch [18100/29572] Loss: 5.2936\n",
      "Batch [18200/29572] Loss: 5.1459\n",
      "Batch [18300/29572] Loss: 4.8393\n",
      "Batch [18400/29572] Loss: 5.4929\n",
      "Batch [18500/29572] Loss: 5.2140\n",
      "Batch [18600/29572] Loss: 5.1867\n",
      "Batch [18700/29572] Loss: 4.7594\n",
      "Batch [18800/29572] Loss: 5.9755\n",
      "Batch [18900/29572] Loss: 5.1191\n",
      "Batch [19000/29572] Loss: 5.3346\n",
      "Batch [19100/29572] Loss: 4.9142\n",
      "Batch [19200/29572] Loss: 5.5100\n",
      "Batch [19300/29572] Loss: 5.2582\n",
      "Batch [19400/29572] Loss: 5.0514\n",
      "Batch [19500/29572] Loss: 5.2959\n",
      "Batch [19600/29572] Loss: 5.6378\n",
      "Batch [19700/29572] Loss: 4.9691\n",
      "Batch [19800/29572] Loss: 5.9306\n",
      "Batch [19900/29572] Loss: 5.8389\n",
      "Batch [20000/29572] Loss: 5.6577\n",
      "Batch [20100/29572] Loss: 5.5814\n",
      "Batch [20200/29572] Loss: 4.2310\n",
      "Batch [20300/29572] Loss: 5.9663\n",
      "Batch [20400/29572] Loss: 5.4994\n",
      "Batch [20500/29572] Loss: 5.5435\n",
      "Batch [20600/29572] Loss: 5.7134\n",
      "Batch [20700/29572] Loss: 4.3933\n",
      "Batch [20800/29572] Loss: 5.2792\n",
      "Batch [20900/29572] Loss: 5.5143\n",
      "Batch [21000/29572] Loss: 3.9309\n",
      "Batch [21100/29572] Loss: 5.1529\n",
      "Batch [21200/29572] Loss: 4.7434\n",
      "Batch [21300/29572] Loss: 5.1897\n",
      "Batch [21400/29572] Loss: 5.1485\n",
      "Batch [21500/29572] Loss: 4.4910\n",
      "Batch [21600/29572] Loss: 4.8700\n",
      "Batch [21700/29572] Loss: 4.9607\n",
      "Batch [21800/29572] Loss: 4.7970\n",
      "Batch [21900/29572] Loss: 5.4392\n",
      "Batch [22000/29572] Loss: 4.7570\n",
      "Batch [22100/29572] Loss: 5.2286\n",
      "Batch [22200/29572] Loss: 5.1277\n",
      "Batch [22300/29572] Loss: 5.4647\n",
      "Batch [22400/29572] Loss: 4.9255\n",
      "Batch [22500/29572] Loss: 5.9602\n",
      "Batch [22600/29572] Loss: 5.6875\n",
      "Batch [22700/29572] Loss: 5.3849\n",
      "Batch [22800/29572] Loss: 5.4915\n",
      "Batch [22900/29572] Loss: 5.1282\n",
      "Batch [23000/29572] Loss: 6.2827\n",
      "Batch [23100/29572] Loss: 4.6986\n",
      "Batch [23200/29572] Loss: 4.6551\n",
      "Batch [23300/29572] Loss: 5.1511\n",
      "Batch [23400/29572] Loss: 5.7358\n",
      "Batch [23500/29572] Loss: 4.8969\n",
      "Batch [23600/29572] Loss: 4.6369\n",
      "Batch [23700/29572] Loss: 5.1376\n",
      "Batch [23800/29572] Loss: 5.5562\n",
      "Batch [23900/29572] Loss: 4.3824\n",
      "Batch [24000/29572] Loss: 4.1316\n",
      "Batch [24100/29572] Loss: 4.8611\n",
      "Batch [24200/29572] Loss: 4.0528\n",
      "Batch [24300/29572] Loss: 5.0193\n",
      "Batch [24400/29572] Loss: 5.5707\n",
      "Batch [24500/29572] Loss: 5.6004\n",
      "Batch [24600/29572] Loss: 4.7210\n",
      "Batch [24700/29572] Loss: 4.7419\n",
      "Batch [24800/29572] Loss: 5.5300\n",
      "Batch [24900/29572] Loss: 5.1630\n",
      "Batch [25000/29572] Loss: 5.9572\n",
      "Batch [25100/29572] Loss: 6.0213\n",
      "Batch [25200/29572] Loss: 5.6873\n",
      "Batch [25300/29572] Loss: 5.0558\n",
      "Batch [25400/29572] Loss: 5.0835\n",
      "Batch [25500/29572] Loss: 5.5434\n",
      "Batch [25600/29572] Loss: 4.3099\n",
      "Batch [25700/29572] Loss: 4.8592\n",
      "Batch [25800/29572] Loss: 5.3614\n",
      "Batch [25900/29572] Loss: 4.8237\n",
      "Batch [26000/29572] Loss: 5.2167\n",
      "Batch [26100/29572] Loss: 4.6761\n",
      "Batch [26200/29572] Loss: 5.4015\n",
      "Batch [26300/29572] Loss: 4.4308\n",
      "Batch [26400/29572] Loss: 4.8455\n",
      "Batch [26500/29572] Loss: 5.1515\n",
      "Batch [26600/29572] Loss: 4.8955\n",
      "Batch [26700/29572] Loss: 5.1782\n",
      "Batch [26800/29572] Loss: 3.9012\n",
      "Batch [26900/29572] Loss: 5.3136\n",
      "Batch [27000/29572] Loss: 4.7438\n",
      "Batch [27100/29572] Loss: 4.6558\n",
      "Batch [27200/29572] Loss: 4.6964\n",
      "Batch [27300/29572] Loss: 6.1633\n",
      "Batch [27400/29572] Loss: 5.5987\n",
      "Batch [27500/29572] Loss: 4.8994\n",
      "Batch [27600/29572] Loss: 5.6546\n",
      "Batch [27700/29572] Loss: 3.4685\n",
      "Batch [27800/29572] Loss: 5.7277\n",
      "Batch [27900/29572] Loss: 4.8978\n",
      "Batch [28000/29572] Loss: 4.6792\n",
      "Batch [28100/29572] Loss: 6.0347\n",
      "Batch [28200/29572] Loss: 4.1991\n",
      "Batch [28300/29572] Loss: 5.4437\n",
      "Batch [28400/29572] Loss: 5.1910\n",
      "Batch [28500/29572] Loss: 5.0627\n",
      "Batch [28600/29572] Loss: 5.0022\n",
      "Batch [28700/29572] Loss: 5.7828\n",
      "Batch [28800/29572] Loss: 4.9670\n",
      "Batch [28900/29572] Loss: 4.2115\n",
      "Batch [29000/29572] Loss: 5.3172\n",
      "Batch [29100/29572] Loss: 5.0400\n",
      "Batch [29200/29572] Loss: 4.4770\n",
      "Batch [29300/29572] Loss: 4.5148\n",
      "Batch [29400/29572] Loss: 5.0242\n",
      "Batch [29500/29572] Loss: 5.1131\n",
      "Train Loss: 5.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-077716cb4c09>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lengths_tensor = torch.tensor(lengths).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2 ---\n",
      "Batch [0/29572] Loss: 5.0782\n",
      "Batch [100/29572] Loss: 5.2417\n",
      "Batch [200/29572] Loss: 4.5414\n",
      "Batch [300/29572] Loss: 4.9663\n",
      "Batch [400/29572] Loss: 4.2598\n",
      "Batch [500/29572] Loss: 4.7526\n",
      "Batch [600/29572] Loss: 3.8190\n",
      "Batch [700/29572] Loss: 4.8924\n",
      "Batch [800/29572] Loss: 5.2121\n",
      "Batch [900/29572] Loss: 4.2045\n",
      "Batch [1000/29572] Loss: 4.1200\n",
      "Batch [1100/29572] Loss: 4.0981\n",
      "Batch [1200/29572] Loss: 5.1626\n",
      "Batch [1300/29572] Loss: 5.3859\n",
      "Batch [1400/29572] Loss: 5.3284\n",
      "Batch [1500/29572] Loss: 4.9445\n",
      "Batch [1600/29572] Loss: 5.3362\n",
      "Batch [1700/29572] Loss: 4.5177\n",
      "Batch [1800/29572] Loss: 4.7029\n",
      "Batch [1900/29572] Loss: 4.8125\n",
      "Batch [2000/29572] Loss: 5.0534\n",
      "Batch [2100/29572] Loss: 4.6046\n",
      "Batch [2200/29572] Loss: 5.0933\n",
      "Batch [2300/29572] Loss: 5.2704\n",
      "Batch [2400/29572] Loss: 5.7575\n",
      "Batch [2500/29572] Loss: 4.6535\n",
      "Batch [2600/29572] Loss: 4.6617\n",
      "Batch [2700/29572] Loss: 5.0252\n",
      "Batch [2800/29572] Loss: 4.0284\n",
      "Batch [2900/29572] Loss: 5.8273\n",
      "Batch [3000/29572] Loss: 4.3100\n",
      "Batch [3100/29572] Loss: 5.4338\n",
      "Batch [3200/29572] Loss: 5.3353\n",
      "Batch [3300/29572] Loss: 5.7614\n",
      "Batch [3400/29572] Loss: 4.2581\n",
      "Batch [3500/29572] Loss: 4.5239\n",
      "Batch [3600/29572] Loss: 5.1513\n",
      "Batch [3700/29572] Loss: 4.1946\n",
      "Batch [3800/29572] Loss: 3.8723\n",
      "Batch [3900/29572] Loss: 4.4128\n",
      "Batch [4000/29572] Loss: 5.0311\n",
      "Batch [4100/29572] Loss: 4.1358\n",
      "Batch [4200/29572] Loss: 4.9791\n",
      "Batch [4300/29572] Loss: 5.0766\n",
      "Batch [4400/29572] Loss: 4.4382\n",
      "Batch [4500/29572] Loss: 5.1233\n",
      "Batch [4600/29572] Loss: 4.9545\n",
      "Batch [4700/29572] Loss: 4.6659\n",
      "Batch [4800/29572] Loss: 4.3413\n",
      "Batch [4900/29572] Loss: 3.3899\n",
      "Batch [5000/29572] Loss: 4.7664\n",
      "Batch [5100/29572] Loss: 5.1998\n",
      "Batch [5200/29572] Loss: 5.1598\n",
      "Batch [5300/29572] Loss: 4.9538\n",
      "Batch [5400/29572] Loss: 6.0720\n",
      "Batch [5500/29572] Loss: 4.3118\n",
      "Batch [5600/29572] Loss: 5.5536\n",
      "Batch [5700/29572] Loss: 5.4919\n",
      "Batch [5800/29572] Loss: 4.9439\n",
      "Batch [5900/29572] Loss: 4.5334\n",
      "Batch [6000/29572] Loss: 4.4654\n",
      "Batch [6100/29572] Loss: 5.1622\n",
      "Batch [6200/29572] Loss: 5.3801\n",
      "Batch [6300/29572] Loss: 3.6673\n",
      "Batch [6400/29572] Loss: 4.3802\n",
      "Batch [6500/29572] Loss: 5.1780\n",
      "Batch [6600/29572] Loss: 4.9550\n",
      "Batch [6700/29572] Loss: 5.1876\n",
      "Batch [6800/29572] Loss: 6.0722\n",
      "Batch [6900/29572] Loss: 5.0538\n",
      "Batch [7000/29572] Loss: 5.0865\n",
      "Batch [7100/29572] Loss: 4.8581\n",
      "Batch [7200/29572] Loss: 5.8951\n",
      "Batch [7300/29572] Loss: 5.1777\n",
      "Batch [7400/29572] Loss: 5.2044\n",
      "Batch [7500/29572] Loss: 4.5714\n",
      "Batch [7600/29572] Loss: 5.2579\n",
      "Batch [7700/29572] Loss: 5.3012\n",
      "Batch [7800/29572] Loss: 4.8920\n",
      "Batch [7900/29572] Loss: 4.2694\n",
      "Batch [8000/29572] Loss: 3.3687\n",
      "Batch [8100/29572] Loss: 4.9947\n",
      "Batch [8200/29572] Loss: 5.4768\n",
      "Batch [8300/29572] Loss: 4.4903\n",
      "Batch [8400/29572] Loss: 3.7740\n",
      "Batch [8500/29572] Loss: 4.4923\n",
      "Batch [8600/29572] Loss: 4.7269\n",
      "Batch [8700/29572] Loss: 4.6598\n",
      "Batch [8800/29572] Loss: 6.9546\n",
      "Batch [8900/29572] Loss: 4.5046\n",
      "Batch [9000/29572] Loss: 5.1492\n",
      "Batch [9100/29572] Loss: 5.2953\n",
      "Batch [9200/29572] Loss: 5.8404\n",
      "Batch [9300/29572] Loss: 5.1179\n",
      "Batch [9400/29572] Loss: 4.4352\n",
      "Batch [9500/29572] Loss: 4.4539\n",
      "Batch [9600/29572] Loss: 4.7778\n",
      "Batch [9700/29572] Loss: 5.3435\n",
      "Batch [9800/29572] Loss: 5.2886\n",
      "Batch [9900/29572] Loss: 5.8172\n",
      "Batch [10000/29572] Loss: 4.5136\n",
      "Batch [10100/29572] Loss: 4.8681\n",
      "Batch [10200/29572] Loss: 4.7079\n",
      "Batch [10300/29572] Loss: 5.3312\n",
      "Batch [10400/29572] Loss: 4.1882\n",
      "Batch [10500/29572] Loss: 4.5277\n",
      "Batch [10600/29572] Loss: 5.0552\n",
      "Batch [10700/29572] Loss: 4.8756\n",
      "Batch [10800/29572] Loss: 5.3073\n",
      "Batch [10900/29572] Loss: 4.7942\n",
      "Batch [11000/29572] Loss: 5.2270\n",
      "Batch [11100/29572] Loss: 4.8098\n",
      "Batch [11200/29572] Loss: 5.3456\n",
      "Batch [11300/29572] Loss: 5.2712\n",
      "Batch [11400/29572] Loss: 5.3657\n",
      "Batch [11500/29572] Loss: 5.3731\n",
      "Batch [11600/29572] Loss: 4.2747\n",
      "Batch [11700/29572] Loss: 4.0262\n",
      "Batch [11800/29572] Loss: 5.4617\n",
      "Batch [11900/29572] Loss: 5.3478\n",
      "Batch [12000/29572] Loss: 5.8328\n",
      "Batch [12100/29572] Loss: 4.5203\n",
      "Batch [12200/29572] Loss: 5.0039\n",
      "Batch [12300/29572] Loss: 4.9898\n",
      "Batch [12400/29572] Loss: 4.9132\n",
      "Batch [12500/29572] Loss: 5.6571\n",
      "Batch [12600/29572] Loss: 5.5679\n",
      "Batch [12700/29572] Loss: 6.3240\n",
      "Batch [12800/29572] Loss: 5.5600\n",
      "Batch [12900/29572] Loss: 5.1132\n",
      "Batch [13000/29572] Loss: 4.4596\n",
      "Batch [13100/29572] Loss: 5.3147\n",
      "Batch [13200/29572] Loss: 3.8971\n",
      "Batch [13300/29572] Loss: 5.4096\n",
      "Batch [13400/29572] Loss: 5.0779\n",
      "Batch [13500/29572] Loss: 5.6574\n",
      "Batch [13600/29572] Loss: 5.0592\n",
      "Batch [13700/29572] Loss: 5.1527\n",
      "Batch [13800/29572] Loss: 5.0067\n",
      "Batch [13900/29572] Loss: 5.0789\n",
      "Batch [14000/29572] Loss: 4.5250\n",
      "Batch [14100/29572] Loss: 4.7522\n",
      "Batch [14200/29572] Loss: 3.7611\n",
      "Batch [14300/29572] Loss: 5.5914\n",
      "Batch [14400/29572] Loss: 5.0218\n",
      "Batch [14500/29572] Loss: 4.3315\n",
      "Batch [14600/29572] Loss: 5.2139\n",
      "Batch [14700/29572] Loss: 4.5493\n",
      "Batch [14800/29572] Loss: 4.4709\n",
      "Batch [14900/29572] Loss: 5.3579\n",
      "Batch [15000/29572] Loss: 4.8410\n",
      "Batch [15100/29572] Loss: 5.0910\n",
      "Batch [15200/29572] Loss: 4.4419\n",
      "Batch [15300/29572] Loss: 4.9460\n",
      "Batch [15400/29572] Loss: 4.2880\n",
      "Batch [15500/29572] Loss: 4.9156\n",
      "Batch [15600/29572] Loss: 5.3389\n",
      "Batch [15700/29572] Loss: 5.6642\n",
      "Batch [15800/29572] Loss: 4.9518\n",
      "Batch [15900/29572] Loss: 5.1067\n",
      "Batch [16000/29572] Loss: 5.5730\n",
      "Batch [16100/29572] Loss: 6.0130\n",
      "Batch [16200/29572] Loss: 3.6508\n",
      "Batch [16300/29572] Loss: 5.2121\n",
      "Batch [16400/29572] Loss: 3.1700\n",
      "Batch [16500/29572] Loss: 4.6891\n",
      "Batch [16600/29572] Loss: 5.9365\n",
      "Batch [16700/29572] Loss: 4.9200\n",
      "Batch [16800/29572] Loss: 5.7308\n",
      "Batch [16900/29572] Loss: 5.2006\n",
      "Batch [17000/29572] Loss: 5.1837\n",
      "Batch [17100/29572] Loss: 5.2519\n",
      "Batch [17200/29572] Loss: 5.4705\n",
      "Batch [17300/29572] Loss: 5.6248\n",
      "Batch [17400/29572] Loss: 4.3483\n",
      "Batch [17500/29572] Loss: 3.9635\n",
      "Batch [17600/29572] Loss: 3.3981\n",
      "Batch [17700/29572] Loss: 5.2118\n",
      "Batch [17800/29572] Loss: 5.0603\n",
      "Batch [17900/29572] Loss: 5.0481\n",
      "Batch [18000/29572] Loss: 4.5499\n",
      "Batch [18100/29572] Loss: 5.0593\n",
      "Batch [18200/29572] Loss: 4.2913\n",
      "Batch [18300/29572] Loss: 4.4397\n",
      "Batch [18400/29572] Loss: 5.0911\n",
      "Batch [18500/29572] Loss: 5.0658\n",
      "Batch [18600/29572] Loss: 5.2161\n",
      "Batch [18700/29572] Loss: 5.8034\n",
      "Batch [18800/29572] Loss: 4.9556\n",
      "Batch [18900/29572] Loss: 4.6205\n",
      "Batch [19000/29572] Loss: 5.0394\n",
      "Batch [19100/29572] Loss: 4.6836\n",
      "Batch [19200/29572] Loss: 5.6426\n",
      "Batch [19300/29572] Loss: 4.6119\n",
      "Batch [19400/29572] Loss: 5.7815\n",
      "Batch [19500/29572] Loss: 4.1982\n",
      "Batch [19600/29572] Loss: 4.1711\n",
      "Batch [19700/29572] Loss: 4.4596\n",
      "Batch [19800/29572] Loss: 4.9286\n",
      "Batch [19900/29572] Loss: 4.6825\n",
      "Batch [20000/29572] Loss: 5.1494\n",
      "Batch [20100/29572] Loss: 3.9059\n",
      "Batch [20200/29572] Loss: 4.8154\n",
      "Batch [20300/29572] Loss: 5.0417\n",
      "Batch [20400/29572] Loss: 4.9348\n",
      "Batch [20500/29572] Loss: 5.5413\n",
      "Batch [20600/29572] Loss: 5.2951\n",
      "Batch [20700/29572] Loss: 4.1725\n",
      "Batch [20800/29572] Loss: 5.1578\n",
      "Batch [20900/29572] Loss: 4.0620\n",
      "Batch [21000/29572] Loss: 3.9385\n",
      "Batch [21100/29572] Loss: 4.5204\n",
      "Batch [21200/29572] Loss: 4.3722\n",
      "Batch [21300/29572] Loss: 4.9894\n",
      "Batch [21400/29572] Loss: 5.0701\n",
      "Batch [21500/29572] Loss: 4.8977\n",
      "Batch [21600/29572] Loss: 4.7654\n",
      "Batch [21700/29572] Loss: 5.5223\n",
      "Batch [21800/29572] Loss: 3.6042\n",
      "Batch [21900/29572] Loss: 5.8714\n",
      "Batch [22000/29572] Loss: 5.0520\n",
      "Batch [22100/29572] Loss: 5.4874\n",
      "Batch [22200/29572] Loss: 5.1223\n",
      "Batch [22300/29572] Loss: 4.4477\n",
      "Batch [22400/29572] Loss: 5.3627\n",
      "Batch [22500/29572] Loss: 4.3109\n",
      "Batch [22600/29572] Loss: 5.4140\n",
      "Batch [22700/29572] Loss: 4.7419\n",
      "Batch [22800/29572] Loss: 4.4983\n",
      "Batch [22900/29572] Loss: 4.5100\n",
      "Batch [23000/29572] Loss: 5.2183\n",
      "Batch [23100/29572] Loss: 4.6683\n",
      "Batch [23200/29572] Loss: 4.7514\n",
      "Batch [23300/29572] Loss: 4.1375\n",
      "Batch [23400/29572] Loss: 5.2387\n",
      "Batch [23500/29572] Loss: 4.2010\n",
      "Batch [23600/29572] Loss: 4.1679\n",
      "Batch [23700/29572] Loss: 5.0378\n",
      "Batch [23800/29572] Loss: 4.7098\n",
      "Batch [23900/29572] Loss: 4.4765\n",
      "Batch [24000/29572] Loss: 4.6056\n",
      "Batch [24100/29572] Loss: 3.6121\n",
      "Batch [24200/29572] Loss: 4.4196\n",
      "Batch [24300/29572] Loss: 3.6205\n",
      "Batch [24400/29572] Loss: 5.0364\n",
      "Batch [24500/29572] Loss: 5.2801\n",
      "Batch [24600/29572] Loss: 5.6216\n",
      "Batch [24700/29572] Loss: 4.8520\n",
      "Batch [24800/29572] Loss: 5.2288\n",
      "Batch [24900/29572] Loss: 4.8748\n",
      "Batch [25000/29572] Loss: 5.6212\n",
      "Batch [25100/29572] Loss: 4.7377\n",
      "Batch [25200/29572] Loss: 5.3695\n",
      "Batch [25300/29572] Loss: 4.7330\n",
      "Batch [25400/29572] Loss: 5.3949\n",
      "Batch [25500/29572] Loss: 5.5638\n",
      "Batch [25600/29572] Loss: 4.6875\n",
      "Batch [25700/29572] Loss: 4.9404\n",
      "Batch [25800/29572] Loss: 4.5935\n",
      "Batch [25900/29572] Loss: 5.3897\n",
      "Batch [26000/29572] Loss: 4.9847\n",
      "Batch [26100/29572] Loss: 4.7553\n",
      "Batch [26200/29572] Loss: 5.2586\n",
      "Batch [26300/29572] Loss: 5.3127\n",
      "Batch [26400/29572] Loss: 5.1400\n",
      "Batch [26500/29572] Loss: 4.6031\n",
      "Batch [26600/29572] Loss: 5.4888\n",
      "Batch [26700/29572] Loss: 5.3313\n",
      "Batch [26800/29572] Loss: 4.9230\n",
      "Batch [26900/29572] Loss: 5.0639\n",
      "Batch [27000/29572] Loss: 5.0014\n",
      "Batch [27100/29572] Loss: 4.9087\n",
      "Batch [27200/29572] Loss: 5.5123\n",
      "Batch [27300/29572] Loss: 4.9132\n",
      "Batch [27400/29572] Loss: 6.2269\n",
      "Batch [27500/29572] Loss: 4.3062\n",
      "Batch [27600/29572] Loss: 4.3164\n",
      "Batch [27700/29572] Loss: 4.3088\n",
      "Batch [27800/29572] Loss: 5.2976\n",
      "Batch [27900/29572] Loss: 5.4263\n",
      "Batch [28000/29572] Loss: 4.2930\n",
      "Batch [28100/29572] Loss: 5.0084\n",
      "Batch [28200/29572] Loss: 4.6453\n",
      "Batch [28300/29572] Loss: 5.7986\n",
      "Batch [28400/29572] Loss: 5.1951\n",
      "Batch [28500/29572] Loss: 5.5801\n",
      "Batch [28600/29572] Loss: 4.8356\n",
      "Batch [28700/29572] Loss: 3.9054\n",
      "Batch [28800/29572] Loss: 4.9080\n",
      "Batch [28900/29572] Loss: 5.1970\n",
      "Batch [29000/29572] Loss: 4.4846\n",
      "Batch [29100/29572] Loss: 5.5198\n",
      "Batch [29200/29572] Loss: 4.7391\n",
      "Batch [29300/29572] Loss: 4.0635\n",
      "Batch [29400/29572] Loss: 5.5441\n",
      "Batch [29500/29572] Loss: 3.9620\n",
      "Train Loss: 4.9655\n",
      "\n",
      "--- Epoch 3 ---\n",
      "Batch [0/29572] Loss: 5.2775\n",
      "Batch [100/29572] Loss: 4.3956\n",
      "Batch [200/29572] Loss: 5.4890\n",
      "Batch [300/29572] Loss: 5.4531\n",
      "Batch [400/29572] Loss: 5.3195\n",
      "Batch [500/29572] Loss: 5.6172\n",
      "Batch [600/29572] Loss: 5.4572\n",
      "Batch [700/29572] Loss: 4.6061\n",
      "Batch [800/29572] Loss: 4.9813\n",
      "Batch [900/29572] Loss: 5.6692\n",
      "Batch [1000/29572] Loss: 4.8987\n",
      "Batch [1100/29572] Loss: 4.5369\n",
      "Batch [1200/29572] Loss: 4.9734\n",
      "Batch [1300/29572] Loss: 4.7834\n",
      "Batch [1400/29572] Loss: 5.3486\n",
      "Batch [1500/29572] Loss: 5.3761\n",
      "Batch [1600/29572] Loss: 4.8196\n",
      "Batch [1700/29572] Loss: 5.4777\n",
      "Batch [1800/29572] Loss: 4.5809\n",
      "Batch [1900/29572] Loss: 4.4644\n",
      "Batch [2000/29572] Loss: 3.6100\n",
      "Batch [2100/29572] Loss: 5.8159\n",
      "Batch [2200/29572] Loss: 3.9149\n",
      "Batch [2300/29572] Loss: 4.9927\n",
      "Batch [2400/29572] Loss: 6.0460\n",
      "Batch [2500/29572] Loss: 5.0071\n",
      "Batch [2600/29572] Loss: 4.7303\n",
      "Batch [2700/29572] Loss: 5.0120\n",
      "Batch [2800/29572] Loss: 4.8653\n",
      "Batch [2900/29572] Loss: 5.7291\n",
      "Batch [3000/29572] Loss: 5.4080\n",
      "Batch [3100/29572] Loss: 5.2788\n",
      "Batch [3200/29572] Loss: 4.5857\n",
      "Batch [3300/29572] Loss: 3.7412\n",
      "Batch [3400/29572] Loss: 6.4910\n",
      "Batch [3500/29572] Loss: 5.3724\n",
      "Batch [3600/29572] Loss: 4.6677\n",
      "Batch [3700/29572] Loss: 5.3723\n",
      "Batch [3800/29572] Loss: 5.9725\n",
      "Batch [3900/29572] Loss: 5.8872\n",
      "Batch [4000/29572] Loss: 5.6213\n",
      "Batch [4100/29572] Loss: 4.4346\n",
      "Batch [4200/29572] Loss: 5.4747\n",
      "Batch [4300/29572] Loss: 5.6770\n",
      "Batch [4400/29572] Loss: 5.1234\n",
      "Batch [4500/29572] Loss: 4.9170\n",
      "Batch [4600/29572] Loss: 4.3232\n",
      "Batch [4700/29572] Loss: 4.8997\n",
      "Batch [4800/29572] Loss: 4.7986\n",
      "Batch [4900/29572] Loss: 5.7340\n",
      "Batch [5000/29572] Loss: 5.3893\n",
      "Batch [5100/29572] Loss: 5.1233\n",
      "Batch [5200/29572] Loss: 5.4278\n",
      "Batch [5300/29572] Loss: 6.2733\n",
      "Batch [5400/29572] Loss: 4.4351\n",
      "Batch [5500/29572] Loss: 5.4821\n",
      "Batch [5600/29572] Loss: 5.4409\n",
      "Batch [5700/29572] Loss: 6.1704\n",
      "Batch [5800/29572] Loss: 4.5294\n",
      "Batch [5900/29572] Loss: 6.2177\n",
      "Batch [6000/29572] Loss: 5.4946\n",
      "Batch [6100/29572] Loss: 5.4274\n",
      "Batch [6200/29572] Loss: 5.2794\n",
      "Batch [6300/29572] Loss: 5.3342\n",
      "Batch [6400/29572] Loss: 4.9409\n",
      "Batch [6500/29572] Loss: 4.9054\n",
      "Batch [6600/29572] Loss: 5.6565\n",
      "Batch [6700/29572] Loss: 4.3175\n",
      "Batch [6800/29572] Loss: 4.0100\n",
      "Batch [6900/29572] Loss: 4.8604\n",
      "Batch [7000/29572] Loss: 5.1716\n",
      "Batch [7100/29572] Loss: 5.1972\n",
      "Batch [7200/29572] Loss: 4.2965\n",
      "Batch [7300/29572] Loss: 4.3861\n",
      "Batch [7400/29572] Loss: 5.6078\n",
      "Batch [7500/29572] Loss: 4.9564\n",
      "Batch [7600/29572] Loss: 4.3640\n",
      "Batch [7700/29572] Loss: 6.1374\n",
      "Batch [7800/29572] Loss: 5.1065\n",
      "Batch [7900/29572] Loss: 4.6942\n",
      "Batch [8000/29572] Loss: 5.3017\n",
      "Batch [8100/29572] Loss: 4.6688\n",
      "Batch [8200/29572] Loss: 5.6464\n",
      "Batch [8300/29572] Loss: 5.2539\n",
      "Batch [8400/29572] Loss: 4.8566\n",
      "Batch [8500/29572] Loss: 5.0899\n",
      "Batch [8600/29572] Loss: 4.4979\n",
      "Batch [8700/29572] Loss: 5.1454\n",
      "Batch [8800/29572] Loss: 4.5350\n",
      "Batch [8900/29572] Loss: 5.2716\n",
      "Batch [9000/29572] Loss: 5.5092\n",
      "Batch [9100/29572] Loss: 5.1571\n",
      "Batch [9200/29572] Loss: 4.1495\n",
      "Batch [9300/29572] Loss: 5.3481\n",
      "Batch [9400/29572] Loss: 5.0904\n",
      "Batch [9500/29572] Loss: 3.8329\n",
      "Batch [9600/29572] Loss: 5.2404\n",
      "Batch [9700/29572] Loss: 4.8468\n",
      "Batch [9800/29572] Loss: 4.9415\n",
      "Batch [9900/29572] Loss: 5.5118\n",
      "Batch [10000/29572] Loss: 5.0734\n",
      "Batch [10100/29572] Loss: 4.8371\n",
      "Batch [10200/29572] Loss: 4.1935\n",
      "Batch [10300/29572] Loss: 5.3574\n",
      "Batch [10400/29572] Loss: 6.2758\n",
      "Batch [10500/29572] Loss: 3.4790\n",
      "Batch [10600/29572] Loss: 5.0914\n",
      "Batch [10700/29572] Loss: 4.5235\n",
      "Batch [10800/29572] Loss: 5.5529\n",
      "Batch [10900/29572] Loss: 5.2832\n",
      "Batch [11000/29572] Loss: 4.7978\n",
      "Batch [11100/29572] Loss: 3.7000\n",
      "Batch [11200/29572] Loss: 5.9650\n",
      "Batch [11300/29572] Loss: 4.5268\n",
      "Batch [11400/29572] Loss: 4.7809\n",
      "Batch [11500/29572] Loss: 5.6474\n",
      "Batch [11600/29572] Loss: 4.4687\n",
      "Batch [11700/29572] Loss: 4.8654\n",
      "Batch [11800/29572] Loss: 5.4733\n",
      "Batch [11900/29572] Loss: 5.3263\n",
      "Batch [12000/29572] Loss: 6.0091\n",
      "Batch [12100/29572] Loss: 5.4417\n",
      "Batch [12200/29572] Loss: 5.1629\n",
      "Batch [12300/29572] Loss: 4.3222\n",
      "Batch [12400/29572] Loss: 4.9985\n",
      "Batch [12500/29572] Loss: 4.7644\n",
      "Batch [12600/29572] Loss: 4.4920\n",
      "Batch [12700/29572] Loss: 4.1050\n",
      "Batch [12800/29572] Loss: 5.0620\n",
      "Batch [12900/29572] Loss: 4.2190\n",
      "Batch [13000/29572] Loss: 5.1302\n",
      "Batch [13100/29572] Loss: 5.5026\n",
      "Batch [13200/29572] Loss: 4.9656\n",
      "Batch [13300/29572] Loss: 4.3687\n",
      "Batch [13400/29572] Loss: 5.3981\n",
      "Batch [13500/29572] Loss: 4.8062\n",
      "Batch [13600/29572] Loss: 4.7044\n",
      "Batch [13700/29572] Loss: 5.4030\n",
      "Batch [13800/29572] Loss: 5.0178\n",
      "Batch [13900/29572] Loss: 4.9828\n",
      "Batch [14000/29572] Loss: 4.4464\n",
      "Batch [14100/29572] Loss: 4.7408\n",
      "Batch [14200/29572] Loss: 5.2798\n",
      "Batch [14300/29572] Loss: 4.3413\n",
      "Batch [14400/29572] Loss: 4.9810\n",
      "Batch [14500/29572] Loss: 5.7533\n",
      "Batch [14600/29572] Loss: 4.9667\n",
      "Batch [14700/29572] Loss: 5.6527\n",
      "Batch [14800/29572] Loss: 4.6276\n",
      "Batch [14900/29572] Loss: 5.2293\n",
      "Batch [15000/29572] Loss: 4.3769\n",
      "Batch [15100/29572] Loss: 4.5207\n",
      "Batch [15200/29572] Loss: 4.5974\n",
      "Batch [15300/29572] Loss: 5.2949\n",
      "Batch [15400/29572] Loss: 4.9829\n",
      "Batch [15500/29572] Loss: 4.3971\n",
      "Batch [15600/29572] Loss: 4.2538\n",
      "Batch [15700/29572] Loss: 5.8052\n",
      "Batch [15800/29572] Loss: 4.6414\n",
      "Batch [15900/29572] Loss: 4.1613\n",
      "Batch [16000/29572] Loss: 5.6436\n",
      "Batch [16100/29572] Loss: 5.1262\n",
      "Batch [16200/29572] Loss: 4.9218\n",
      "Batch [16300/29572] Loss: 5.1174\n",
      "Batch [16400/29572] Loss: 5.1291\n",
      "Batch [16500/29572] Loss: 4.7430\n",
      "Batch [16600/29572] Loss: 5.0720\n",
      "Batch [16700/29572] Loss: 4.8061\n",
      "Batch [16800/29572] Loss: 4.4204\n",
      "Batch [16900/29572] Loss: 4.6174\n",
      "Batch [17000/29572] Loss: 5.1352\n",
      "Batch [17100/29572] Loss: 5.4216\n",
      "Batch [17200/29572] Loss: 4.4998\n",
      "Batch [17300/29572] Loss: 5.3727\n",
      "Batch [17400/29572] Loss: 4.1147\n",
      "Batch [17500/29572] Loss: 5.6142\n",
      "Batch [17600/29572] Loss: 5.1285\n",
      "Batch [17700/29572] Loss: 4.8590\n",
      "Batch [17800/29572] Loss: 5.0761\n",
      "Batch [17900/29572] Loss: 5.6018\n",
      "Batch [18000/29572] Loss: 5.9257\n",
      "Batch [18100/29572] Loss: 5.4408\n",
      "Batch [18200/29572] Loss: 5.5971\n",
      "Batch [18300/29572] Loss: 4.9243\n",
      "Batch [18400/29572] Loss: 5.1184\n",
      "Batch [18500/29572] Loss: 4.5672\n",
      "Batch [18600/29572] Loss: 5.2303\n",
      "Batch [18700/29572] Loss: 4.9913\n",
      "Batch [18800/29572] Loss: 5.1063\n",
      "Batch [18900/29572] Loss: 3.6355\n",
      "Batch [19000/29572] Loss: 5.2909\n",
      "Batch [19100/29572] Loss: 4.7429\n",
      "Batch [19200/29572] Loss: 5.1131\n",
      "Batch [19300/29572] Loss: 5.0833\n",
      "Batch [19400/29572] Loss: 4.9524\n",
      "Batch [19500/29572] Loss: 4.1867\n",
      "Batch [19600/29572] Loss: 4.2132\n",
      "Batch [19700/29572] Loss: 5.2321\n",
      "Batch [19800/29572] Loss: 4.5853\n",
      "Batch [19900/29572] Loss: 5.0341\n",
      "Batch [20000/29572] Loss: 5.4338\n",
      "Batch [20100/29572] Loss: 5.3103\n",
      "Batch [20200/29572] Loss: 4.8220\n",
      "Batch [20300/29572] Loss: 5.6274\n",
      "Batch [20400/29572] Loss: 4.8005\n",
      "Batch [20500/29572] Loss: 5.5450\n",
      "Batch [20600/29572] Loss: 5.5802\n",
      "Batch [20700/29572] Loss: 4.8057\n",
      "Batch [20800/29572] Loss: 5.2783\n",
      "Batch [20900/29572] Loss: 5.3232\n",
      "Batch [21000/29572] Loss: 5.4852\n",
      "Batch [21100/29572] Loss: 4.4779\n",
      "Batch [21200/29572] Loss: 5.0714\n",
      "Batch [21300/29572] Loss: 4.3490\n",
      "Batch [21400/29572] Loss: 6.2780\n",
      "Batch [21500/29572] Loss: 4.8078\n",
      "Batch [21600/29572] Loss: 4.5412\n",
      "Batch [21700/29572] Loss: 4.8123\n",
      "Batch [21800/29572] Loss: 4.8964\n",
      "Batch [21900/29572] Loss: 4.6691\n",
      "Batch [22000/29572] Loss: 5.4466\n",
      "Batch [22100/29572] Loss: 5.4692\n",
      "Batch [22200/29572] Loss: 4.9640\n",
      "Batch [22300/29572] Loss: 4.9154\n",
      "Batch [22400/29572] Loss: 4.9438\n",
      "Batch [22500/29572] Loss: 5.6000\n",
      "Batch [22600/29572] Loss: 4.8460\n",
      "Batch [22700/29572] Loss: 3.8568\n",
      "Batch [22800/29572] Loss: 4.7367\n",
      "Batch [22900/29572] Loss: 4.7312\n",
      "Batch [23000/29572] Loss: 4.3144\n",
      "Batch [23100/29572] Loss: 5.3994\n",
      "Batch [23200/29572] Loss: 5.2249\n",
      "Batch [23300/29572] Loss: 4.2733\n",
      "Batch [23400/29572] Loss: 5.1933\n",
      "Batch [23500/29572] Loss: 4.8260\n",
      "Batch [23600/29572] Loss: 5.4674\n",
      "Batch [23700/29572] Loss: 5.0224\n",
      "Batch [23800/29572] Loss: 4.8112\n",
      "Batch [23900/29572] Loss: 5.6747\n",
      "Batch [24000/29572] Loss: 5.3387\n",
      "Batch [24100/29572] Loss: 5.6958\n",
      "Batch [24200/29572] Loss: 5.0747\n",
      "Batch [24300/29572] Loss: 4.4329\n",
      "Batch [24400/29572] Loss: 5.0140\n",
      "Batch [24500/29572] Loss: 4.6288\n",
      "Batch [24600/29572] Loss: 5.4876\n",
      "Batch [24700/29572] Loss: 5.3664\n",
      "Batch [24800/29572] Loss: 5.0842\n",
      "Batch [24900/29572] Loss: 5.4963\n",
      "Batch [25000/29572] Loss: 4.4256\n",
      "Batch [25100/29572] Loss: 4.1865\n",
      "Batch [25200/29572] Loss: 5.4728\n",
      "Batch [25300/29572] Loss: 5.6653\n",
      "Batch [25400/29572] Loss: 4.7063\n",
      "Batch [25500/29572] Loss: 5.3493\n",
      "Batch [25600/29572] Loss: 4.5936\n",
      "Batch [25700/29572] Loss: 4.7593\n",
      "Batch [25800/29572] Loss: 5.2148\n",
      "Batch [25900/29572] Loss: 4.7407\n",
      "Batch [26000/29572] Loss: 4.9203\n",
      "Batch [26100/29572] Loss: 5.5390\n",
      "Batch [26200/29572] Loss: 4.5146\n",
      "Batch [26300/29572] Loss: 4.4647\n",
      "Batch [26400/29572] Loss: 5.6866\n",
      "Batch [26500/29572] Loss: 4.6195\n",
      "Batch [26600/29572] Loss: 4.3587\n",
      "Batch [26700/29572] Loss: 6.1702\n",
      "Batch [26800/29572] Loss: 5.0034\n",
      "Batch [26900/29572] Loss: 3.6765\n",
      "Batch [27000/29572] Loss: 4.8778\n",
      "Batch [27100/29572] Loss: 4.8382\n",
      "Batch [27200/29572] Loss: 5.4629\n",
      "Batch [27300/29572] Loss: 4.9756\n",
      "Batch [27400/29572] Loss: 4.4578\n",
      "Batch [27500/29572] Loss: 4.8650\n",
      "Batch [27600/29572] Loss: 4.7267\n",
      "Batch [27700/29572] Loss: 5.9681\n",
      "Batch [27800/29572] Loss: 5.7754\n",
      "Batch [27900/29572] Loss: 5.5537\n",
      "Batch [28000/29572] Loss: 5.9742\n",
      "Batch [28100/29572] Loss: 3.7821\n",
      "Batch [28200/29572] Loss: 4.6346\n",
      "Batch [28300/29572] Loss: 4.6417\n",
      "Batch [28400/29572] Loss: 4.5676\n",
      "Batch [28500/29572] Loss: 4.7329\n",
      "Batch [28600/29572] Loss: 4.5674\n",
      "Batch [28700/29572] Loss: 4.2294\n",
      "Batch [28800/29572] Loss: 5.5399\n",
      "Batch [28900/29572] Loss: 4.1313\n",
      "Batch [29000/29572] Loss: 4.7954\n",
      "Batch [29100/29572] Loss: 4.1995\n",
      "Batch [29200/29572] Loss: 5.7783\n",
      "Batch [29300/29572] Loss: 6.0835\n",
      "Batch [29400/29572] Loss: 4.6714\n",
      "Batch [29500/29572] Loss: 5.1472\n",
      "Train Loss: 4.9478\n",
      "\n",
      "--- Epoch 4 ---\n",
      "Batch [0/29572] Loss: 4.0281\n",
      "Batch [100/29572] Loss: 4.2658\n",
      "Batch [200/29572] Loss: 4.9230\n",
      "Batch [300/29572] Loss: 5.8177\n",
      "Batch [400/29572] Loss: 5.0688\n",
      "Batch [500/29572] Loss: 5.3116\n",
      "Batch [600/29572] Loss: 4.5000\n",
      "Batch [700/29572] Loss: 4.9362\n",
      "Batch [800/29572] Loss: 4.3513\n",
      "Batch [900/29572] Loss: 4.6964\n",
      "Batch [1000/29572] Loss: 5.2723\n",
      "Batch [1100/29572] Loss: 3.7118\n",
      "Batch [1200/29572] Loss: 5.2137\n",
      "Batch [1300/29572] Loss: 3.5518\n",
      "Batch [1400/29572] Loss: 5.5132\n",
      "Batch [1500/29572] Loss: 5.2813\n",
      "Batch [1600/29572] Loss: 4.8429\n",
      "Batch [1700/29572] Loss: 5.5204\n",
      "Batch [1800/29572] Loss: 3.9891\n",
      "Batch [1900/29572] Loss: 5.5325\n",
      "Batch [2000/29572] Loss: 5.7880\n",
      "Batch [2100/29572] Loss: 4.0518\n",
      "Batch [2200/29572] Loss: 4.1730\n",
      "Batch [2300/29572] Loss: 6.4650\n",
      "Batch [2400/29572] Loss: 4.8465\n",
      "Batch [2500/29572] Loss: 4.5090\n",
      "Batch [2600/29572] Loss: 3.5991\n",
      "Batch [2700/29572] Loss: 5.0493\n",
      "Batch [2800/29572] Loss: 4.9998\n",
      "Batch [2900/29572] Loss: 4.6975\n",
      "Batch [3000/29572] Loss: 4.9046\n",
      "Batch [3100/29572] Loss: 5.2418\n",
      "Batch [3200/29572] Loss: 5.2410\n",
      "Batch [3300/29572] Loss: 4.9697\n",
      "Batch [3400/29572] Loss: 6.1805\n",
      "Batch [3500/29572] Loss: 5.2155\n",
      "Batch [3600/29572] Loss: 4.6764\n",
      "Batch [3700/29572] Loss: 5.3894\n",
      "Batch [3800/29572] Loss: 4.9089\n",
      "Batch [3900/29572] Loss: 3.8954\n",
      "Batch [4000/29572] Loss: 5.3929\n",
      "Batch [4100/29572] Loss: 6.1777\n",
      "Batch [4200/29572] Loss: 5.3583\n",
      "Batch [4300/29572] Loss: 4.4825\n",
      "Batch [4400/29572] Loss: 5.4005\n",
      "Batch [4500/29572] Loss: 4.9207\n",
      "Batch [4600/29572] Loss: 5.5836\n",
      "Batch [4700/29572] Loss: 5.5976\n",
      "Batch [4800/29572] Loss: 5.7737\n",
      "Batch [4900/29572] Loss: 4.9187\n",
      "Batch [5000/29572] Loss: 5.0223\n",
      "Batch [5100/29572] Loss: 4.0547\n",
      "Batch [5200/29572] Loss: 5.4568\n",
      "Batch [5300/29572] Loss: 4.0063\n",
      "Batch [5400/29572] Loss: 5.9492\n",
      "Batch [5500/29572] Loss: 5.8463\n",
      "Batch [5600/29572] Loss: 4.5362\n",
      "Batch [5700/29572] Loss: 4.5028\n",
      "Batch [5800/29572] Loss: 5.2486\n",
      "Batch [5900/29572] Loss: 5.1553\n",
      "Batch [6000/29572] Loss: 5.1094\n",
      "Batch [6100/29572] Loss: 3.9186\n",
      "Batch [6200/29572] Loss: 5.0363\n",
      "Batch [6300/29572] Loss: 4.5438\n",
      "Batch [6400/29572] Loss: 4.9598\n",
      "Batch [6500/29572] Loss: 5.4833\n",
      "Batch [6600/29572] Loss: 5.9154\n",
      "Batch [6700/29572] Loss: 5.0748\n",
      "Batch [6800/29572] Loss: 4.7393\n",
      "Batch [6900/29572] Loss: 4.1589\n",
      "Batch [7000/29572] Loss: 4.9398\n",
      "Batch [7100/29572] Loss: 5.3801\n",
      "Batch [7200/29572] Loss: 4.4065\n",
      "Batch [7300/29572] Loss: 4.9141\n",
      "Batch [7400/29572] Loss: 5.1122\n",
      "Batch [7500/29572] Loss: 3.7325\n",
      "Batch [7600/29572] Loss: 5.6371\n",
      "Batch [7700/29572] Loss: 5.0497\n",
      "Batch [7800/29572] Loss: 4.0864\n",
      "Batch [7900/29572] Loss: 4.3905\n",
      "Batch [8000/29572] Loss: 5.4465\n",
      "Batch [8100/29572] Loss: 5.0781\n",
      "Batch [8200/29572] Loss: 5.2676\n",
      "Batch [8300/29572] Loss: 4.3121\n",
      "Batch [8400/29572] Loss: 5.5557\n",
      "Batch [8500/29572] Loss: 5.0167\n",
      "Batch [8600/29572] Loss: 4.5648\n",
      "Batch [8700/29572] Loss: 3.5891\n",
      "Batch [8800/29572] Loss: 4.6153\n",
      "Batch [8900/29572] Loss: 5.2559\n",
      "Batch [9000/29572] Loss: 4.7826\n",
      "Batch [9100/29572] Loss: 4.4505\n",
      "Batch [9200/29572] Loss: 5.1875\n",
      "Batch [9300/29572] Loss: 4.1576\n",
      "Batch [9400/29572] Loss: 3.7843\n",
      "Batch [9500/29572] Loss: 4.8279\n",
      "Batch [9600/29572] Loss: 4.7801\n",
      "Batch [9700/29572] Loss: 4.8831\n",
      "Batch [9800/29572] Loss: 5.7451\n",
      "Batch [9900/29572] Loss: 4.1728\n",
      "Batch [10000/29572] Loss: 4.2458\n",
      "Batch [10100/29572] Loss: 5.8939\n",
      "Batch [10200/29572] Loss: 5.9384\n",
      "Batch [10300/29572] Loss: 4.7811\n",
      "Batch [10400/29572] Loss: 4.0318\n",
      "Batch [10500/29572] Loss: 4.6511\n",
      "Batch [10600/29572] Loss: 4.9647\n",
      "Batch [10700/29572] Loss: 4.7758\n",
      "Batch [10800/29572] Loss: 5.3226\n",
      "Batch [10900/29572] Loss: 4.4393\n",
      "Batch [11000/29572] Loss: 4.5649\n",
      "Batch [11100/29572] Loss: 5.6805\n",
      "Batch [11200/29572] Loss: 4.3379\n",
      "Batch [11300/29572] Loss: 4.3896\n",
      "Batch [11400/29572] Loss: 4.6654\n",
      "Batch [11500/29572] Loss: 4.0712\n",
      "Batch [11600/29572] Loss: 4.5429\n",
      "Batch [11700/29572] Loss: 5.0773\n",
      "Batch [11800/29572] Loss: 4.6671\n",
      "Batch [11900/29572] Loss: 4.6088\n",
      "Batch [12000/29572] Loss: 4.5774\n",
      "Batch [12100/29572] Loss: 4.9033\n",
      "Batch [12200/29572] Loss: 4.7719\n",
      "Batch [12300/29572] Loss: 4.5252\n",
      "Batch [12400/29572] Loss: 4.8591\n",
      "Batch [12500/29572] Loss: 4.2145\n",
      "Batch [12600/29572] Loss: 4.4791\n",
      "Batch [12700/29572] Loss: 5.7103\n",
      "Batch [12800/29572] Loss: 5.4120\n",
      "Batch [12900/29572] Loss: 4.2340\n",
      "Batch [13000/29572] Loss: 5.8033\n",
      "Batch [13100/29572] Loss: 4.9748\n",
      "Batch [13200/29572] Loss: 3.8737\n",
      "Batch [13300/29572] Loss: 5.2021\n",
      "Batch [13400/29572] Loss: 5.5420\n",
      "Batch [13500/29572] Loss: 4.7846\n",
      "Batch [13600/29572] Loss: 4.9137\n",
      "Batch [13700/29572] Loss: 5.0045\n",
      "Batch [13800/29572] Loss: 3.9969\n",
      "Batch [13900/29572] Loss: 4.5729\n",
      "Batch [14000/29572] Loss: 4.3842\n",
      "Batch [14100/29572] Loss: 5.1905\n",
      "Batch [14200/29572] Loss: 5.0773\n",
      "Batch [14300/29572] Loss: 5.1757\n",
      "Batch [14400/29572] Loss: 5.4683\n",
      "Batch [14500/29572] Loss: 4.0061\n",
      "Batch [14600/29572] Loss: 5.4118\n",
      "Batch [14700/29572] Loss: 5.5286\n",
      "Batch [14800/29572] Loss: 5.6688\n",
      "Batch [14900/29572] Loss: 4.0738\n",
      "Batch [15000/29572] Loss: 5.5665\n",
      "Batch [15100/29572] Loss: 6.9417\n",
      "Batch [15200/29572] Loss: 5.3604\n",
      "Batch [15300/29572] Loss: 5.3549\n",
      "Batch [15400/29572] Loss: 4.5637\n",
      "Batch [15500/29572] Loss: 4.7577\n",
      "Batch [15600/29572] Loss: 4.6788\n",
      "Batch [15700/29572] Loss: 5.6235\n",
      "Batch [15800/29572] Loss: 5.9156\n",
      "Batch [15900/29572] Loss: 4.0197\n",
      "Batch [16000/29572] Loss: 3.5343\n",
      "Batch [16100/29572] Loss: 4.8936\n",
      "Batch [16200/29572] Loss: 6.3201\n",
      "Batch [16300/29572] Loss: 5.0651\n",
      "Batch [16400/29572] Loss: 4.2335\n",
      "Batch [16500/29572] Loss: 4.7092\n",
      "Batch [16600/29572] Loss: 5.6584\n",
      "Batch [16700/29572] Loss: 3.4334\n",
      "Batch [16800/29572] Loss: 3.8527\n",
      "Batch [16900/29572] Loss: 5.2749\n",
      "Batch [17000/29572] Loss: 4.7274\n",
      "Batch [17100/29572] Loss: 4.8630\n",
      "Batch [17200/29572] Loss: 5.2895\n",
      "Batch [17300/29572] Loss: 5.5994\n",
      "Batch [17400/29572] Loss: 4.8479\n",
      "Batch [17500/29572] Loss: 5.4075\n",
      "Batch [17600/29572] Loss: 4.6379\n",
      "Batch [17700/29572] Loss: 3.4286\n",
      "Batch [17800/29572] Loss: 4.2643\n",
      "Batch [17900/29572] Loss: 5.1224\n",
      "Batch [18000/29572] Loss: 4.3123\n",
      "Batch [18100/29572] Loss: 5.0670\n",
      "Batch [18200/29572] Loss: 4.7588\n",
      "Batch [18300/29572] Loss: 5.1491\n",
      "Batch [18400/29572] Loss: 4.7964\n",
      "Batch [18500/29572] Loss: 6.1726\n",
      "Batch [18600/29572] Loss: 5.5883\n",
      "Batch [18700/29572] Loss: 4.5039\n",
      "Batch [18800/29572] Loss: 5.2052\n",
      "Batch [18900/29572] Loss: 4.3037\n",
      "Batch [19000/29572] Loss: 5.7187\n",
      "Batch [19100/29572] Loss: 5.1414\n",
      "Batch [19200/29572] Loss: 5.9426\n",
      "Batch [19300/29572] Loss: 5.4397\n",
      "Batch [19400/29572] Loss: 4.6566\n",
      "Batch [19500/29572] Loss: 5.1030\n",
      "Batch [19600/29572] Loss: 5.4944\n",
      "Batch [19700/29572] Loss: 5.2520\n",
      "Batch [19800/29572] Loss: 5.1148\n",
      "Batch [19900/29572] Loss: 4.5776\n",
      "Batch [20000/29572] Loss: 3.7871\n",
      "Batch [20100/29572] Loss: 4.6919\n",
      "Batch [20200/29572] Loss: 5.0629\n",
      "Batch [20300/29572] Loss: 5.7174\n",
      "Batch [20400/29572] Loss: 5.0067\n",
      "Batch [20500/29572] Loss: 4.2975\n",
      "Batch [20600/29572] Loss: 5.6384\n",
      "Batch [20700/29572] Loss: 4.7881\n",
      "Batch [20800/29572] Loss: 4.9750\n",
      "Batch [20900/29572] Loss: 4.4545\n",
      "Batch [21000/29572] Loss: 5.1525\n",
      "Batch [21100/29572] Loss: 5.4999\n",
      "Batch [21200/29572] Loss: 4.6687\n",
      "Batch [21300/29572] Loss: 4.3031\n",
      "Batch [21400/29572] Loss: 5.8513\n",
      "Batch [21500/29572] Loss: 5.9810\n",
      "Batch [21600/29572] Loss: 5.7217\n",
      "Batch [21700/29572] Loss: 4.8201\n",
      "Batch [21800/29572] Loss: 5.1753\n",
      "Batch [21900/29572] Loss: 4.6957\n",
      "Batch [22000/29572] Loss: 4.9319\n",
      "Batch [22100/29572] Loss: 4.9517\n",
      "Batch [22200/29572] Loss: 4.3434\n",
      "Batch [22300/29572] Loss: 4.7626\n",
      "Batch [22400/29572] Loss: 5.5413\n",
      "Batch [22500/29572] Loss: 4.8678\n",
      "Batch [22600/29572] Loss: 4.4170\n",
      "Batch [22700/29572] Loss: 4.3664\n",
      "Batch [22800/29572] Loss: 5.8911\n",
      "Batch [22900/29572] Loss: 4.8640\n",
      "Batch [23000/29572] Loss: 4.0254\n",
      "Batch [23100/29572] Loss: 4.1402\n",
      "Batch [23200/29572] Loss: 4.5528\n",
      "Batch [23300/29572] Loss: 5.0921\n",
      "Batch [23400/29572] Loss: 5.3308\n",
      "Batch [23500/29572] Loss: 5.1957\n",
      "Batch [23600/29572] Loss: 5.3888\n",
      "Batch [23700/29572] Loss: 4.9073\n",
      "Batch [23800/29572] Loss: 5.0865\n",
      "Batch [23900/29572] Loss: 5.2192\n",
      "Batch [24000/29572] Loss: 4.7411\n",
      "Batch [24100/29572] Loss: 5.4028\n",
      "Batch [24200/29572] Loss: 5.3006\n",
      "Batch [24300/29572] Loss: 5.6802\n",
      "Batch [24400/29572] Loss: 5.0145\n",
      "Batch [24500/29572] Loss: 5.0512\n",
      "Batch [24600/29572] Loss: 5.0038\n",
      "Batch [24700/29572] Loss: 4.7866\n",
      "Batch [24800/29572] Loss: 4.4857\n",
      "Batch [24900/29572] Loss: 4.3995\n",
      "Batch [25000/29572] Loss: 4.8563\n",
      "Batch [25100/29572] Loss: 4.4028\n",
      "Batch [25200/29572] Loss: 5.2254\n",
      "Batch [25300/29572] Loss: 5.1583\n",
      "Batch [25400/29572] Loss: 4.2146\n",
      "Batch [25500/29572] Loss: 4.1737\n",
      "Batch [25600/29572] Loss: 3.7938\n",
      "Batch [25700/29572] Loss: 4.4353\n",
      "Batch [25800/29572] Loss: 5.2437\n",
      "Batch [25900/29572] Loss: 4.8390\n",
      "Batch [26000/29572] Loss: 4.7580\n",
      "Batch [26100/29572] Loss: 4.6637\n",
      "Batch [26200/29572] Loss: 4.9116\n",
      "Batch [26300/29572] Loss: 4.7532\n",
      "Batch [26400/29572] Loss: 5.2852\n",
      "Batch [26500/29572] Loss: 4.6581\n",
      "Batch [26600/29572] Loss: 4.8647\n",
      "Batch [26700/29572] Loss: 3.6641\n",
      "Batch [26800/29572] Loss: 4.3099\n",
      "Batch [26900/29572] Loss: 3.7281\n",
      "Batch [27000/29572] Loss: 3.9197\n",
      "Batch [27100/29572] Loss: 5.2453\n",
      "Batch [27200/29572] Loss: 5.1683\n",
      "Batch [27300/29572] Loss: 5.5201\n",
      "Batch [27400/29572] Loss: 4.9850\n",
      "Batch [27500/29572] Loss: 4.0215\n",
      "Batch [27600/29572] Loss: 4.9463\n",
      "Batch [27700/29572] Loss: 4.6914\n",
      "Batch [27800/29572] Loss: 4.9058\n",
      "Batch [27900/29572] Loss: 5.8430\n",
      "Batch [28000/29572] Loss: 5.1606\n",
      "Batch [28100/29572] Loss: 4.8885\n",
      "Batch [28200/29572] Loss: 4.4091\n",
      "Batch [28300/29572] Loss: 4.8305\n",
      "Batch [28400/29572] Loss: 4.9344\n",
      "Batch [28500/29572] Loss: 5.3627\n",
      "Batch [28600/29572] Loss: 5.6060\n",
      "Batch [28700/29572] Loss: 4.0575\n",
      "Batch [28800/29572] Loss: 5.3536\n",
      "Batch [28900/29572] Loss: 5.1667\n",
      "Batch [29000/29572] Loss: 4.8486\n",
      "Batch [29100/29572] Loss: 4.3872\n",
      "Batch [29200/29572] Loss: 4.9657\n",
      "Batch [29300/29572] Loss: 4.2805\n",
      "Batch [29400/29572] Loss: 4.2418\n",
      "Batch [29500/29572] Loss: 4.1667\n",
      "Train Loss: 4.9339\n",
      "\n",
      "--- Epoch 5 ---\n",
      "Batch [0/29572] Loss: 5.7715\n",
      "Batch [100/29572] Loss: 4.8434\n",
      "Batch [200/29572] Loss: 5.0563\n",
      "Batch [300/29572] Loss: 4.8030\n",
      "Batch [400/29572] Loss: 5.1747\n",
      "Batch [500/29572] Loss: 4.9700\n",
      "Batch [600/29572] Loss: 4.5172\n",
      "Batch [700/29572] Loss: 5.6940\n",
      "Batch [800/29572] Loss: 4.7308\n",
      "Batch [900/29572] Loss: 4.8129\n",
      "Batch [1000/29572] Loss: 4.5207\n",
      "Batch [1100/29572] Loss: 5.1752\n",
      "Batch [1200/29572] Loss: 3.8650\n",
      "Batch [1300/29572] Loss: 5.3363\n",
      "Batch [1400/29572] Loss: 5.1041\n",
      "Batch [1500/29572] Loss: 5.2277\n",
      "Batch [1600/29572] Loss: 5.4692\n",
      "Batch [1700/29572] Loss: 4.3152\n",
      "Batch [1800/29572] Loss: 4.6936\n",
      "Batch [1900/29572] Loss: 4.5147\n",
      "Batch [2000/29572] Loss: 4.3740\n",
      "Batch [2100/29572] Loss: 4.9082\n",
      "Batch [2200/29572] Loss: 4.5044\n",
      "Batch [2300/29572] Loss: 6.2615\n",
      "Batch [2400/29572] Loss: 4.8701\n",
      "Batch [2500/29572] Loss: 5.2715\n",
      "Batch [2600/29572] Loss: 4.8700\n",
      "Batch [2700/29572] Loss: 5.7624\n",
      "Batch [2800/29572] Loss: 4.4545\n",
      "Batch [2900/29572] Loss: 5.6543\n",
      "Batch [3000/29572] Loss: 5.2342\n",
      "Batch [3100/29572] Loss: 3.6727\n",
      "Batch [3200/29572] Loss: 4.7568\n",
      "Batch [3300/29572] Loss: 5.9514\n",
      "Batch [3400/29572] Loss: 5.6530\n",
      "Batch [3500/29572] Loss: 5.3133\n",
      "Batch [3600/29572] Loss: 5.1555\n",
      "Batch [3700/29572] Loss: 3.2691\n",
      "Batch [3800/29572] Loss: 4.4410\n",
      "Batch [3900/29572] Loss: 3.5770\n",
      "Batch [4000/29572] Loss: 5.0138\n",
      "Batch [4100/29572] Loss: 3.8826\n",
      "Batch [4200/29572] Loss: 5.7282\n",
      "Batch [4300/29572] Loss: 5.6898\n",
      "Batch [4400/29572] Loss: 4.7284\n",
      "Batch [4500/29572] Loss: 5.0079\n",
      "Batch [4600/29572] Loss: 5.8994\n",
      "Batch [4700/29572] Loss: 4.8136\n",
      "Batch [4800/29572] Loss: 4.0301\n",
      "Batch [4900/29572] Loss: 5.1626\n",
      "Batch [5000/29572] Loss: 4.6504\n",
      "Batch [5100/29572] Loss: 6.4849\n",
      "Batch [5200/29572] Loss: 4.5138\n",
      "Batch [5300/29572] Loss: 5.3521\n",
      "Batch [5400/29572] Loss: 4.0573\n",
      "Batch [5500/29572] Loss: 4.8591\n",
      "Batch [5600/29572] Loss: 5.3886\n",
      "Batch [5700/29572] Loss: 4.8852\n",
      "Batch [5800/29572] Loss: 5.3345\n",
      "Batch [5900/29572] Loss: 5.6273\n",
      "Batch [6000/29572] Loss: 4.9883\n",
      "Batch [6100/29572] Loss: 5.0752\n",
      "Batch [6200/29572] Loss: 6.3045\n",
      "Batch [6300/29572] Loss: 5.2785\n",
      "Batch [6400/29572] Loss: 5.3855\n",
      "Batch [6500/29572] Loss: 4.4732\n",
      "Batch [6600/29572] Loss: 5.0190\n",
      "Batch [6700/29572] Loss: 4.9450\n",
      "Batch [6800/29572] Loss: 4.2812\n",
      "Batch [6900/29572] Loss: 5.3818\n",
      "Batch [7000/29572] Loss: 5.8498\n",
      "Batch [7100/29572] Loss: 5.7993\n",
      "Batch [7200/29572] Loss: 5.5453\n",
      "Batch [7300/29572] Loss: 5.1947\n",
      "Batch [7400/29572] Loss: 5.1770\n",
      "Batch [7500/29572] Loss: 5.1652\n",
      "Batch [7600/29572] Loss: 5.5439\n",
      "Batch [7700/29572] Loss: 5.4360\n",
      "Batch [7800/29572] Loss: 5.0314\n",
      "Batch [7900/29572] Loss: 5.5814\n",
      "Batch [8000/29572] Loss: 4.8531\n",
      "Batch [8100/29572] Loss: 5.0055\n",
      "Batch [8200/29572] Loss: 4.7873\n",
      "Batch [8300/29572] Loss: 4.2479\n",
      "Batch [8400/29572] Loss: 5.0478\n",
      "Batch [8500/29572] Loss: 4.8517\n",
      "Batch [8600/29572] Loss: 4.4880\n",
      "Batch [8700/29572] Loss: 4.6668\n",
      "Batch [8800/29572] Loss: 5.8448\n",
      "Batch [8900/29572] Loss: 4.9128\n",
      "Batch [9000/29572] Loss: 3.8732\n",
      "Batch [9100/29572] Loss: 5.1403\n",
      "Batch [9200/29572] Loss: 4.2886\n",
      "Batch [9300/29572] Loss: 4.2757\n",
      "Batch [9400/29572] Loss: 6.0042\n",
      "Batch [9500/29572] Loss: 4.9976\n",
      "Batch [9600/29572] Loss: 4.8591\n",
      "Batch [9700/29572] Loss: 4.4657\n",
      "Batch [9800/29572] Loss: 5.1341\n",
      "Batch [9900/29572] Loss: 5.0150\n",
      "Batch [10000/29572] Loss: 5.4352\n",
      "Batch [10100/29572] Loss: 5.8140\n",
      "Batch [10200/29572] Loss: 5.3691\n",
      "Batch [10300/29572] Loss: 5.5835\n",
      "Batch [10400/29572] Loss: 6.6043\n",
      "Batch [10500/29572] Loss: 4.6607\n",
      "Batch [10600/29572] Loss: 4.8941\n",
      "Batch [10700/29572] Loss: 6.9171\n",
      "Batch [10800/29572] Loss: 5.3466\n",
      "Batch [10900/29572] Loss: 4.9521\n",
      "Batch [11000/29572] Loss: 4.6920\n",
      "Batch [11100/29572] Loss: 5.1058\n",
      "Batch [11200/29572] Loss: 5.0849\n",
      "Batch [11300/29572] Loss: 4.6207\n",
      "Batch [11400/29572] Loss: 5.1611\n",
      "Batch [11500/29572] Loss: 4.9399\n",
      "Batch [11600/29572] Loss: 5.0187\n",
      "Batch [11700/29572] Loss: 4.7203\n",
      "Batch [11800/29572] Loss: 5.9312\n",
      "Batch [11900/29572] Loss: 5.3392\n",
      "Batch [12000/29572] Loss: 5.1148\n",
      "Batch [12100/29572] Loss: 5.5380\n",
      "Batch [12200/29572] Loss: 4.8521\n",
      "Batch [12300/29572] Loss: 5.4281\n",
      "Batch [12400/29572] Loss: 5.0302\n",
      "Batch [12500/29572] Loss: 5.2361\n",
      "Batch [12600/29572] Loss: 5.2154\n",
      "Batch [12700/29572] Loss: 5.4014\n",
      "Batch [12800/29572] Loss: 4.5821\n",
      "Batch [12900/29572] Loss: 4.6757\n",
      "Batch [13000/29572] Loss: 5.1268\n",
      "Batch [13100/29572] Loss: 4.8586\n",
      "Batch [13200/29572] Loss: 4.5562\n",
      "Batch [13300/29572] Loss: 5.3753\n",
      "Batch [13400/29572] Loss: 5.1610\n",
      "Batch [13500/29572] Loss: 4.9594\n",
      "Batch [13600/29572] Loss: 5.1073\n",
      "Batch [13700/29572] Loss: 5.3532\n",
      "Batch [13800/29572] Loss: 5.1770\n",
      "Batch [13900/29572] Loss: 5.0135\n",
      "Batch [14000/29572] Loss: 4.4057\n",
      "Batch [14100/29572] Loss: 5.6899\n",
      "Batch [14200/29572] Loss: 5.0163\n",
      "Batch [14300/29572] Loss: 5.3026\n",
      "Batch [14400/29572] Loss: 5.0065\n",
      "Batch [14500/29572] Loss: 5.0256\n",
      "Batch [14600/29572] Loss: 4.2929\n",
      "Batch [14700/29572] Loss: 4.5654\n",
      "Batch [14800/29572] Loss: 4.0241\n",
      "Batch [14900/29572] Loss: 4.9062\n",
      "Batch [15000/29572] Loss: 5.2380\n",
      "Batch [15100/29572] Loss: 5.1906\n",
      "Batch [15200/29572] Loss: 5.0716\n",
      "Batch [15300/29572] Loss: 4.4884\n",
      "Batch [15400/29572] Loss: 4.6022\n",
      "Batch [15500/29572] Loss: 5.5510\n",
      "Batch [15600/29572] Loss: 5.1066\n",
      "Batch [15700/29572] Loss: 4.1069\n",
      "Batch [15800/29572] Loss: 5.0359\n",
      "Batch [15900/29572] Loss: 5.4448\n",
      "Batch [16000/29572] Loss: 4.9727\n",
      "Batch [16100/29572] Loss: 5.1295\n",
      "Batch [16200/29572] Loss: 4.6584\n",
      "Batch [16300/29572] Loss: 4.8052\n",
      "Batch [16400/29572] Loss: 4.8251\n",
      "Batch [16500/29572] Loss: 4.3123\n",
      "Batch [16600/29572] Loss: 4.3783\n",
      "Batch [16700/29572] Loss: 5.3202\n",
      "Batch [16800/29572] Loss: 4.0231\n",
      "Batch [16900/29572] Loss: 5.0872\n",
      "Batch [17000/29572] Loss: 4.8521\n",
      "Batch [17100/29572] Loss: 4.5871\n",
      "Batch [17200/29572] Loss: 4.7322\n",
      "Batch [17300/29572] Loss: 5.5979\n",
      "Batch [17400/29572] Loss: 5.0897\n",
      "Batch [17500/29572] Loss: 5.2968\n",
      "Batch [17600/29572] Loss: 5.2244\n",
      "Batch [17700/29572] Loss: 5.0796\n",
      "Batch [17800/29572] Loss: 4.7627\n",
      "Batch [17900/29572] Loss: 4.0570\n",
      "Batch [18000/29572] Loss: 4.9998\n",
      "Batch [18100/29572] Loss: 4.8971\n",
      "Batch [18200/29572] Loss: 4.1882\n",
      "Batch [18300/29572] Loss: 5.3770\n",
      "Batch [18400/29572] Loss: 4.3576\n",
      "Batch [18500/29572] Loss: 5.6354\n",
      "Batch [18600/29572] Loss: 4.5678\n",
      "Batch [18700/29572] Loss: 5.2879\n",
      "Batch [18800/29572] Loss: 5.2438\n",
      "Batch [18900/29572] Loss: 4.7696\n",
      "Batch [19000/29572] Loss: 4.3038\n",
      "Batch [19100/29572] Loss: 5.0495\n",
      "Batch [19200/29572] Loss: 5.4312\n",
      "Batch [19300/29572] Loss: 4.5400\n",
      "Batch [19400/29572] Loss: 5.2841\n",
      "Batch [19500/29572] Loss: 5.4590\n",
      "Batch [19600/29572] Loss: 4.5959\n",
      "Batch [19700/29572] Loss: 4.4264\n",
      "Batch [19800/29572] Loss: 4.5030\n",
      "Batch [19900/29572] Loss: 5.3712\n",
      "Batch [20000/29572] Loss: 4.4914\n",
      "Batch [20100/29572] Loss: 5.7912\n",
      "Batch [20200/29572] Loss: 5.1858\n",
      "Batch [20300/29572] Loss: 5.3214\n",
      "Batch [20400/29572] Loss: 4.9964\n",
      "Batch [20500/29572] Loss: 4.7642\n",
      "Batch [20600/29572] Loss: 4.3471\n",
      "Batch [20700/29572] Loss: 5.2778\n",
      "Batch [20800/29572] Loss: 5.3085\n",
      "Batch [20900/29572] Loss: 3.7390\n",
      "Batch [21000/29572] Loss: 5.1071\n",
      "Batch [21100/29572] Loss: 4.6546\n",
      "Batch [21200/29572] Loss: 4.3950\n",
      "Batch [21300/29572] Loss: 4.6419\n",
      "Batch [21400/29572] Loss: 4.5799\n",
      "Batch [21500/29572] Loss: 5.5114\n",
      "Batch [21600/29572] Loss: 4.2600\n",
      "Batch [21700/29572] Loss: 3.2481\n",
      "Batch [21800/29572] Loss: 4.8313\n",
      "Batch [21900/29572] Loss: 5.1501\n",
      "Batch [22000/29572] Loss: 4.7808\n",
      "Batch [22100/29572] Loss: 3.7692\n",
      "Batch [22200/29572] Loss: 3.7166\n",
      "Batch [22300/29572] Loss: 5.2567\n",
      "Batch [22400/29572] Loss: 4.7476\n",
      "Batch [22500/29572] Loss: 3.4712\n",
      "Batch [22600/29572] Loss: 4.6585\n",
      "Batch [22700/29572] Loss: 6.3309\n",
      "Batch [22800/29572] Loss: 5.8420\n",
      "Batch [22900/29572] Loss: 4.1413\n",
      "Batch [23000/29572] Loss: 4.6249\n",
      "Batch [23100/29572] Loss: 5.1525\n",
      "Batch [23200/29572] Loss: 4.1584\n",
      "Batch [23300/29572] Loss: 5.1561\n",
      "Batch [23400/29572] Loss: 4.1951\n",
      "Batch [23500/29572] Loss: 3.4843\n",
      "Batch [23600/29572] Loss: 4.8882\n",
      "Batch [23700/29572] Loss: 5.0081\n",
      "Batch [23800/29572] Loss: 4.2205\n",
      "Batch [23900/29572] Loss: 5.2424\n",
      "Batch [24000/29572] Loss: 5.2642\n",
      "Batch [24100/29572] Loss: 5.1722\n",
      "Batch [24200/29572] Loss: 4.4293\n",
      "Batch [24300/29572] Loss: 5.6834\n",
      "Batch [24400/29572] Loss: 5.3808\n",
      "Batch [24500/29572] Loss: 5.0911\n",
      "Batch [24600/29572] Loss: 5.7391\n",
      "Batch [24700/29572] Loss: 5.2227\n",
      "Batch [24800/29572] Loss: 4.8779\n",
      "Batch [24900/29572] Loss: 4.8103\n",
      "Batch [25000/29572] Loss: 3.9803\n",
      "Batch [25100/29572] Loss: 4.6724\n",
      "Batch [25200/29572] Loss: 4.9597\n",
      "Batch [25300/29572] Loss: 4.8780\n",
      "Batch [25400/29572] Loss: 5.0137\n",
      "Batch [25500/29572] Loss: 5.1305\n",
      "Batch [25600/29572] Loss: 5.6099\n",
      "Batch [25700/29572] Loss: 4.8734\n",
      "Batch [25800/29572] Loss: 4.0147\n",
      "Batch [25900/29572] Loss: 5.1459\n",
      "Batch [26000/29572] Loss: 4.9701\n",
      "Batch [26100/29572] Loss: 5.0213\n",
      "Batch [26200/29572] Loss: 5.0730\n",
      "Batch [26300/29572] Loss: 5.2413\n",
      "Batch [26400/29572] Loss: 5.0238\n",
      "Batch [26500/29572] Loss: 5.4564\n",
      "Batch [26600/29572] Loss: 5.6031\n",
      "Batch [26700/29572] Loss: 5.5166\n",
      "Batch [26800/29572] Loss: 5.9033\n",
      "Batch [26900/29572] Loss: 4.9774\n",
      "Batch [27000/29572] Loss: 5.8717\n",
      "Batch [27100/29572] Loss: 3.9794\n",
      "Batch [27200/29572] Loss: 5.0122\n",
      "Batch [27300/29572] Loss: 4.9541\n",
      "Batch [27400/29572] Loss: 5.9721\n",
      "Batch [27500/29572] Loss: 4.4371\n",
      "Batch [27600/29572] Loss: 5.3620\n",
      "Batch [27700/29572] Loss: 5.2658\n",
      "Batch [27800/29572] Loss: 3.9493\n",
      "Batch [27900/29572] Loss: 4.4810\n",
      "Batch [28000/29572] Loss: 5.4292\n",
      "Batch [28100/29572] Loss: 5.4680\n",
      "Batch [28200/29572] Loss: 4.9439\n",
      "Batch [28300/29572] Loss: 4.8970\n",
      "Batch [28400/29572] Loss: 5.3439\n",
      "Batch [28500/29572] Loss: 4.9427\n",
      "Batch [28600/29572] Loss: 5.4693\n",
      "Batch [28700/29572] Loss: 4.4687\n",
      "Batch [28800/29572] Loss: 5.0613\n",
      "Batch [28900/29572] Loss: 4.5757\n",
      "Batch [29000/29572] Loss: 5.7458\n",
      "Batch [29100/29572] Loss: 5.7299\n",
      "Batch [29200/29572] Loss: 4.4647\n",
      "Batch [29300/29572] Loss: 5.0808\n",
      "Batch [29400/29572] Loss: 4.4825\n",
      "Batch [29500/29572] Loss: 4.9929\n",
      "Train Loss: 4.9286\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- Epoch {epoch + 1} ---\")\n",
    "\n",
    "    train_loss = train_epoch(\n",
    "        encoder, decoder, dataloader,\n",
    "        criterion, optimizer, device\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_loss = validate(\n",
    "        encoder, decoder, val_dataloader,\n",
    "        criterion, device\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1011404,
     "sourceId": 1706129,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 4235478,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 163770428,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 184692496,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9437.990584,
   "end_time": "2025-04-17T09:54:56.036348",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-17T07:17:38.045764",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
