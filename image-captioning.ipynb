{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":1706129,"sourceType":"datasetVersion","datasetId":1011404},{"sourceId":4235478,"sourceType":"kernelVersion"},{"sourceId":163770428,"sourceType":"kernelVersion"},{"sourceId":184692496,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nfrom collections import defaultdict\n\n# Paths\nDATA_DIR = '/kaggle/input/coco-2017-dataset/coco2017'\nANNOTATION_FILE = os.path.join(DATA_DIR, 'annotations', 'captions_train2017.json')\nANNOTATION_FILE2 = os.path.join(DATA_DIR, 'annotations', 'captions_val2017.json')\nIMAGE_FOLDER = os.path.join(DATA_DIR, 'train2017')\nIMAGE_FOLDER2 = os.path.join(DATA_DIR, 'val2017')\n\n# Load annotations\nwith open(ANNOTATION_FILE, 'r') as f:\n    annotations = json.load(f)\n\n# Build a dictionary: image_id -> list of captions\ncaptions_dict = defaultdict(list)\nfor ann in annotations['annotations']:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    captions_dict[image_id].append(caption)\n\n# Check sample\nsample_image_id = list(captions_dict.keys())[0]\nprint(f\"Image ID: {sample_image_id}\")\nprint(\"Captions:\")\nfor cap in captions_dict[sample_image_id]:\n    print(\"-\", cap)","metadata":{"_uuid":"d961bf5c-64df-40ca-bd69-4a4586029471","_cell_guid":"62deee2b-37ec-4dc8-8ab8-6e28389729dd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-16T08:20:48.525079Z","iopub.execute_input":"2025-04-16T08:20:48.525485Z","iopub.status.idle":"2025-04-16T08:20:50.011301Z","shell.execute_reply.started":"2025-04-16T08:20:48.525455Z","shell.execute_reply":"2025-04-16T08:20:50.010381Z"}},"outputs":[{"name":"stdout","text":"Image ID: 203564\nCaptions:\n- A bicycle replica with a clock as the front wheel.\n- The bike has a clock as a tire.\n- A black metal bicycle with a clock inside the front wheel.\n- A bicycle figurine in which the front wheel is replaced with a clock\n\n- A clock with the appearance of the wheel of a bicycle \n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('punkt')  # for word_tokenize\nfrom nltk.tokenize import word_tokenize\n\ndef clean_caption(caption):\n    caption = caption.lower()                            # Lowercase\n    caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)        # Remove punctuation\n    caption = re.sub(r\"\\s+\", \" \", caption).strip()       # Trim extra spaces\n    return caption\n\n# Clean and tokenize all captions\ncleaned_captions_dict = {}\nfor image_id, captions in captions_dict.items():\n    cleaned_captions = []\n    for cap in captions:\n        clean_cap = clean_caption(cap)\n        tokens = word_tokenize(clean_cap)\n        cleaned_captions.append(tokens)\n    cleaned_captions_dict[image_id] = cleaned_captions\n\n# Check cleaned sample\nprint(\"Cleaned captions for image ID:\", sample_image_id)\nfor cap in cleaned_captions_dict[sample_image_id]:\n    print(cap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:20:50.012418Z","iopub.execute_input":"2025-04-16T08:20:50.012668Z","iopub.status.idle":"2025-04-16T08:21:44.683959Z","shell.execute_reply.started":"2025-04-16T08:20:50.012648Z","shell.execute_reply":"2025-04-16T08:21:44.683154Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nCleaned captions for image ID: 203564\n['a', 'bicycle', 'replica', 'with', 'a', 'clock', 'as', 'the', 'front', 'wheel']\n['the', 'bike', 'has', 'a', 'clock', 'as', 'a', 'tire']\n['a', 'black', 'metal', 'bicycle', 'with', 'a', 'clock', 'inside', 'the', 'front', 'wheel']\n['a', 'bicycle', 'figurine', 'in', 'which', 'the', 'front', 'wheel', 'is', 'replaced', 'with', 'a', 'clock']\n['a', 'clock', 'with', 'the', 'appearance', 'of', 'the', 'wheel', 'of', 'a', 'bicycle']\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"from collections import Counter\n\nmin_word_freq = 5  # You can tune this\nword_freq = Counter()\n\n# Count word frequencies\nfor captions in cleaned_captions_dict.values():\n    for tokens in captions:\n        word_freq.update(tokens)\n\n# Filter words below the threshold\nwords = [word for word in word_freq if word_freq[word] >= min_word_freq]\n\n# Special tokens\nword_map = {\n    '<pad>': 0,\n    '<start>': 1,\n    '<end>': 2,\n    '<unk>': 3\n}\n\n# Add the remaining words\nfor i, word in enumerate(words, start=4):\n    word_map[word] = i\n\n# Reverse map\nidx2word = {v: k for k, v in word_map.items()}\n\nprint(f\"Vocabulary size: {len(word_map)}\")\nprint(\"Sample word map entries:\")\nfor i, (word, idx) in enumerate(list(word_map.items())[:10]):\n    print(f\"{word}: {idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:44.685323Z","iopub.execute_input":"2025-04-16T08:21:44.685545Z","iopub.status.idle":"2025-04-16T08:21:45.834597Z","shell.execute_reply.started":"2025-04-16T08:21:44.685526Z","shell.execute_reply":"2025-04-16T08:21:45.833746Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 10307\nSample word map entries:\n<pad>: 0\n<start>: 1\n<end>: 2\n<unk>: 3\na: 4\nbicycle: 5\nreplica: 6\nwith: 7\nclock: 8\nas: 9\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"encoded_captions = {}\n\nfor image_id, captions in cleaned_captions_dict.items():\n    encoded = []\n    for tokens in captions:\n        # Encode each word or use <unk> if not in vocab\n        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n        # Add <start> and <end> tokens\n        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n        encoded.append(enc)\n    encoded_captions[image_id] = encoded\n\n# Check sample\nprint(\"Encoded captions for image ID:\", sample_image_id)\nfor cap in encoded_captions[sample_image_id]:\n    print(cap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:45.835975Z","iopub.execute_input":"2025-04-16T08:21:45.836327Z","iopub.status.idle":"2025-04-16T08:21:48.454187Z","shell.execute_reply.started":"2025-04-16T08:21:45.836277Z","shell.execute_reply":"2025-04-16T08:21:48.453240Z"}},"outputs":[{"name":"stdout","text":"Encoded captions for image ID: 203564\n[1, 4, 5, 6, 7, 4, 8, 9, 10, 11, 12, 2]\n[1, 10, 13, 14, 4, 8, 9, 4, 15, 2]\n[1, 4, 16, 17, 5, 7, 4, 8, 18, 10, 11, 12, 2]\n[1, 4, 5, 19, 20, 21, 10, 11, 12, 22, 23, 7, 4, 8, 2]\n[1, 4, 8, 7, 10, 24, 25, 10, 12, 25, 4, 5, 2]\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import json\n\n# Save encoded captions\nwith open('encoded_captions.json', 'w') as f:\n    json.dump({str(k): v for k, v in encoded_captions.items()}, f)\n\n# Save word map\nwith open('word_map.json', 'w') as f:\n    json.dump(word_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:48.455174Z","iopub.execute_input":"2025-04-16T08:21:48.455530Z","iopub.status.idle":"2025-04-16T08:21:53.252103Z","shell.execute_reply.started":"2025-04-16T08:21:48.455496Z","shell.execute_reply":"2025-04-16T08:21:53.251448Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nimport json\nimport os\n\nclass CaptionDataset(Dataset):\n    def __init__(self, image_folder, encoded_captions_file, word_map_file, transform=None):\n        # Load encoded captions and word map\n        with open(encoded_captions_file, 'r') as j:\n            self.captions = json.load(j)\n        with open(word_map_file, 'r') as j:\n            self.word_map = json.load(j)\n\n        self.image_folder = image_folder\n        self.image_ids = list(self.captions.keys())\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        image_path = os.path.join(self.image_folder, f\"{int(image_id):012}.jpg\")\n        \n        # Load image\n        img = Image.open(image_path).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n\n        # Randomly select one caption for the image\n        caps = self.captions[image_id]\n        caption = random.choice(caps)\n        caption = torch.tensor(caption, dtype=torch.long)\n\n        return img, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:53.252970Z","iopub.execute_input":"2025-04-16T08:21:53.253276Z","iopub.status.idle":"2025-04-16T08:21:53.261560Z","shell.execute_reply.started":"2025-04-16T08:21:53.253246Z","shell.execute_reply":"2025-04-16T08:21:53.260770Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def caption_collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle batches of (image, caption) with variable-length captions.\n    \"\"\"\n    images = []\n    captions = []\n\n    for img, cap in batch:\n        images.append(img)\n        captions.append(cap)\n\n    # Stack images (they are all same size)\n    images = torch.stack(images, dim=0)\n\n    # Pad captions to the max length in the batch\n    lengths = [len(cap) for cap in captions]\n    max_len = max(lengths)\n    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        padded_captions[i, :end] = cap[:end]\n\n    return images, padded_captions, torch.tensor(lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:53.262143Z","iopub.execute_input":"2025-04-16T08:21:53.262400Z","iopub.status.idle":"2025-04-16T08:21:53.282029Z","shell.execute_reply.started":"2025-04-16T08:21:53.262379Z","shell.execute_reply":"2025-04-16T08:21:53.281382Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # from ImageNet\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Dataset\ndataset = CaptionDataset(\n    image_folder='/kaggle/input/coco-2017-dataset/coco2017/train2017',\n    encoded_captions_file='encoded_captions.json',\n    word_map_file='word_map.json',\n    transform=transform\n)\n\n# DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=caption_collate_fn\n)\n\n# Check sample batch\nfor images, captions, lengths in dataloader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Caption batch shape:\", captions.shape)\n    print(\"Lengths:\", lengths)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:53.284867Z","iopub.execute_input":"2025-04-16T08:21:53.285065Z","iopub.status.idle":"2025-04-16T08:21:55.564485Z","shell.execute_reply.started":"2025-04-16T08:21:53.285047Z","shell.execute_reply":"2025-04-16T08:21:55.563410Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([4, 3, 256, 256])\nCaption batch shape: torch.Size([4, 13])\nLengths: tensor([12, 13, 12, 11])\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"# ----------- Process Validation Captions -----------\nwith open(ANNOTATION_FILE2, 'r') as f:\n    val_annotations = json.load(f)\n\nval_captions_dict = defaultdict(list)\nfor ann in val_annotations['annotations']:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    val_captions_dict[image_id].append(caption)\n\n# Clean and tokenize validation captions\ncleaned_val_captions_dict = {}\nfor image_id, captions in val_captions_dict.items():\n    cleaned_captions = []\n    for cap in captions:\n        clean_cap = clean_caption(cap)\n        tokens = word_tokenize(clean_cap)\n        cleaned_captions.append(tokens)\n    cleaned_val_captions_dict[image_id] = cleaned_captions\n\n# Encode validation captions\nencoded_val_captions = {}\nfor image_id, captions in cleaned_val_captions_dict.items():\n    encoded = []\n    for tokens in captions:\n        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n        encoded.append(enc)\n    encoded_val_captions[image_id] = encoded\n\n# Save encoded val captions\nwith open('encoded_captions_val.json', 'w') as f:\n    json.dump({str(k): v for k, v in encoded_val_captions.items()}, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:55.566582Z","iopub.execute_input":"2025-04-16T08:21:55.566864Z","iopub.status.idle":"2025-04-16T08:21:58.086850Z","shell.execute_reply.started":"2025-04-16T08:21:55.566838Z","shell.execute_reply":"2025-04-16T08:21:58.086127Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Validation dataset\nval_dataset = CaptionDataset(\n    image_folder='/kaggle/input/coco-2017-dataset/coco2017/val2017',\n    encoded_captions_file='encoded_captions_val.json',  # You need to create this\n    word_map_file='word_map.json',\n    transform=transform\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=caption_collate_fn\n)\n\n# Check sample batch\nfor images, captions, lengths in dataloader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Caption batch shape:\", captions.shape)\n    print(\"Lengths:\", lengths)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:58.087699Z","iopub.execute_input":"2025-04-16T08:21:58.087995Z","iopub.status.idle":"2025-04-16T08:21:58.456922Z","shell.execute_reply.started":"2025-04-16T08:21:58.087962Z","shell.execute_reply":"2025-04-16T08:21:58.455919Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([4, 3, 256, 256])\nCaption batch shape: torch.Size([4, 15])\nLengths: tensor([12, 12, 15, 12])\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"## Attention Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Linear layer to transform encoder's output\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # Combine them and produce scalar energy\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # Softmax over the pixels\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: encoded images, shape -> (batch_size, num_pixels, encoder_dim)\n        decoder_hidden: previous decoder hidden state, shape -> (batch_size, decoder_dim)\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:58.458377Z","iopub.execute_input":"2025-04-16T08:21:58.458676Z","iopub.status.idle":"2025-04-16T08:21:58.465233Z","shell.execute_reply.started":"2025-04-16T08:21:58.458651Z","shell.execute_reply":"2025-04-16T08:21:58.464440Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"## Decoder with Attention","metadata":{}},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # initialize hidden state\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # initialize cell state\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # create a gating scalar\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # output layer\n\n        self.init_weights()  # initialize weights\n\n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, shape (batch_size, num_pixels, encoder_dim)\n        :param encoded_captions: encoded captions, shape (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, shape (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths\n        # Corrected line\n        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word prediction scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(encoder_out.device)\n\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar\n            attention_weighted_encoding = gate * attention_weighted_encoding\n\n            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n            h, c = self.decode_step(input_lstm, (h[:batch_size_t], c[:batch_size_t]))  # LSTM step\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:58.466065Z","iopub.execute_input":"2025-04-16T08:21:58.466259Z","iopub.status.idle":"2025-04-16T08:21:58.480638Z","shell.execute_reply.started":"2025-04-16T08:21:58.466243Z","shell.execute_reply":"2025-04-16T08:21:58.479851Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"import torchvision.models as models\n\nclass Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.cnn = models.resnet101(pretrained=True)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune(fine_tune=False)\n\n    def forward(self, images):\n        x = self.cnn.conv1(images)\n        x = self.cnn.bn1(x)\n        x = self.cnn.relu(x)\n        x = self.cnn.maxpool(x)\n\n        x = self.cnn.layer1(x)\n        x = self.cnn.layer2(x)\n        x = self.cnn.layer3(x)\n        x = self.cnn.layer4(x)  # Shape: (batch_size, 2048, 7, 7)\n        \n        x = self.adaptive_pool(x)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        x = x.permute(0, 2, 3, 1)  # (batch_size, encoded_size, encoded_size, 2048)\n        x = x.view(x.size(0), -1, x.size(-1))  # (batch_size, num_pixels=encoded_size^2, 2048)\n        return x\n\n    def fine_tune(self, fine_tune=True):\n        for p in self.cnn.parameters():\n            p.requires_grad = fine_tune","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:58.481563Z","iopub.execute_input":"2025-04-16T08:21:58.481876Z","iopub.status.idle":"2025-04-16T08:21:58.496500Z","shell.execute_reply.started":"2025-04-16T08:21:58.481847Z","shell.execute_reply":"2025-04-16T08:21:58.495836Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Test encoder-decoder integration\nimport torch\n\n# Check if GPU is available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\nencoder = Encoder().to(device)\ndecoder = DecoderWithAttention(\n    attention_dim=512,\n    embed_dim=512,\n    decoder_dim=512,\n    vocab_size=len(word_map),\n    encoder_dim=2048,\n    dropout=0.5\n).to(device)\n\n# Test forward pass\nimages, captions, lengths = next(iter(dataloader))\nimages = images.to(device)\ncaptions = captions.to(device)\n\nencoder_out = encoder(images)\npredictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths))\n\nprint(\"Encoder output shape:\", encoder_out.shape)  # Should be (batch_size, 196, 2048)\nprint(\"Predictions shape:\", predictions.shape)     # Should be (batch_size, max_len, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:58.497412Z","iopub.execute_input":"2025-04-16T08:21:58.497709Z","iopub.status.idle":"2025-04-16T08:21:59.930741Z","shell.execute_reply.started":"2025-04-16T08:21:58.497679Z","shell.execute_reply":"2025-04-16T08:21:59.929691Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nEncoder output shape: torch.Size([4, 196, 2048])\nPredictions shape: torch.Size([4, 10, 10307])\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-62-91d566bfc588>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths))\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"class MaskedCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=0)  # ignore <pad>\n\n    def forward(self, predictions, targets, lengths):\n        batch_size, max_len, vocab_size = predictions.shape\n\n        predictions = predictions.view(-1, vocab_size)      # (batch_size * max_len, vocab_size)\n        targets = targets.contiguous().view(-1)              # (batch_size * max_len)\n\n        losses = self.criterion(predictions, targets)        # (batch_size * max_len)\n\n        # Create mask\n        mask = torch.arange(max_len).expand(batch_size, max_len).to(lengths.device)\n        mask = (mask < lengths.unsqueeze(1)).float()         # (batch_size, max_len)\n        mask = mask.view(-1)                                 # Flatten to (batch_size * max_len)\n\n        losses = losses * mask\n        return losses.sum() / mask.sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:59.931741Z","iopub.execute_input":"2025-04-16T08:21:59.932045Z","iopub.status.idle":"2025-04-16T08:21:59.937817Z","shell.execute_reply.started":"2025-04-16T08:21:59.932020Z","shell.execute_reply":"2025-04-16T08:21:59.936980Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"encoder = Encoder().to(device)\ndecoder = DecoderWithAttention(\n    attention_dim=512,\n    embed_dim=512,\n    decoder_dim=512,\n    vocab_size=len(word_map),\n    encoder_dim=2048,\n    dropout=0.5\n).to(device)\n\n# Only fine-tune the encoder's adaptive pool layer\nencoder_params = list(encoder.adaptive_pool.parameters()) + list(encoder.cnn.layer4.parameters())\ndecoder_params = decoder.parameters()\n\noptimizer = torch.optim.Adam(\n    params=[\n        {'params': encoder_params, 'lr': 1e-4},  # Lower LR for encoder\n        {'params': decoder_params, 'lr': 4e-4}    # Higher LR for decoder\n    ],\n    weight_decay=1e-5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:21:59.938952Z","iopub.execute_input":"2025-04-16T08:21:59.939195Z","iopub.status.idle":"2025-04-16T08:22:01.033980Z","shell.execute_reply.started":"2025-04-16T08:21:59.939165Z","shell.execute_reply":"2025-04-16T08:22:01.033248Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"def train_epoch(encoder, decoder, dataloader, criterion, optimizer, device, grad_clip=5.0):\n    encoder.train()\n    decoder.train()\n    total_loss = 0\n    \n    for i, (images, captions, lengths) in enumerate(dataloader):\n        images = images.to(device)\n        captions = captions.to(device)\n        lengths = torch.tensor(lengths).to(device)\n        \n        # Forward pass\n        encoder_out = encoder(images)\n        predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n        \n        # Remove <start> token and truncate to actual lengths\n        targets = captions[:, 1:]  # (batch_size, max_len-1)\n        predictions = predictions[:, :max(decode_lengths), :]  # (batch_size, actual_max_len, vocab_size)\n\n        # Calculate loss\n        loss = criterion(predictions, targets, decode_lengths)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n        torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        if i % 100 == 0:\n            print(f\"Batch [{i}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n    \n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:22:01.034779Z","iopub.execute_input":"2025-04-16T08:22:01.035100Z","iopub.status.idle":"2025-04-16T08:22:01.041738Z","shell.execute_reply.started":"2025-04-16T08:22:01.035070Z","shell.execute_reply":"2025-04-16T08:22:01.040790Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"def validate(encoder, decoder, val_loader, criterion, device):\n    encoder.eval()\n    decoder.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for images, captions, lengths in val_loader:\n            images = images.to(device)\n            captions = captions.to(device)\n            lengths = torch.tensor(lengths).to(device)\n            \n            encoder_out = encoder(images)\n            predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n            \n            targets = captions[:, 1:]\n            predictions = predictions[:, :max(decode_lengths), :]\n            \n            loss = criterion(predictions, targets, decode_lengths)\n            total_loss += loss.item()\n    \n    return total_loss / len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:22:01.042496Z","iopub.execute_input":"2025-04-16T08:22:01.042698Z","iopub.status.idle":"2025-04-16T08:22:01.060427Z","shell.execute_reply.started":"2025-04-16T08:22:01.042678Z","shell.execute_reply":"2025-04-16T08:22:01.059717Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"# Initialize components\ncriterion = MaskedCrossEntropyLoss().to(device)\nnum_epochs = 10  # For initial test\n\n# Quick test with 1 batch\ntest_images, test_captions, test_lengths = next(iter(dataloader))\ntest_images = test_images.to(device)\ntest_captions = test_captions.to(device)\ntest_lengths = torch.tensor(test_lengths).to(device)\n\n# Forward test\nencoder_out = encoder(test_images)\npredictions, _, decode_lengths, _, _ = decoder(encoder_out, test_captions, test_lengths)\ntargets = test_captions[:, 1:]\n\n# Convert decode_lengths to tensor\n# decode_lengths = torch.tensor(decode_lengths).to(device)\n\n# loss = criterion(predictions, targets, decode_lengths)\n# decode_lengths = torch.tensor(decode_lengths).to(device)\n# loss = criterion(predictions, targets, decode_lengths)\n\n\nprint(f\"Initial loss: {loss.item():.4f}\")  # Should be ~log(vocab_size) = ~9.2 for vocab_size=10307\noptimizer.step()  # Verify backprop works without errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:22:01.061249Z","iopub.execute_input":"2025-04-16T08:22:01.061514Z","iopub.status.idle":"2025-04-16T08:22:01.448152Z","shell.execute_reply.started":"2025-04-16T08:22:01.061494Z","shell.execute_reply":"2025-04-16T08:22:01.447038Z"}},"outputs":[{"name":"stdout","text":"Initial loss: 8.6433\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-67-99fa07994068>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  test_lengths = torch.tensor(test_lengths).to(device)\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print(f\"\\n--- Epoch {epoch + 1} ---\")\n\n    train_loss = train_epoch(\n        encoder, decoder, dataloader,\n        criterion, optimizer, device\n    )\n    print(f\"Train Loss: {train_loss:.4f}\")\n\n    val_loss = validate(\n        encoder, decoder, val_dataloader,\n        criterion, device\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:22:01.449470Z","iopub.execute_input":"2025-04-16T08:22:01.449829Z","iopub.status.idle":"2025-04-16T08:22:01.808365Z","shell.execute_reply.started":"2025-04-16T08:22:01.449793Z","shell.execute_reply":"2025-04-16T08:22:01.807101Z"}},"outputs":[{"name":"stdout","text":"\n--- Epoch 1 ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-65-d6ba65574821>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  lengths = torch.tensor(lengths).to(device)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-4b90ba6574fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n--- Epoch {epoch + 1} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     train_loss = train_epoch(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-d6ba65574821>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, dataloader, criterion, optimizer, device, grad_clip)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-0dbac41c6d3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, predictions, targets, lengths)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Create mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# (batch_size, max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                                 \u001b[0;31m# Flatten to (batch_size * max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'device'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'device'","output_type":"error"}],"execution_count":68}]}