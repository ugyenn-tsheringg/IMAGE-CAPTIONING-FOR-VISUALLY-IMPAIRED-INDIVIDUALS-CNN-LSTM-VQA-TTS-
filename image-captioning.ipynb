{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":1706129,"sourceType":"datasetVersion","datasetId":1011404},{"sourceId":4235478,"sourceType":"kernelVersion"},{"sourceId":163770428,"sourceType":"kernelVersion"},{"sourceId":184692496,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport random\nfrom collections import defaultdict\n\n# Paths\nDATA_DIR = '/kaggle/input/coco-2017-dataset/coco2017'\nANNOTATION_FILE = os.path.join(DATA_DIR, 'annotations', 'captions_train2017.json')\nANNOTATION_FILE2 = os.path.join(DATA_DIR, 'annotations', 'captions_val2017.json')\nIMAGE_FOLDER = os.path.join(DATA_DIR, 'train2017')\nIMAGE_FOLDER2 = os.path.join(DATA_DIR, 'val2017')\n\n# Load annotations\nwith open(ANNOTATION_FILE, 'r') as f:\n    annotations = json.load(f)\n\n# Build a dictionary: image_id -> list of captions\ncaptions_dict = defaultdict(list)\nfor ann in annotations['annotations']:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    captions_dict[image_id].append(caption)\n\n# Check sample\n# sample_image_id = list(captions_dict.keys())[0]\nsample_image_id = random.choice(list(captions_dict.keys()))\nprint(f\"Image ID: {sample_image_id}\")\nprint(\"Captions:\")\nfor cap in captions_dict[sample_image_id]:\n    print(\"-\", cap)","metadata":{"_uuid":"d961bf5c-64df-40ca-bd69-4a4586029471","_cell_guid":"62deee2b-37ec-4dc8-8ab8-6e28389729dd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-20T16:24:12.281930Z","iopub.execute_input":"2025-04-20T16:24:12.282212Z","iopub.status.idle":"2025-04-20T16:24:14.266226Z","shell.execute_reply.started":"2025-04-20T16:24:12.282190Z","shell.execute_reply":"2025-04-20T16:24:14.265361Z"}},"outputs":[{"name":"stdout","text":"Image ID: 283627\nCaptions:\n- A black dog with a red collar under a pink blanket.\n- A dog asleep in a bed with a blanket over him \n- Black dog in a bed on a pillow under a pink blanket \n- a black lab lying in bed under covers\n- A dog takes a nap under a blanket. \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import re\nimport nltk\nnltk.download('punkt')  # for word_tokenize\nfrom nltk.tokenize import word_tokenize\n\ndef clean_caption(caption):\n    caption = caption.lower()                            # Lowercase\n    caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)        # Remove punctuation\n    caption = re.sub(r\"\\s+\", \" \", caption).strip()       # Trim extra spaces\n    return caption\n\n# Clean and tokenize all captions\ncleaned_captions_dict = {}\nfor image_id, captions in captions_dict.items():\n    cleaned_captions = []\n    for cap in captions:\n        clean_cap = clean_caption(cap)\n        tokens = word_tokenize(clean_cap)\n        cleaned_captions.append(tokens)\n    cleaned_captions_dict[image_id] = cleaned_captions\n\n# Check cleaned sample\nprint(\"Cleaned captions for image ID:\", sample_image_id)\nfor cap in cleaned_captions_dict[sample_image_id]:\n    print(cap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:24:14.267487Z","iopub.execute_input":"2025-04-20T16:24:14.267845Z","iopub.status.idle":"2025-04-20T16:25:11.569621Z","shell.execute_reply.started":"2025-04-20T16:24:14.267813Z","shell.execute_reply":"2025-04-20T16:25:11.568638Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nCleaned captions for image ID: 283627\n['a', 'black', 'dog', 'with', 'a', 'red', 'collar', 'under', 'a', 'pink', 'blanket']\n['a', 'dog', 'asleep', 'in', 'a', 'bed', 'with', 'a', 'blanket', 'over', 'him']\n['black', 'dog', 'in', 'a', 'bed', 'on', 'a', 'pillow', 'under', 'a', 'pink', 'blanket']\n['a', 'black', 'lab', 'lying', 'in', 'bed', 'under', 'covers']\n['a', 'dog', 'takes', 'a', 'nap', 'under', 'a', 'blanket']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os \n# os.makedirs('/kaggle/working/models', exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:25:11.571391Z","iopub.execute_input":"2025-04-20T16:25:11.571806Z","iopub.status.idle":"2025-04-20T16:25:11.575552Z","shell.execute_reply.started":"2025-04-20T16:25:11.571784Z","shell.execute_reply":"2025-04-20T16:25:11.574608Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# open('/kaggle/working/models/__init__.py', 'a').close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:25:11.576994Z","iopub.execute_input":"2025-04-20T16:25:11.577300Z","iopub.status.idle":"2025-04-20T16:25:11.593445Z","shell.execute_reply.started":"2025-04-20T16:25:11.577270Z","shell.execute_reply":"2025-04-20T16:25:11.592608Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from collections import Counter\n\nmin_word_freq = 5  # You can tune this\nword_freq = Counter()\n\n# Count word frequencies\nfor captions in cleaned_captions_dict.values():\n    for tokens in captions:\n        word_freq.update(tokens)\n\n# Filter words below the threshold\nwords = [word for word in word_freq if word_freq[word] >= min_word_freq]\n\n# Special tokens\nword_map = {\n    '<pad>': 0,\n    '<start>': 1,\n    '<end>': 2,\n    '<unk>': 3\n}\n\n# Add the remaining words\nfor i, word in enumerate(words, start=4):\n    word_map[word] = i\n\n# Reverse map\nidx2word = {v: k for k, v in word_map.items()}\n\nprint(f\"Vocabulary size: {len(word_map)}\")\nprint(\"Sample word map entries:\")\nfor i, (word, idx) in enumerate(list(word_map.items())[:10]):\n    print(f\"{word}: {idx}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:25:11.594322Z","iopub.execute_input":"2025-04-20T16:25:11.594634Z","iopub.status.idle":"2025-04-20T16:25:12.732892Z","shell.execute_reply.started":"2025-04-20T16:25:11.594607Z","shell.execute_reply":"2025-04-20T16:25:12.731610Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 10307\nSample word map entries:\n<pad>: 0\n<start>: 1\n<end>: 2\n<unk>: 3\na: 4\nbicycle: 5\nreplica: 6\nwith: 7\nclock: 8\nas: 9\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"encoded_captions = {}\n\nfor image_id, captions in cleaned_captions_dict.items():\n    encoded = []\n    for tokens in captions:\n        # Encode each word or use <unk> if not in vocab\n        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n        # Add <start> and <end> tokens\n        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n        encoded.append(enc)\n    encoded_captions[image_id] = encoded\n\n# Check sample\nprint(\"Encoded captions for image ID:\", sample_image_id)\nfor cap in encoded_captions[sample_image_id]:\n    print(cap)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:25:12.733689Z","iopub.execute_input":"2025-04-20T16:25:12.733898Z","iopub.status.idle":"2025-04-20T16:25:15.481706Z","shell.execute_reply.started":"2025-04-20T16:25:12.733881Z","shell.execute_reply":"2025-04-20T16:25:15.480810Z"}},"outputs":[{"name":"stdout","text":"Encoded captions for image ID: 283627\n[1, 4, 16, 372, 7, 4, 89, 1490, 859, 4, 328, 1018, 2]\n[1, 4, 372, 945, 20, 4, 779, 7, 4, 1018, 286, 780, 2]\n[1, 16, 372, 20, 4, 779, 39, 4, 1184, 859, 4, 328, 1018, 2]\n[1, 4, 16, 6338, 1054, 20, 779, 859, 2302, 2]\n[1, 4, 372, 475, 4, 1439, 859, 4, 1018, 2]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import json\n\n# Save encoded captions\nwith open('encoded_captions.json', 'w') as f:\n    json.dump({str(k): v for k, v in encoded_captions.items()}, f)\n\n# Save word map\nwith open('word_map.json', 'w') as f:\n    json.dump(word_map, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:25:15.482668Z","iopub.execute_input":"2025-04-20T16:25:15.483067Z","iopub.status.idle":"2025-04-20T16:25:20.140633Z","shell.execute_reply.started":"2025-04-20T16:25:15.483034Z","shell.execute_reply":"2025-04-20T16:25:20.139739Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:25:20.144025Z","iopub.execute_input":"2025-04-20T16:25:20.144269Z","iopub.status.idle":"2025-04-20T16:28:20.680038Z","shell.execute_reply.started":"2025-04-20T16:25:20.144249Z","shell.execute_reply":"2025-04-20T16:28:20.678832Z"}},"outputs":[{"name":"stdout","text":"--2025-04-20 16:25:20--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2025-04-20 16:25:20--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n--2025-04-20 16:25:21--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\nResolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\nConnecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: ‘glove.6B.zip’\n\nglove.6B.zip        100%[===================>] 822.24M  5.03MB/s    in 2m 39s  \n\n2025-04-20 16:28:00 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n\nArchive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt       \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\n\n# Path to GloVe 300d\nglove_path = '/kaggle/working/glove.6B.300d.txt'\nembedding_dim = 300\nvocab_size = len(word_map)\n\n# Load GloVe embeddings\nprint(\"Loading GloVe...\")\nglove = {}\nwith open(glove_path, 'r', encoding='utf-8') as f:\n    for line in f:\n        tokens = line.split()\n        word = tokens[0]\n        vec = np.array(tokens[1:], dtype=np.float32)\n        glove[word] = vec\n\n# Create embedding matrix\nprint(\"Building embedding matrix...\")\nembedding_matrix = np.random.uniform(-0.1, 0.1, (vocab_size, embedding_dim)).astype(np.float32)\n\nfor word, idx in word_map.items():\n    if word in glove:\n        embedding_matrix[idx] = glove[word]\n\nprint(\"Done. Shape:\", embedding_matrix.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:20.681971Z","iopub.execute_input":"2025-04-20T16:28:20.682220Z","iopub.status.idle":"2025-04-20T16:28:43.899946Z","shell.execute_reply.started":"2025-04-20T16:28:20.682196Z","shell.execute_reply":"2025-04-20T16:28:43.898950Z"}},"outputs":[{"name":"stdout","text":"Loading GloVe...\nBuilding embedding matrix...\nDone. Shape: (10307, 300)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nimport json\nimport os\n\nclass CaptionDataset(Dataset):\n    def __init__(self, image_folder, encoded_captions_file, word_map_file, transform=None):\n        # Load encoded captions and word map\n        with open(encoded_captions_file, 'r') as j:\n            self.captions = json.load(j)\n        with open(word_map_file, 'r') as j:\n            self.word_map = json.load(j)\n\n        self.image_folder = image_folder\n        self.image_ids = list(self.captions.keys())\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, index):\n        image_id = self.image_ids[index]\n        image_path = os.path.join(self.image_folder, f\"{int(image_id):012}.jpg\")\n        \n        # Load image\n        img = Image.open(image_path).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n\n        # Randomly select one caption for the image\n        caps = self.captions[image_id]\n        caption = random.choice(caps)\n        caption = torch.tensor(caption, dtype=torch.long)\n\n        return img, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:43.901330Z","iopub.execute_input":"2025-04-20T16:28:43.901690Z","iopub.status.idle":"2025-04-20T16:28:48.594163Z","shell.execute_reply.started":"2025-04-20T16:28:43.901654Z","shell.execute_reply":"2025-04-20T16:28:48.593238Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def caption_collate_fn(batch):\n    \"\"\"\n    Custom collate function to handle batches of (image, caption) with variable-length captions.\n    \"\"\"\n    images = []\n    captions = []\n\n    for img, cap in batch:\n        images.append(img)\n        captions.append(cap)\n\n    # Stack images (they are all same size)\n    images = torch.stack(images, dim=0)\n\n    # Pad captions to the max length in the batch\n    lengths = [len(cap) for cap in captions]\n    max_len = max(lengths)\n    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        padded_captions[i, :end] = cap[:end]\n\n    return images, padded_captions, torch.tensor(lengths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:48.595052Z","iopub.execute_input":"2025-04-20T16:28:48.595461Z","iopub.status.idle":"2025-04-20T16:28:48.600483Z","shell.execute_reply.started":"2025-04-20T16:28:48.595437Z","shell.execute_reply":"2025-04-20T16:28:48.599621Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\n# Image preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # from ImageNet\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Dataset\ndataset = CaptionDataset(\n    image_folder='/kaggle/input/coco-2017-dataset/coco2017/train2017',\n    encoded_captions_file='encoded_captions.json',\n    word_map_file='word_map.json',\n    transform=transform\n)\n\n# DataLoader\ndataloader = DataLoader(\n    dataset,\n    batch_size=4,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=caption_collate_fn\n)\n\n# Check sample batch\nfor images, captions, lengths in dataloader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Caption batch shape:\", captions.shape)\n    print(\"Lengths:\", lengths)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:48.601296Z","iopub.execute_input":"2025-04-20T16:28:48.601570Z","iopub.status.idle":"2025-04-20T16:28:51.104926Z","shell.execute_reply.started":"2025-04-20T16:28:48.601542Z","shell.execute_reply":"2025-04-20T16:28:51.103800Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([4, 3, 256, 256])\nCaption batch shape: torch.Size([4, 13])\nLengths: tensor([10, 11, 12, 13])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ----------- Process Validation Captions -----------\nwith open(ANNOTATION_FILE2, 'r') as f:\n    val_annotations = json.load(f)\n\nval_captions_dict = defaultdict(list)\nfor ann in val_annotations['annotations']:\n    image_id = ann['image_id']\n    caption = ann['caption']\n    val_captions_dict[image_id].append(caption)\n\n# Clean and tokenize validation captions\ncleaned_val_captions_dict = {}\nfor image_id, captions in val_captions_dict.items():\n    cleaned_captions = []\n    for cap in captions:\n        clean_cap = clean_caption(cap)\n        tokens = word_tokenize(clean_cap)\n        cleaned_captions.append(tokens)\n    cleaned_val_captions_dict[image_id] = cleaned_captions\n\n# Encode validation captions\nencoded_val_captions = {}\nfor image_id, captions in cleaned_val_captions_dict.items():\n    encoded = []\n    for tokens in captions:\n        enc = [word_map.get(word, word_map['<unk>']) for word in tokens]\n        enc = [word_map['<start>']] + enc + [word_map['<end>']]\n        encoded.append(enc)\n    encoded_val_captions[image_id] = encoded\n\n# Save encoded val captions\nwith open('encoded_captions_val.json', 'w') as f:\n    json.dump({str(k): v for k, v in encoded_val_captions.items()}, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:51.106133Z","iopub.execute_input":"2025-04-20T16:28:51.106479Z","iopub.status.idle":"2025-04-20T16:28:53.856919Z","shell.execute_reply.started":"2025-04-20T16:28:51.106440Z","shell.execute_reply":"2025-04-20T16:28:53.855962Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Validation dataset\nval_dataset = CaptionDataset(\n    image_folder='/kaggle/input/coco-2017-dataset/coco2017/val2017',\n    encoded_captions_file='encoded_captions_val.json',  # You need to create this\n    word_map_file='word_map.json',\n    transform=transform\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n    collate_fn=caption_collate_fn\n)\n\n# Check sample batch\nfor images, captions, lengths in dataloader:\n    print(\"Image batch shape:\", images.shape)\n    print(\"Caption batch shape:\", captions.shape)\n    print(\"Lengths:\", lengths)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:53.857834Z","iopub.execute_input":"2025-04-20T16:28:53.858151Z","iopub.status.idle":"2025-04-20T16:28:54.184535Z","shell.execute_reply.started":"2025-04-20T16:28:53.858121Z","shell.execute_reply":"2025-04-20T16:28:54.183687Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([4, 3, 256, 256])\nCaption batch shape: torch.Size([4, 13])\nLengths: tensor([11, 13, 10, 13])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Attention Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Linear layer to transform encoder's output\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # Combine them and produce scalar energy\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # Softmax over the pixels\n\n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: encoded images, shape -> (batch_size, num_pixels, encoder_dim)\n        decoder_hidden: previous decoder hidden state, shape -> (batch_size, decoder_dim)\n        \"\"\"\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:54.185734Z","iopub.execute_input":"2025-04-20T16:28:54.186027Z","iopub.status.idle":"2025-04-20T16:28:54.192564Z","shell.execute_reply.started":"2025-04-20T16:28:54.185999Z","shell.execute_reply":"2025-04-20T16:28:54.191713Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Decoder with Attention","metadata":{}},{"cell_type":"code","source":"class DecoderWithAttention(nn.Module):\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n        self.embedding.weight.requires_grad = False  # Optional: freeze during training\n\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # initialize hidden state\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # initialize cell state\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # create a gating scalar\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # output layer\n\n        self.init_weights()  # initialize weights\n\n    def init_weights(self):\n        # self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, shape (batch_size, num_pixels, encoder_dim)\n        :param encoded_captions: encoded captions, shape (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, shape (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths\n        # Corrected line\n        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        decode_lengths = caption_lengths - 1\n\n        # Create tensors to hold word prediction scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(encoder_out.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(encoder_out.device)\n\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar\n            attention_weighted_encoding = gate * attention_weighted_encoding\n\n            input_lstm = torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1)\n            h, c = self.decode_step(input_lstm, (h[:batch_size_t], c[:batch_size_t]))  # LSTM step\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:54.193552Z","iopub.execute_input":"2025-04-20T16:28:54.193864Z","iopub.status.idle":"2025-04-20T16:28:54.212041Z","shell.execute_reply.started":"2025-04-20T16:28:54.193832Z","shell.execute_reply":"2025-04-20T16:28:54.211343Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torchvision.models as models\n\nclass Encoder(nn.Module):\n    def __init__(self, encoded_image_size=14):\n        super().__init__()\n        self.cnn = models.resnet101(pretrained=True)\n        # self.cnn = models.resnet101(weights=weights)\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n        self.fine_tune(fine_tune=False)\n\n    def forward(self, images):\n        x = self.cnn.conv1(images)\n        x = self.cnn.bn1(x)\n        x = self.cnn.relu(x)\n        x = self.cnn.maxpool(x)\n\n        x = self.cnn.layer1(x)\n        x = self.cnn.layer2(x)\n        x = self.cnn.layer3(x)\n        x = self.cnn.layer4(x)  # Shape: (batch_size, 2048, 7, 7)\n        \n        x = self.adaptive_pool(x)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        x = x.permute(0, 2, 3, 1)  # (batch_size, encoded_size, encoded_size, 2048)\n        x = x.view(x.size(0), -1, x.size(-1))  # (batch_size, num_pixels=encoded_size^2, 2048)\n        return x\n\n    def fine_tune(self, fine_tune=True):\n        for p in self.cnn.parameters():\n            p.requires_grad = fine_tune","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:54.212900Z","iopub.execute_input":"2025-04-20T16:28:54.213097Z","iopub.status.idle":"2025-04-20T16:28:54.232348Z","shell.execute_reply.started":"2025-04-20T16:28:54.213081Z","shell.execute_reply":"2025-04-20T16:28:54.231500Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import os\nimport glob\n\ndef save_checkpoint(encoder, decoder, optimizer, epoch, train_loss, val_loss, word_map, \n                   checkpoint_dir, best_val_loss=float('inf'), is_best=False):\n    \"\"\"Save model checkpoint\"\"\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Save regular checkpoint\n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n    \n    checkpoint = {\n        'epoch': epoch + 1,  # Save as next epoch to resume from\n        'encoder_state_dict': encoder.state_dict(),\n        'decoder_state_dict': decoder.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'word_map': word_map,\n        'best_val_loss': best_val_loss\n    }\n    \n    torch.save(checkpoint, checkpoint_path)\n    print(f\"Checkpoint saved: {checkpoint_path}\")\n    \n    # Save best model separately if this is the best one\n    if is_best:\n        best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n        torch.save(checkpoint, best_model_path)\n        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n\ndef resume_from_checkpoint(checkpoint_path, encoder, decoder, optimizer, device):\n    \"\"\"Load checkpoint and resume training\"\"\"\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    \n    # Load checkpoint on CPU to avoid GPU memory issues\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Load model states\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    \n    # Move models to device after loading\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n    \n    # Load optimizer state\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    # Return the starting epoch and best validation loss\n    start_epoch = checkpoint['epoch']  # Continue from next epoch\n    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n    \n    print(f\"Resuming from epoch {start_epoch} with best validation loss: {best_val_loss:.4f}\")\n    return encoder, decoder, optimizer, start_epoch, best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:54.233121Z","iopub.execute_input":"2025-04-20T16:28:54.233330Z","iopub.status.idle":"2025-04-20T16:28:54.250950Z","shell.execute_reply.started":"2025-04-20T16:28:54.233299Z","shell.execute_reply":"2025-04-20T16:28:54.250197Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Test encoder-decoder integration\nimport torch\nfrom torchvision.models import ResNet101_Weights\n\n# Check if GPU is available, otherwise use CPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# encoder = Encoder(weights=ResNet101_Weights.IMAGENET1K_V1).to(device)\nencoder = Encoder().to(device)\n# encoder = Encoder(weights=ResNet101_Weights.DEFAULT).to(device)\n\ndecoder = DecoderWithAttention(\n    attention_dim=512,\n    embed_dim=300,\n    decoder_dim=512,\n    vocab_size=len(word_map),\n    encoder_dim=2048,\n    dropout=0.5\n).to(device)\n\n# Test forward pass\nimages, captions, lengths = next(iter(dataloader))\nimages = images.to(device)\ncaptions = captions.to(device)\n\nencoder_out = encoder(images)\n# predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths, device=device))\npredictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths).clone().detach().to(device))\n\nprint(\"Encoder output shape:\", encoder_out.shape)  # Should be (batch_size, 196, 2048)\nprint(\"Predictions shape:\", predictions.shape)     # Should be (batch_size, max_len, vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:54.251763Z","iopub.execute_input":"2025-04-20T16:28:54.252016Z","iopub.status.idle":"2025-04-20T16:28:57.598427Z","shell.execute_reply.started":"2025-04-20T16:28:54.251998Z","shell.execute_reply":"2025-04-20T16:28:57.597528Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n100%|██████████| 171M/171M [00:00<00:00, 223MB/s] \n<ipython-input-21-e68131b897fa>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  predictions, _, _, _, _ = decoder(encoder_out, captions, torch.tensor(lengths).clone().detach().to(device))\n","output_type":"stream"},{"name":"stdout","text":"Encoder output shape: torch.Size([4, 196, 2048])\nPredictions shape: torch.Size([4, 16, 10307])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"class MaskedCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=0)  # ignore <pad>\n\n    def forward(self, predictions, targets, lengths):\n        batch_size, max_len, vocab_size = predictions.shape\n\n        predictions = predictions.view(-1, vocab_size)      # (batch_size * max_len, vocab_size)\n        targets = targets.contiguous().view(-1)              # (batch_size * max_len)\n\n        losses = self.criterion(predictions, targets)        # (batch_size * max_len)\n\n        # Create mask\n        mask = torch.arange(max_len).expand(batch_size, max_len).to(lengths.device)\n        mask = (mask < lengths.unsqueeze(1)).float()         # (batch_size, max_len)\n        mask = mask.view(-1)                                 # Flatten to (batch_size * max_len)\n\n        losses = losses * mask\n        return losses.sum() / mask.sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:57.599580Z","iopub.execute_input":"2025-04-20T16:28:57.599882Z","iopub.status.idle":"2025-04-20T16:28:57.605581Z","shell.execute_reply.started":"2025-04-20T16:28:57.599855Z","shell.execute_reply":"2025-04-20T16:28:57.604727Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"encoder = Encoder().to(device)\ndecoder = DecoderWithAttention(\n    attention_dim=512,\n    embed_dim=300,\n    decoder_dim=512,\n    vocab_size=len(word_map),\n    encoder_dim=2048,\n    dropout=0.5\n).to(device)\n\n# Only fine-tune the encoder's adaptive pool layer\nencoder_params = list(encoder.adaptive_pool.parameters()) + list(encoder.cnn.layer4.parameters())\ndecoder_params = decoder.parameters()\n\noptimizer = torch.optim.Adam(\n    params=[\n        {'params': encoder_params, 'lr': 1e-4},  # Lower LR for encoder\n        {'params': decoder_params, 'lr': 4e-4}    # Higher LR for decoder\n    ],\n    weight_decay=1e-5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:57.606415Z","iopub.execute_input":"2025-04-20T16:28:57.606651Z","iopub.status.idle":"2025-04-20T16:28:58.711392Z","shell.execute_reply.started":"2025-04-20T16:28:57.606623Z","shell.execute_reply":"2025-04-20T16:28:58.710474Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train_epoch(encoder, decoder, dataloader, criterion, optimizer, device, grad_clip=5.0):\n    encoder.train()\n    decoder.train()\n    total_loss = 0\n    \n    for i, (images, captions, lengths) in enumerate(dataloader):\n        images = images.to(device)\n        captions = captions.to(device)\n        # lengths = torch.tensor(lengths).to(device)\n        lengths_tensor = torch.tensor(lengths).to(device)\n\n        # Forward pass\n        encoder_out = encoder(images)\n        predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n        \n        # Remove <start> token and truncate to actual lengths\n        targets = captions[:, 1:]  # (batch_size, max_len-1)\n        predictions = predictions[:, :max(decode_lengths), :]  # (batch_size, actual_max_len, vocab_size)\n\n        # Calculate loss\n        loss = criterion(predictions, targets, lengths_tensor)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n        torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n        \n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        if i % 100 == 0:\n            print(f\"Batch [{i}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n    \n    return total_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:58.712354Z","iopub.execute_input":"2025-04-20T16:28:58.712647Z","iopub.status.idle":"2025-04-20T16:28:58.718611Z","shell.execute_reply.started":"2025-04-20T16:28:58.712625Z","shell.execute_reply":"2025-04-20T16:28:58.717792Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def validate(encoder, decoder, val_loader, criterion, device):\n    encoder.eval()\n    decoder.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for images, captions, lengths in val_loader:\n            images = images.to(device)\n            captions = captions.to(device)\n            # lengths = torch.tensor(lengths).to(device)\n            lengths_tensor = torch.tensor(lengths).to(device)\n\n            \n            encoder_out = encoder(images)\n            predictions, _, decode_lengths, _, _ = decoder(encoder_out, captions, lengths)\n            \n            targets = captions[:, 1:]\n            predictions = predictions[:, :max(decode_lengths), :]\n            \n            loss = criterion(predictions, targets, lengths_tensor)\n            total_loss += loss.item()\n    \n    return total_loss / len(val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:58.722913Z","iopub.execute_input":"2025-04-20T16:28:58.723105Z","iopub.status.idle":"2025-04-20T16:28:58.738145Z","shell.execute_reply.started":"2025-04-20T16:28:58.723089Z","shell.execute_reply":"2025-04-20T16:28:58.737496Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import os\nimport glob\n\ndef save_checkpoint(encoder, decoder, optimizer, epoch, train_loss, val_loss, word_map, \n                   checkpoint_dir, best_val_loss=float('inf'), is_best=False):\n    \"\"\"Save model checkpoint\"\"\"\n    os.makedirs(checkpoint_dir, exist_ok=True)\n    \n    # Save regular checkpoint\n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n    \n    checkpoint = {\n        'epoch': epoch + 1,  # Save as next epoch to resume from\n        'encoder_state_dict': encoder.state_dict(),\n        'decoder_state_dict': decoder.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'word_map': word_map,\n        'best_val_loss': best_val_loss\n    }\n    \n    try:\n        torch.save(checkpoint, checkpoint_path)\n        print(f\"Checkpoint saved: {checkpoint_path}\")\n        \n        # Save best model separately if this is the best one\n        if is_best:\n            best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n            torch.save(checkpoint, best_model_path)\n            print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n            \n        # Keep only the last 3 checkpoints to save disk space\n        checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth')))\n        if len(checkpoint_files) > 3:\n            for old_checkpoint in checkpoint_files[:-3]:\n                os.remove(old_checkpoint)\n                \n    except Exception as e:\n        print(f\"Error saving checkpoint: {e}\")\n        # Try alternative save location\n        torch.save(checkpoint, '/kaggle/working/emergency_checkpoint.pth')\n\ndef resume_from_checkpoint(checkpoint_path, encoder, decoder, optimizer, device):\n    \"\"\"Load checkpoint and resume training\"\"\"\n    print(f\"Loading checkpoint from {checkpoint_path}\")\n    \n    # Load checkpoint on CPU to avoid GPU memory issues\n    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n    \n    # Load model states\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    \n    # Move models to device after loading\n    encoder = encoder.to(device)\n    decoder = decoder.to(device)\n    \n    # Load optimizer state\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    # Move optimizer state to device\n    for state in optimizer.state.values():\n        for k, v in state.items():\n            if isinstance(v, torch.Tensor):\n                state[k] = v.to(device)\n    \n    # Return the starting epoch and best validation loss\n    start_epoch = checkpoint['epoch']  # Continue from next epoch\n    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n    \n    print(f\"Resuming from epoch {start_epoch} with best validation loss: {best_val_loss:.4f}\")\n    return encoder, decoder, optimizer, start_epoch, best_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:58.739615Z","iopub.execute_input":"2025-04-20T16:28:58.739805Z","iopub.status.idle":"2025-04-20T16:28:58.752651Z","shell.execute_reply.started":"2025-04-20T16:28:58.739788Z","shell.execute_reply":"2025-04-20T16:28:58.751947Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Initialize components\ncriterion = MaskedCrossEntropyLoss().to(device)\nnum_epochs = 1  # For initial test\n\n# Quick test with 1 batch\ntest_images, test_captions, test_lengths = next(iter(dataloader))\ntest_images = test_images.to(device)\ntest_captions = test_captions.to(device)\n# test_lengths = torch.tensor(test_lengths).to(device)\ntest_lengths = test_lengths.clone().detach().to(device)\n\n\n# Forward test\nencoder_out = encoder(test_images)\npredictions, _, decode_lengths, _, _ = decoder(encoder_out, test_captions, test_lengths)\ntargets = test_captions[:, 1:]\n\n# Convert decode_lengths to tensor\n# decode_lengths = torch.tensor(decode_lengths).to(device)\n\nloss = criterion(predictions, targets, decode_lengths)\n# decode_lengths = torch.tensor(decode_lengths).to(device)\n# loss = criterion(predictions, targets, decode_lengths)\n\nprint(f\"Initial loss: {loss.item():.4f}\")  # Should be ~log(vocab_size) = ~9.2 for vocab_size=10307\noptimizer.step()  # Verify backprop works without errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:58.753402Z","iopub.execute_input":"2025-04-20T16:28:58.753665Z","iopub.status.idle":"2025-04-20T16:28:59.180383Z","shell.execute_reply.started":"2025-04-20T16:28:58.753634Z","shell.execute_reply":"2025-04-20T16:28:59.179508Z"}},"outputs":[{"name":"stdout","text":"Initial loss: 8.3699\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Define checkpoint directory\ncheckpoint_dir = '/kaggle/working/checkpoints'\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Find latest checkpoint if it exists\nlatest_checkpoint = None\ncheckpoint_files = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth'))\nif checkpoint_files:\n    latest_checkpoint = max(checkpoint_files, key=os.path.getctime)\n\n# Resume from checkpoint if available\nstart_epoch = 0\nbest_val_loss = float('inf')\n\nif latest_checkpoint:\n    encoder, decoder, optimizer, start_epoch, best_val_loss = resume_from_checkpoint(\n        latest_checkpoint, encoder, decoder, optimizer, device\n    )\n\nnum_epochs = 1  # Set your desired number of epochs\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n    \n    # Train for one epoch\n    train_loss = train_epoch(\n        encoder, decoder, dataloader,\n        criterion, optimizer, device\n    )\n    print(f\"Train Loss: {train_loss:.4f}\")\n    \n    # Validate\n    val_loss = validate(\n        encoder, decoder, val_dataloader,\n        criterion, device\n    )\n    print(f\"Validation Loss: {val_loss:.4f}\")\n    \n    # Check if this is the best model\n    is_best = val_loss < best_val_loss\n    if is_best:\n        best_val_loss = val_loss\n    \n    # Save checkpoint\n    save_checkpoint(\n        encoder, decoder, optimizer,\n        epoch, train_loss, val_loss,\n        word_map, checkpoint_dir,\n        best_val_loss, is_best\n    )\n\n# Mark training as complete\nwith open(os.path.join(checkpoint_dir, 'TRAINING_COMPLETE'), 'w') as f:\n    f.write('Training completed successfully')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:28:59.181293Z","iopub.execute_input":"2025-04-20T16:28:59.181580Z","iopub.status.idle":"2025-04-20T16:59:25.471513Z","shell.execute_reply.started":"2025-04-20T16:28:59.181543Z","shell.execute_reply":"2025-04-20T16:59:25.470464Z"}},"outputs":[{"name":"stdout","text":"\n--- Epoch 1/1 ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-24-8702f4ae3cc5>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  lengths_tensor = torch.tensor(lengths).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Batch [0/29572] Loss: 8.7839\nBatch [100/29572] Loss: 6.0670\nBatch [200/29572] Loss: 5.7006\nBatch [300/29572] Loss: 5.1492\nBatch [400/29572] Loss: 4.5294\nBatch [500/29572] Loss: 4.3419\nBatch [600/29572] Loss: 5.4314\nBatch [700/29572] Loss: 5.2734\nBatch [800/29572] Loss: 5.3129\nBatch [900/29572] Loss: 5.0946\nBatch [1000/29572] Loss: 5.5115\nBatch [1100/29572] Loss: 5.1531\nBatch [1200/29572] Loss: 4.2772\nBatch [1300/29572] Loss: 5.8138\nBatch [1400/29572] Loss: 4.6566\nBatch [1500/29572] Loss: 5.5000\nBatch [1600/29572] Loss: 5.3487\nBatch [1700/29572] Loss: 5.8966\nBatch [1800/29572] Loss: 4.5530\nBatch [1900/29572] Loss: 4.4664\nBatch [2000/29572] Loss: 4.7916\nBatch [2100/29572] Loss: 4.8951\nBatch [2200/29572] Loss: 4.8632\nBatch [2300/29572] Loss: 5.1999\nBatch [2400/29572] Loss: 4.2018\nBatch [2500/29572] Loss: 5.5036\nBatch [2600/29572] Loss: 4.8867\nBatch [2700/29572] Loss: 5.1693\nBatch [2800/29572] Loss: 5.4849\nBatch [2900/29572] Loss: 5.2772\nBatch [3000/29572] Loss: 5.0046\nBatch [3100/29572] Loss: 5.3472\nBatch [3200/29572] Loss: 4.9672\nBatch [3300/29572] Loss: 5.1863\nBatch [3400/29572] Loss: 5.5611\nBatch [3500/29572] Loss: 5.7213\nBatch [3600/29572] Loss: 4.6847\nBatch [3700/29572] Loss: 5.4556\nBatch [3800/29572] Loss: 4.8591\nBatch [3900/29572] Loss: 4.4303\nBatch [4000/29572] Loss: 4.7132\nBatch [4100/29572] Loss: 4.7068\nBatch [4200/29572] Loss: 5.1768\nBatch [4300/29572] Loss: 5.2305\nBatch [4400/29572] Loss: 4.9270\nBatch [4500/29572] Loss: 4.3648\nBatch [4600/29572] Loss: 5.3892\nBatch [4700/29572] Loss: 4.8770\nBatch [4800/29572] Loss: 5.8170\nBatch [4900/29572] Loss: 4.8024\nBatch [5000/29572] Loss: 5.0378\nBatch [5100/29572] Loss: 4.8983\nBatch [5200/29572] Loss: 5.7813\nBatch [5300/29572] Loss: 5.3990\nBatch [5400/29572] Loss: 4.3387\nBatch [5500/29572] Loss: 5.3401\nBatch [5600/29572] Loss: 4.3697\nBatch [5700/29572] Loss: 4.2225\nBatch [5800/29572] Loss: 5.6951\nBatch [5900/29572] Loss: 4.9721\nBatch [6000/29572] Loss: 5.8744\nBatch [6100/29572] Loss: 5.7729\nBatch [6200/29572] Loss: 4.9924\nBatch [6300/29572] Loss: 5.0655\nBatch [6400/29572] Loss: 5.3910\nBatch [6500/29572] Loss: 4.7446\nBatch [6600/29572] Loss: 5.2772\nBatch [6700/29572] Loss: 5.2476\nBatch [6800/29572] Loss: 4.9413\nBatch [6900/29572] Loss: 5.1328\nBatch [7000/29572] Loss: 4.3602\nBatch [7100/29572] Loss: 4.4909\nBatch [7200/29572] Loss: 5.1020\nBatch [7300/29572] Loss: 4.4530\nBatch [7400/29572] Loss: 4.8975\nBatch [7500/29572] Loss: 4.6514\nBatch [7600/29572] Loss: 4.6813\nBatch [7700/29572] Loss: 4.4642\nBatch [7800/29572] Loss: 4.5802\nBatch [7900/29572] Loss: 5.7283\nBatch [8000/29572] Loss: 4.6386\nBatch [8100/29572] Loss: 4.0604\nBatch [8200/29572] Loss: 5.1916\nBatch [8300/29572] Loss: 5.5253\nBatch [8400/29572] Loss: 4.3926\nBatch [8500/29572] Loss: 4.7167\nBatch [8600/29572] Loss: 5.3547\nBatch [8700/29572] Loss: 4.7630\nBatch [8800/29572] Loss: 5.3743\nBatch [8900/29572] Loss: 4.0737\nBatch [9000/29572] Loss: 5.3157\nBatch [9100/29572] Loss: 5.6131\nBatch [9200/29572] Loss: 4.7244\nBatch [9300/29572] Loss: 5.1188\nBatch [9400/29572] Loss: 4.7854\nBatch [9500/29572] Loss: 4.1506\nBatch [9600/29572] Loss: 4.8897\nBatch [9700/29572] Loss: 4.9462\nBatch [9800/29572] Loss: 4.6665\nBatch [9900/29572] Loss: 5.6378\nBatch [10000/29572] Loss: 5.3008\nBatch [10100/29572] Loss: 5.6854\nBatch [10200/29572] Loss: 5.5315\nBatch [10300/29572] Loss: 5.2977\nBatch [10400/29572] Loss: 5.0396\nBatch [10500/29572] Loss: 4.8743\nBatch [10600/29572] Loss: 5.2818\nBatch [10700/29572] Loss: 5.6802\nBatch [10800/29572] Loss: 4.8277\nBatch [10900/29572] Loss: 5.0777\nBatch [11000/29572] Loss: 4.6985\nBatch [11100/29572] Loss: 5.2162\nBatch [11200/29572] Loss: 4.7110\nBatch [11300/29572] Loss: 4.8729\nBatch [11400/29572] Loss: 5.1005\nBatch [11500/29572] Loss: 5.1279\nBatch [11600/29572] Loss: 5.0672\nBatch [11700/29572] Loss: 5.1290\nBatch [11800/29572] Loss: 4.0988\nBatch [11900/29572] Loss: 5.0562\nBatch [12000/29572] Loss: 5.6305\nBatch [12100/29572] Loss: 5.1785\nBatch [12200/29572] Loss: 4.7508\nBatch [12300/29572] Loss: 4.7299\nBatch [12400/29572] Loss: 5.3582\nBatch [12500/29572] Loss: 5.4208\nBatch [12600/29572] Loss: 5.1782\nBatch [12700/29572] Loss: 4.8213\nBatch [12800/29572] Loss: 5.4629\nBatch [12900/29572] Loss: 5.3266\nBatch [13000/29572] Loss: 6.0713\nBatch [13100/29572] Loss: 4.7572\nBatch [13200/29572] Loss: 5.0991\nBatch [13300/29572] Loss: 5.8648\nBatch [13400/29572] Loss: 5.7112\nBatch [13500/29572] Loss: 5.6125\nBatch [13600/29572] Loss: 4.3021\nBatch [13700/29572] Loss: 5.9556\nBatch [13800/29572] Loss: 5.3560\nBatch [13900/29572] Loss: 5.5065\nBatch [14000/29572] Loss: 5.0905\nBatch [14100/29572] Loss: 4.1988\nBatch [14200/29572] Loss: 5.1734\nBatch [14300/29572] Loss: 5.1459\nBatch [14400/29572] Loss: 4.5007\nBatch [14500/29572] Loss: 5.7031\nBatch [14600/29572] Loss: 5.7920\nBatch [14700/29572] Loss: 5.1250\nBatch [14800/29572] Loss: 4.7284\nBatch [14900/29572] Loss: 5.7055\nBatch [15000/29572] Loss: 4.9727\nBatch [15100/29572] Loss: 4.4932\nBatch [15200/29572] Loss: 4.5008\nBatch [15300/29572] Loss: 5.6474\nBatch [15400/29572] Loss: 4.9514\nBatch [15500/29572] Loss: 5.2160\nBatch [15600/29572] Loss: 6.0984\nBatch [15700/29572] Loss: 6.0229\nBatch [15800/29572] Loss: 5.4885\nBatch [15900/29572] Loss: 4.6008\nBatch [16000/29572] Loss: 5.4665\nBatch [16100/29572] Loss: 5.2220\nBatch [16200/29572] Loss: 5.3175\nBatch [16300/29572] Loss: 4.9264\nBatch [16400/29572] Loss: 5.3388\nBatch [16500/29572] Loss: 5.8176\nBatch [16600/29572] Loss: 4.9197\nBatch [16700/29572] Loss: 5.5368\nBatch [16800/29572] Loss: 4.9870\nBatch [16900/29572] Loss: 5.6737\nBatch [17000/29572] Loss: 5.6167\nBatch [17100/29572] Loss: 4.5808\nBatch [17200/29572] Loss: 4.0613\nBatch [17300/29572] Loss: 5.4107\nBatch [17400/29572] Loss: 5.5895\nBatch [17500/29572] Loss: 4.8471\nBatch [17600/29572] Loss: 5.7741\nBatch [17700/29572] Loss: 5.1315\nBatch [17800/29572] Loss: 5.5813\nBatch [17900/29572] Loss: 4.9984\nBatch [18000/29572] Loss: 5.0240\nBatch [18100/29572] Loss: 5.0635\nBatch [18200/29572] Loss: 4.6025\nBatch [18300/29572] Loss: 4.4305\nBatch [18400/29572] Loss: 5.8176\nBatch [18500/29572] Loss: 5.0905\nBatch [18600/29572] Loss: 4.7906\nBatch [18700/29572] Loss: 4.0792\nBatch [18800/29572] Loss: 5.3451\nBatch [18900/29572] Loss: 5.0988\nBatch [19000/29572] Loss: 5.8350\nBatch [19100/29572] Loss: 6.1889\nBatch [19200/29572] Loss: 5.4912\nBatch [19300/29572] Loss: 5.1423\nBatch [19400/29572] Loss: 6.4458\nBatch [19500/29572] Loss: 5.2019\nBatch [19600/29572] Loss: 4.8903\nBatch [19700/29572] Loss: 4.6879\nBatch [19800/29572] Loss: 5.4970\nBatch [19900/29572] Loss: 4.7633\nBatch [20000/29572] Loss: 5.0292\nBatch [20100/29572] Loss: 4.4289\nBatch [20200/29572] Loss: 5.0547\nBatch [20300/29572] Loss: 5.4704\nBatch [20400/29572] Loss: 4.9576\nBatch [20500/29572] Loss: 4.9900\nBatch [20600/29572] Loss: 5.0809\nBatch [20700/29572] Loss: 4.9295\nBatch [20800/29572] Loss: 4.2615\nBatch [20900/29572] Loss: 4.6061\nBatch [21000/29572] Loss: 5.0869\nBatch [21100/29572] Loss: 5.4172\nBatch [21200/29572] Loss: 5.2201\nBatch [21300/29572] Loss: 5.3869\nBatch [21400/29572] Loss: 4.8413\nBatch [21500/29572] Loss: 5.3277\nBatch [21600/29572] Loss: 3.9289\nBatch [21700/29572] Loss: 4.5943\nBatch [21800/29572] Loss: 4.9872\nBatch [21900/29572] Loss: 5.7157\nBatch [22000/29572] Loss: 4.2382\nBatch [22100/29572] Loss: 4.0024\nBatch [22200/29572] Loss: 4.7243\nBatch [22300/29572] Loss: 5.7991\nBatch [22400/29572] Loss: 5.6540\nBatch [22500/29572] Loss: 4.7296\nBatch [22600/29572] Loss: 4.7128\nBatch [22700/29572] Loss: 5.5456\nBatch [22800/29572] Loss: 4.5086\nBatch [22900/29572] Loss: 4.6111\nBatch [23000/29572] Loss: 5.0062\nBatch [23100/29572] Loss: 4.6653\nBatch [23200/29572] Loss: 4.8315\nBatch [23300/29572] Loss: 4.9545\nBatch [23400/29572] Loss: 4.2238\nBatch [23500/29572] Loss: 4.7127\nBatch [23600/29572] Loss: 4.7801\nBatch [23700/29572] Loss: 4.1831\nBatch [23800/29572] Loss: 4.8693\nBatch [23900/29572] Loss: 4.9958\nBatch [24000/29572] Loss: 4.7545\nBatch [24100/29572] Loss: 4.8718\nBatch [24200/29572] Loss: 5.2868\nBatch [24300/29572] Loss: 4.8456\nBatch [24400/29572] Loss: 4.4824\nBatch [24500/29572] Loss: 4.2548\nBatch [24600/29572] Loss: 5.0086\nBatch [24700/29572] Loss: 4.4740\nBatch [24800/29572] Loss: 5.1011\nBatch [24900/29572] Loss: 5.0292\nBatch [25000/29572] Loss: 5.1356\nBatch [25100/29572] Loss: 3.9149\nBatch [25200/29572] Loss: 6.6829\nBatch [25300/29572] Loss: 5.3841\nBatch [25400/29572] Loss: 5.2535\nBatch [25500/29572] Loss: 3.9825\nBatch [25600/29572] Loss: 4.6594\nBatch [25700/29572] Loss: 4.8390\nBatch [25800/29572] Loss: 4.6630\nBatch [25900/29572] Loss: 4.9989\nBatch [26000/29572] Loss: 5.9556\nBatch [26100/29572] Loss: 4.7001\nBatch [26200/29572] Loss: 4.3631\nBatch [26300/29572] Loss: 4.8942\nBatch [26400/29572] Loss: 5.4511\nBatch [26500/29572] Loss: 5.1130\nBatch [26600/29572] Loss: 5.4417\nBatch [26700/29572] Loss: 5.3390\nBatch [26800/29572] Loss: 5.1679\nBatch [26900/29572] Loss: 5.0631\nBatch [27000/29572] Loss: 4.7949\nBatch [27100/29572] Loss: 5.3991\nBatch [27200/29572] Loss: 4.6712\nBatch [27300/29572] Loss: 5.4547\nBatch [27400/29572] Loss: 5.3998\nBatch [27500/29572] Loss: 5.5743\nBatch [27600/29572] Loss: 4.8351\nBatch [27700/29572] Loss: 6.4097\nBatch [27800/29572] Loss: 4.7461\nBatch [27900/29572] Loss: 5.7086\nBatch [28000/29572] Loss: 5.2069\nBatch [28100/29572] Loss: 5.8414\nBatch [28200/29572] Loss: 3.9552\nBatch [28300/29572] Loss: 4.4661\nBatch [28400/29572] Loss: 4.7577\nBatch [28500/29572] Loss: 5.0164\nBatch [28600/29572] Loss: 4.2377\nBatch [28700/29572] Loss: 5.7610\nBatch [28800/29572] Loss: 4.7146\nBatch [28900/29572] Loss: 5.0769\nBatch [29000/29572] Loss: 5.6703\nBatch [29100/29572] Loss: 4.8101\nBatch [29200/29572] Loss: 4.7309\nBatch [29300/29572] Loss: 4.5618\nBatch [29400/29572] Loss: 4.9529\nBatch [29500/29572] Loss: 3.9557\nTrain Loss: 5.0448\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-25-077716cb4c09>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  lengths_tensor = torch.tensor(lengths).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 4.9426\nCheckpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1.pth\nNew best model saved with validation loss: 4.9426\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# for epoch in range(num_epochs):\n#     print(f\"\\n--- Epoch {epoch + 1} ---\")\n\n#     train_loss = train_epoch(\n#         encoder, decoder, dataloader,\n#         criterion, optimizer, device\n#     )\n#     print(f\"Train Loss: {train_loss:.4f}\")\n\n#     val_loss = validate(\n#         encoder, decoder, val_dataloader,\n#         criterion, device\n#     )\n#     print(f\"Validation Loss: {val_loss:.4f}\")\n\n#     # Save the model after each epoch\n#     torch.save({\n#         'epoch': epoch,\n#         'encoder_state_dict': encoder.state_dict(),\n#         'decoder_state_dict': decoder.state_dict(),\n#         'optimizer_state_dict': optimizer.state_dict(),\n#         'loss': train_loss\n#     }, f'model_epoch_{epoch + 1}.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:59:25.472856Z","iopub.execute_input":"2025-04-20T16:59:25.473170Z","iopub.status.idle":"2025-04-20T16:59:25.476948Z","shell.execute_reply.started":"2025-04-20T16:59:25.473143Z","shell.execute_reply":"2025-04-20T16:59:25.476017Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"checkpoint_path = '/kaggle/working/checkpoint.pth'\n\ndef save_checkpoint(state, filename=checkpoint_path):\n    torch.save(state, filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:23:07.866070Z","iopub.execute_input":"2025-04-20T17:23:07.866484Z","iopub.status.idle":"2025-04-20T17:23:07.870729Z","shell.execute_reply.started":"2025-04-20T17:23:07.866454Z","shell.execute_reply":"2025-04-20T17:23:07.869815Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, encoder, decoder, optimizer):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    return checkpoint['epoch']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:59:25.498382Z","iopub.execute_input":"2025-04-20T16:59:25.498803Z","iopub.status.idle":"2025-04-20T16:59:25.510855Z","shell.execute_reply.started":"2025-04-20T16:59:25.498765Z","shell.execute_reply":"2025-04-20T16:59:25.510106Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!pip install gensim==4.3.2  # Uses pre-built wheels, avoiding compilation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:59:25.511603Z","iopub.execute_input":"2025-04-20T16:59:25.511872Z","iopub.status.idle":"2025-04-20T16:59:31.877992Z","shell.execute_reply.started":"2025-04-20T16:59:25.511840Z","shell.execute_reply":"2025-04-20T16:59:31.877009Z"}},"outputs":[{"name":"stdout","text":"Collecting gensim==4.3.2\n  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (1.13.1)\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (7.0.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (2.4.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->gensim==4.3.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.18.5->gensim==4.3.2) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->gensim==4.3.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.18.5->gensim==4.3.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.18.5->gensim==4.3.2) (2024.2.0)\nDownloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: gensim\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.3\n    Uninstalling gensim-4.3.3:\n      Successfully uninstalled gensim-4.3.3\nSuccessfully installed gensim-4.3.2\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# First, install all required dependencies\n!pip install numpy scipy scikit-learn pandas gensim==4.3.2 theano\n\n# Install nlg-eval from GitHub (bypass PyPI)\n!pip install git+https://github.com/Maluuba/nlg-eval.git --no-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:59:31.879121Z","iopub.execute_input":"2025-04-20T16:59:31.879451Z","iopub.status.idle":"2025-04-20T16:59:52.660732Z","shell.execute_reply.started":"2025-04-20T16:59:31.879412Z","shell.execute_reply":"2025-04-20T16:59:52.659672Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: gensim==4.3.2 in /usr/local/lib/python3.10/dist-packages (4.3.2)\nRequirement already satisfied: theano in /usr/local/lib/python3.10/dist-packages (1.0.5)\nRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim==4.3.2) (7.0.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from theano) (1.17.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim==4.3.2) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nCollecting git+https://github.com/Maluuba/nlg-eval.git\n  Cloning https://github.com/Maluuba/nlg-eval.git to /tmp/pip-req-build-w2_2x4hz\n  Running command git clone --filter=blob:none --quiet https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-w2_2x4hz\n  Resolved https://github.com/Maluuba/nlg-eval.git to commit 2ab4528fad5548315cf61e40c2249fec8c8ad233\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: nlg-eval\n  Building wheel for nlg-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for nlg-eval: filename=nlg_eval-2.4.1-py3-none-any.whl size=98924372 sha256=ff6bcb59a111edc2c2b619eb17d1393ea61f7c3a634977fffaacdd764d264c7e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-u6uhjt5d/wheels/89/06/a3/78b62739ab38973883fc8239cfbc41cbf08643e105ddd745d8\nSuccessfully built nlg-eval\nInstalling collected packages: nlg-eval\nSuccessfully installed nlg-eval-2.4.1\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!pip install --no-deps git+https://github.com/Theano/Theano.git@adfe319ce6b781083d8dc3200fb4481b00853791","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T16:59:52.661727Z","iopub.execute_input":"2025-04-20T16:59:52.662025Z","iopub.status.idle":"2025-04-20T17:00:07.135505Z","shell.execute_reply.started":"2025-04-20T16:59:52.661989Z","shell.execute_reply":"2025-04-20T17:00:07.134640Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Theano/Theano.git@adfe319ce6b781083d8dc3200fb4481b00853791\n  Cloning https://github.com/Theano/Theano.git (to revision adfe319ce6b781083d8dc3200fb4481b00853791) to /tmp/pip-req-build-da6wcw5c\n  Running command git clone --filter=blob:none --quiet https://github.com/Theano/Theano.git /tmp/pip-req-build-da6wcw5c\n  Running command git rev-parse -q --verify 'sha^adfe319ce6b781083d8dc3200fb4481b00853791'\n  Running command git fetch -q https://github.com/Theano/Theano.git adfe319ce6b781083d8dc3200fb4481b00853791\n  Running command git checkout -q adfe319ce6b781083d8dc3200fb4481b00853791\n  Resolved https://github.com/Theano/Theano.git to commit adfe319ce6b781083d8dc3200fb4481b00853791\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: Theano\n  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for Theano: filename=Theano-0.9.0.dev1-py3-none-any.whl size=2762815 sha256=b92fa48a4a5a2ea9b7ea2d1cee0a7c472e664436d34092c47749a5ec0899ebde\n  Stored in directory: /root/.cache/pip/wheels/c7/0d/3d/9f6f46e82fca7ec07253b04a61356867f9e53e80a90ed3e6b0\nSuccessfully built Theano\nInstalling collected packages: Theano\n  Attempting uninstall: Theano\n    Found existing installation: Theano 1.0.5\n    Uninstalling Theano-1.0.5:\n      Successfully uninstalled Theano-1.0.5\nSuccessfully installed Theano-0.9.0.dev1\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"!pip install git+https://github.com/Maluuba/nlg-eval.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:07.136666Z","iopub.execute_input":"2025-04-20T17:00:07.137019Z","iopub.status.idle":"2025-04-20T17:00:29.821404Z","shell.execute_reply.started":"2025-04-20T17:00:07.136977Z","shell.execute_reply":"2025-04-20T17:00:29.820560Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Maluuba/nlg-eval.git\n  Cloning https://github.com/Maluuba/nlg-eval.git to /tmp/pip-req-build-pq8riq6w\n  Running command git clone --filter=blob:none --quiet https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-pq8riq6w\n  Resolved https://github.com/Maluuba/nlg-eval.git to commit 2ab4528fad5548315cf61e40c2249fec8c8ad233\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: click>=6.3 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (8.1.7)\nCollecting nltk>=3.4.5 (from nlg-eval==2.4.1)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (1.26.4)\nRequirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (5.9.5)\nRequirement already satisfied: requests>=2.19 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (2.32.3)\nRequirement already satisfied: six>=1.11 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (1.17.0)\nRequirement already satisfied: Cython>=0.28.5 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (3.0.11)\nRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (1.13.1)\nRequirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (1.2.2)\nCollecting gensim~=3.8.3 (from nlg-eval==2.4.1)\n  Downloading gensim-3.8.3.tar.gz (23.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: Theano>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (0.9.0.dev1)\nRequirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.10/dist-packages (from nlg-eval==2.4.1) (4.67.1)\nCollecting xdg (from nlg-eval==2.4.1)\n  Downloading xdg-6.0.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim~=3.8.3->nlg-eval==2.4.1) (7.0.5)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.5->nlg-eval==2.4.1) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.5->nlg-eval==2.4.1) (2024.11.6)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11.0->nlg-eval==2.4.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11.0->nlg-eval==2.4.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11.0->nlg-eval==2.4.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11.0->nlg-eval==2.4.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11.0->nlg-eval==2.4.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.11.0->nlg-eval==2.4.1) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->nlg-eval==2.4.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->nlg-eval==2.4.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->nlg-eval==2.4.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19->nlg-eval==2.4.1) (2025.1.31)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17->nlg-eval==2.4.1) (3.5.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart_open>=1.8.1->gensim~=3.8.3->nlg-eval==2.4.1) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.11.0->nlg-eval==2.4.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.11.0->nlg-eval==2.4.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.11.0->nlg-eval==2.4.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.11.0->nlg-eval==2.4.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.11.0->nlg-eval==2.4.1) (2024.2.0)\nDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xdg-6.0.0-py3-none-any.whl (3.9 kB)\nBuilding wheels for collected packages: gensim\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for gensim (setup.py) ... \u001b[?25lerror\n\u001b[31m  ERROR: Failed building wheel for gensim\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25h  Running setup.py clean for gensim\nFailed to build gensim\n\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (gensim)\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from nlgeval import NLGEval\nnlgeval = NLGEval()  # Should work without errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.822507Z","iopub.execute_input":"2025-04-20T17:00:29.822873Z","iopub.status.idle":"2025-04-20T17:00:29.881015Z","shell.execute_reply.started":"2025-04-20T17:00:29.822838Z","shell.execute_reply":"2025-04-20T17:00:29.879876Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-cfd04dd97cba>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlgeval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNLGEval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlgeval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLGEval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should work without errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nlgeval/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, no_overlap, no_skipthoughts, no_glove, metrics_to_omit)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_skipthoughts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_skipthoughts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'SkipThoughtCS'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_to_omit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_skipthoughts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_skipthought_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_glove\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglove_metrics\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_to_omit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nlgeval/__init__.py\u001b[0m in \u001b[0;36mload_skipthought_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_skipthought_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mnlgeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskipthoughts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskipthoughts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nlgeval/skipthoughts/skipthoughts.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/theano/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigdefaults\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# This is the api version for ops that generate C code.  External ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/theano/configdefaults.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from theano.configparser import (AddConfigVar, BoolParam, ConfigParam, EnumStr,\n\u001b[0m\u001b[1;32m     18\u001b[0m                                  \u001b[0mFloatParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStrParam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                  TheanoConfigParser, THEANO_FLAGS_DICT)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/theano/configparser.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigparser\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfigParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/theano/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munbound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutableMapping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDictMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'MutableMapping' from 'collections' (/usr/lib/python3.10/collections/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'MutableMapping' from 'collections' (/usr/lib/python3.10/collections/__init__.py)","output_type":"error"}],"execution_count":36},{"cell_type":"code","source":"# Load the best model for evaluation\nbest_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\nif os.path.exists(best_model_path):\n    print(f\"Loading best model from {best_model_path}\")\n    checkpoint = torch.load(best_model_path, map_location=device)\nelse:\n    # Fall back to latest checkpoint if no best model exists\n    latest_checkpoint = max(glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pth')), \n                           key=os.path.getctime)\n    print(f\"No best model found, loading latest checkpoint: {latest_checkpoint}\")\n    checkpoint = torch.load(latest_checkpoint, map_location=device)\n\nencoder.load_state_dict(checkpoint['encoder_state_dict'])\ndecoder.load_state_dict(checkpoint['decoder_state_dict'])\n\n# Set models to evaluation mode\nencoder.eval()\ndecoder.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.881742Z","iopub.status.idle":"2025-04-20T17:00:29.882165Z","shell.execute_reply":"2025-04-20T17:00:29.881981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checkpoint = torch.load('/kaggle/working/model_epoch_1.pth', map_location=device, weights_only=True)\n\n# encoder.load_state_dict(checkpoint['encoder_state_dict'])\n# decoder.load_state_dict(checkpoint['decoder_state_dict'])\n# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# start_epoch = checkpoint['epoch'] + 1\n# print(f\"Resuming from epoch {start_epoch}\")\n\n# !pip install git+https://github.com/Maluuba/nlg-eval.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.883455Z","iopub.status.idle":"2025-04-20T17:00:29.883870Z","shell.execute_reply":"2025-04-20T17:00:29.883700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install git+https://github.com/Maluuba/nlg-eval.git@2ab4528fad554831","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.884836Z","iopub.status.idle":"2025-04-20T17:00:29.885211Z","shell.execute_reply":"2025-04-20T17:00:29.885049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install nltk rouge pycocoevalcap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.886262Z","iopub.status.idle":"2025-04-20T17:00:29.886669Z","shell.execute_reply":"2025-04-20T17:00:29.886473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!conda install -y gensim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.887953Z","iopub.status.idle":"2025-04-20T17:00:29.888284Z","shell.execute_reply":"2025-04-20T17:00:29.888154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nlgeval import NLGEval\nnlg = NLGEval(no_skipthoughts=True, no_glove=True)\n\nreferences = []\nhypotheses = []\n\nencoder.eval()\ndecoder.eval()\n\nwith torch.no_grad():\n    for images, captions, lengths in val_dataloader:\n        images = images.to(device)\n        encoder_out = encoder(images)\n\n        for i in range(images.size(0)):\n            img_enc = encoder_out[i].unsqueeze(0)\n            generated_caption = generate_caption(decoder, img_enc)\n            hypotheses.append(generated_caption)\n\n            # Use first reference caption\n            ref_tokens = [idx2word[idx] for idx in captions[i].tolist() if idx not in {0, 1, 2, 3}]\n            references.append([' '.join(ref_tokens)])\n\n# Evaluate\nmetrics = nlg.evaluate(hypotheses, references)\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.889298Z","iopub.status.idle":"2025-04-20T17:00:29.889716Z","shell.execute_reply":"2025-04-20T17:00:29.889514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(decoder, encoder_out, max_len=20):\n    vocab_size = decoder.vocab_size\n    decoder.eval()\n\n    h, c = decoder.init_hidden_state(encoder_out)\n    encoder_out = encoder_out.view(1, -1, encoder_out.size(-1))\n\n    word_map_rev = {v: k for k, v in word_map.items()}\n\n    word = torch.tensor([word_map['<start>']]).to(device)\n    caption = []\n    \n    for _ in range(max_len):\n        embeddings = decoder.embedding(word).unsqueeze(0)  # (1, 1, embed_dim)\n        awe, _ = decoder.attention(encoder_out, h)  # (1, encoder_dim)\n        gate = decoder.sigmoid(decoder.f_beta(h))\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings.squeeze(1), awe], dim=1), (h, c))\n        preds = decoder.fc(h)\n        word = preds.argmax(1)\n\n        predicted_word = word.item()\n        if predicted_word == word_map['<end>']:\n            break\n        caption.append(word_map_rev.get(predicted_word, '<unk>'))\n\n    return ' '.join(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.890653Z","iopub.status.idle":"2025-04-20T17:00:29.890904Z","shell.execute_reply":"2025-04-20T17:00:29.890801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Attention Maps","metadata":{}},{"cell_type":"code","source":"def generate_caption_with_attention(decoder, encoder_out, word_map, max_len=20):\n    decoder.eval()\n\n    h, c = decoder.init_hidden_state(encoder_out)\n    encoder_out = encoder_out.view(1, -1, encoder_out.size(-1))\n    word = torch.tensor([word_map['<start>']]).to(device)\n\n    rev_word_map = {v: k for k, v in word_map.items()}\n\n    caption = []\n    alphas = []\n\n    for _ in range(max_len):\n        embeddings = decoder.embedding(word).unsqueeze(0)  # (1, 1, embed_dim)\n        awe, alpha = decoder.attention(encoder_out, h)\n        gate = decoder.sigmoid(decoder.f_beta(h))\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings.squeeze(1), awe], dim=1), (h, c))\n        preds = decoder.fc(h)\n        word = preds.argmax(1)\n\n        predicted_word = word.item()\n        if predicted_word == word_map['<end>']:\n            break\n\n        caption.append(rev_word_map.get(predicted_word, '<unk>'))\n        alphas.append(alpha.cpu().detach().numpy())\n\n    return caption, alphas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.891592Z","iopub.status.idle":"2025-04-20T17:00:29.892001Z","shell.execute_reply":"2025-04-20T17:00:29.891833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\ndef visualize_attention(image_path, caption, alphas, smooth=True):\n    image = Image.open(image_path).convert(\"RGB\")\n    image = image.resize([224, 224], Image.LANCZOS)\n\n    plt.figure(figsize=(15, 15))\n    for t in range(len(caption)):\n        plt.subplot(np.ceil(len(caption) / 5.), 5, t + 1)\n\n        plt.text(0, 1, '%s' % caption[t], color='black', backgroundcolor='white', fontsize=12)\n        plt.imshow(image)\n\n        alpha = alphas[t].reshape(14, 14)  # attention is 14x14 from ResNet\n        if smooth:\n            import cv2\n            alpha = cv2.GaussianBlur(alpha, (5, 5), 0)\n\n        plt.imshow(alpha, alpha=0.6, extent=(0, 224, 224, 0), cmap='viridis')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.893059Z","iopub.status.idle":"2025-04-20T17:00:29.893410Z","shell.execute_reply":"2025-04-20T17:00:29.893268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load image\nimage_path = \"/kaggle/input/coco-2017-dataset/coco2017/val2017/000000391895.jpg\"\nimage = Image.open(image_path).convert(\"RGB\")\n\n# Transform image\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\nimage_tensor = transform(image).unsqueeze(0).to(device)\n\n# Encode image\nencoder_out = encoder(image_tensor)\ncaption, alphas = generate_caption_with_attention(decoder, encoder_out, word_map)\n\n# Visualize\nvisualize_attention(image_path, caption, alphas)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:00:29.894173Z","iopub.status.idle":"2025-04-20T17:00:29.894560Z","shell.execute_reply":"2025-04-20T17:00:29.894377Z"}},"outputs":[],"execution_count":null}]}